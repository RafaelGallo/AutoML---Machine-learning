title,score,id,subreddit,url,num_comments,body,created,timestamp
[D] Simple Questions Thread,14,qorekl,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qorekl/d_simple_questions_thread/,54,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",1636300818.0,2021-11-07 17:00:18
[P][R] Rocket-recycling with Reinforcement Learning,607,qt2tws,MachineLearning,https://v.redd.it/enkc1p6oldz71,33,,1636815127.0,2021-11-13 15:52:07
[Project] PyTorch Implementations of 37 GAN papers (including BigGAN and StyleGAN2),334,qt10az,MachineLearning,https://i.redd.it/fjf94vuj4dz71.png,22,,1636809198.0,2021-11-13 14:13:18
[P] Using Talknet to clone Dreams' voice.,70,qt9yql,MachineLearning,https://v.redd.it/hmw9gpizefz71,19,,1636837002.0,2021-11-13 21:56:42
[R] StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN,699,qsw47b,MachineLearning,https://i.redd.it/arv5dyfjfbz71.jpg,12,,1636788679.0,2021-11-13 08:31:19
[R] Pruning Attention Heads of Transformer Models Using A* Search: A Novel Approach to Compress Big NLP Architectures,8,qtl5fx,MachineLearning,https://arxiv.org/abs/2110.15225,1,,1636877009.0,2021-11-14 09:03:29
Walk-Forward Target Encoding and Data Leakage [D],3,qtiqlw,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qtiqlw/walkforward_target_encoding_and_data_leakage_d/,1,"Hi!

I am working on a time series problem where I need to also test historical predictions through time (using a walk-forward procedure). There is a ""data leakage"" when dealing with time series and using simple target encoding features like :

df.groupby(col)\[target\].transform('mean').

&#x200B;

I am wondering if anyone has built a custom function to build these target encoding features without leaking data... Kind of like a walk-forward target encoding function...",1636866927.0,2021-11-14 06:15:27
[N] Introduction to Data Science book updated,0,qtlmql,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qtlmql/n_introduction_to_data_science_book_updated/,0,"Hello All,

I have updated my ongoing Data Science book. The following sections were added:

1, [Vectors](https://datascience-book.gitlab.io/book.html#_vectors)

2. [Matrices](https://datascience-book.gitlab.io/book.html#_matrices)

3. [Sigmoid](https://datascience-book.gitlab.io/book.html#_sigmoid)

4. [K Means Clustering](https://datascience-book.gitlab.io/book.html#_k_means_clustering)

5. [Gradient Descent](https://datascience-book.gitlab.io/book.html#_gradient_descent)

I  would be very happy if you people read it and suggest feedback and  improvements. One can get the entire book in PDF and epub format [here](https://datascience-book.gitlab.io/).

Thank you.",1636879153.0,2021-11-14 09:39:13
[P] Cedille: The largest French language model,15,qt5ekt,MachineLearning,https://github.com/coteries/cedille-ai,1,,1636823040.0,2021-11-13 18:04:00
[D] Analysis of ICLR 2022 Review Scores,24,qszmuu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qszmuu/d_analysis_of_iclr_2022_review_scores/,1,"We analysed the relationship between ICLR 2022 review scores and factors such as social media popularity and presence in Arxiv ([twitter thread](https://twitter.com/labmlai/status/1459443227543564289)).

Here are some of the results of the analysis,

* Papers that were present on Arxiv had higher recommendation scores.

Mean review scores: 5.1 (papers on Arxiv) > 4.7 (papers not on Arxiv)

https://preview.redd.it/9k59wa6tkcz71.png?width=1860&format=png&auto=webp&s=1a1c8a0b0b8392d90d4962fea4d3dbbea5e264f4

* Papers that were shared on Twitter (with >= 5 likes) also had higher recommendation scores than other papers.

Mean review scores: 5.4 (Tweeted) > 4.8 (not Tweeted)

https://preview.redd.it/08mxiqzvkcz71.png?width=1874&format=png&auto=webp&s=e39bea067d5d8b362b50f451c55b20f3e6f7e1ed

* Papers that had source code available (or promised to upload soon with empty repos) also got better reviews.

Mean review scores: 5.2 (with code) > 4.8 (without code)

https://preview.redd.it/xeyzn8zykcz71.png?width=1902&format=png&auto=webp&s=7748aabf8950215f8a93a4e287fef4cca3b4fa92

* Scatter plot of likes on Twitter against the review score.

  There is a small positive correlation between Twitter likes and review scores. The correlation   
  coefficient is 0.2.

https://preview.redd.it/yvnxrgu1lcz71.png?width=1900&format=png&auto=webp&s=c19bbcd7db77ad7a29c9319e27ecdcd084b4bc9d

All ICLR 2022 submissions sorted by review scores can be found here : [https://papers.labml.ai/papers/iclr\_2022?sort\_by=conference\_score&dsc=0](https://papers.labml.ai/papers/iclr_2022?sort_by=conference_score&dsc=0)",1636803906.0,2021-11-13 12:45:06
[P] Lyric Studio - Artificial Intelligence Song Lyrics,1,qtik3a,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qtik3a/p_lyric_studio_artificial_intelligence_song_lyrics/,0,"I created this demo in two days (last weekend), created an AI Generated Song Lyrics app. Thought this subreddit would love it.  PyTorch for text generation, and also does live sentiment analysis with background shading

Link: [https://LyricStudio.com](https://LyricStudio.com)

Do you have any feedback or improvements?",1636866248.0,2021-11-14 06:04:08
"[D] Is capacity of RAM the only important factor, or is the speed and bandwidth also important for deep learning models?",3,qta1dt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qta1dt/d_is_capacity_of_ram_the_only_important_factor_or/,6,"I will soon be building a deep learning machine, but given the introduction of DDR5 in the new intel platforms, I am unsure whether the extra cost is worth it. DDR5 introduces higher speed with much more bandwidth, but is this important for deep learning? Or is the cost not worth it?

For example, for the same cost, I can get higher capacity of RAM, say 128GB DDR4 at average speed, or I can get smaller capacity, say 64GB DDR5, but with double the speed and double the bandwidth.

Which is more important?",1636837217.0,2021-11-13 22:00:17
[D] (Paper Overview) MAE: Masked Autoencoders Are Scalable Vision Learners,6,qt4y6g,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qt4y6g/d_paper_overview_mae_masked_autoencoders_are/,0," **Video**

[https://youtu.be/LKixq2S2Pz8](https://youtu.be/LKixq2S2Pz8)

**Paper**

[https://arxiv.org/abs/2111.06377](https://arxiv.org/abs/2111.06377)

**Abstract**

 This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.",1636821678.0,2021-11-13 17:41:18
[D] Algorithms for correlation of events/issues,3,qt90rt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qt90rt/d_algorithms_for_correlation_of_eventsissues/,0,"Generally any application software co-exists with multiple other software where a problem with one software can have a cascading effect on some other software, somewhere else in the stack. E.gÂ 

1. you deploy you application as a pod in kubernetes (orchestration software)
2. but the pod or container is not running after sometime because the ec2 machine (or any virtual machine) on which the pod was scheduled to run has some issues.
3. the ec2 machine or the vm is having some issues because the autoscaling software that is supposed to manage the vms properly is not working correctly.
4. the autoscaling software is not working correctly because of some other dependent system not doing this job.
5. basically there can be a CHAIN of issues where one issue can have a significant cascading effect on many other dependent software systems.

You can apply this logic to many other places. Imagine your application is not working properly BECAUSE your load balancer is not working properly which is BECAUSE you load balancer vms are having networking issues because you have some datacenter level failures etc. You have all the logs files spread across the stack which reports these issues independently but usually it takes manual effort to CORRELATE them to figure out what is the root cause.

typically the user sees the symptoms at a high level in the stack, e.g. application is working properly and then they will start debugging and then finally you figure out somewhere down the stack something is wrong. This usually takes specific expertise (SRE) and takes time to arrive at the root cause.

basically what is happening is here is a CHAIN of events with cascading effect. There are ways to catch this using monitoring dashboards but the problem is usually those monitoring dashboard are setup in a static way, manually setup and typically has a maintenance problems long term. i.e. if there is a change in one of the software version which wants you to change a the dashboard accordingly, then you might forget to modify the monitoring dashboard accordingly etc. Also setting up these monitoring dashboards are very specific to the problem, this means you need to setup various different kinds of dashboards for different systems & scenarios.

We want to apply AI to this problem if possible. We want to come up with a well trained AI based system which can tell you which is the actual root cause issue if you give it 10 different issues that happened around a specific time interval.Â 

E.g.

\-Â We will have a model that detects various issues (anomaly detection based software which we already have). So we now have the list of issues that happened across the stack around a certain time interval. Lets say 10 issues

\-Â Now let s say we have 10 different issues that happens around a particular time interval (1-5 mins), we would like to send all the 10 issues to AI based system and we want it to tell that out of these 10 issues, 1 or 2 issues is most probably the root cause which could have caused all the other issues.

If we have such AI based system that will basically CORRELATE multiple issues and tell us the ROOT CAUSE i.e. basically which of those issues is likely to have caused the other issues, that will be very useful.

Is this possible? Any thoughts/guidance will be greatly appreciated. What are all the typical algorithms/approaches people apply for this kind of problem.

Imagine a use case for this where if we have this systems, I can send in a bunch of alerts coming in (assume the alert has the relevant data about the issue attached to it) and this AI system can process it and segregate those alerts in a such a way, it informs the user which alert is actual issue which is causing other. This would mean user can quickly resolve it getting rid of other alerts. There are many such use cases like this",1636834058.0,2021-11-13 21:07:38
[D] Opinion: the recent paper on buggy resizing libraries is misleading,196,qsl5jj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsl5jj/d_opinion_the_recent_paper_on_buggy_resizing/,24,"A recent paper [On Buggy Resizing Libraries and Surprising Subtleties in FID Calculation](https://arxiv.org/abs/2104.11222) claims that the image downsampling methods of OpenCV, Tensorflow and PyTorch are ""buggy"", and therefore PIL should be used instead for FID estimation. The corresponding twitter post was quite popular during the last week.

I believe that this claim and the main figure of the paper is misleading, because the issue is caused by aliasing and it can be fixed by simply setting the right parameters in the functions they used.

&#x200B;

[Their main figure](https://preview.redd.it/9va7lmkwd8z71.png?width=2116&format=png&auto=webp&s=1684c5e6979740787d1bc71d0e32b0bdffb48672)

&#x200B;

[My reproduction and fixed results \(bilinear\)](https://preview.redd.it/t06cmvx0e8z71.png?width=976&format=png&auto=webp&s=cf1fe97a307b6429bd31a20989707f8487d47b14)

On the bottom image, you can see the reproduced and antialiased downsampling results in all the frameworks. In all cases, a single parameter modification was enough to mitigate the issue. I shared the code and my complete opinion in this repository: [https://github.com/beresandras/buggy-resizing-critique](https://github.com/beresandras/buggy-resizing-critique)

Though I believe that the discussion on whether antialiasing should be a default in image libraries is valuable, in my opinion none of these methods is ""buggy"", and the paper presents the issue in a sensationalist way.",1636751991.0,2021-11-12 22:19:51
"[P] KalidoKit â€“ Face, Pose, and Hand Tracking Kinematics",7,qt5d0g,MachineLearning,https://github.com/yeemachine/kalidokit,1,,1636822920.0,2021-11-13 18:02:00
[P] tsflex: flexible and efficient feature extraction for time series,12,qsvzio,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsvzio/p_tsflex_flexible_and_efficient_feature/,2,"Are you looking for a time series feature extraction package that is both efficient and flexible? [tsflex](https://github.com/predict-idlab/tsflex) has got you covered. They just published a release that allows integration with tsfresh as well.

Go check it out ðŸ‘‰ [https://github.com/predict-idlab/tsflex/releases/tag/v0.2.2](https://github.com/predict-idlab/tsflex/releases/tag/v0.2.2)",1636788149.0,2021-11-13 08:22:29
[R] DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories,2,qt4wxw,MachineLearning,https://arxiv.org/abs/2111.04625,1,,1636821570.0,2021-11-13 17:39:30
[P] Text-to-image ruDALL-E Kandinsky (XXL) 12 billion parameter model checkpoint is apparently available for download,26,qsrdyk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsrdyk/p_texttoimage_rudalle_kandinsky_xxl_12_billion/,4,"[Here](https://sbercloud.ru/ru/datahub/rugpt3family/ru-dalle-12b) is a download page for ruDALL-E Kandinsky (XXL) 12 billion parameter model checkpoint ([English translation](https://sbercloud-ru.translate.goog/ru/datahub/rugpt3family/ru-dalle-12b?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=nui)). I did not sign up to try to download the file(s). The 12B parameter model should be available per my interpretation of [this press release](https://www.sberbank.com/news-and-media/press-releases/article?newsID=52a32604-55f8-4558-a748-ae79c929abc4&blockID=7&regionID=77&lang=en&type=NEWS). Edit: According to [this comment](https://www.reddit.com/r/MachineLearning/comments/qsrdyk/comment/hkfslaj/), public availability should be on December 1.

Prior mention of ruDALL-E in this subreddit:

[Text-to-image models ruDALL-E Kandinsky (XXL) (12 billion parameters) and ruDALL-E Malevich (XL) (1.3 billion parameters). A demo for the latter is available.](https://www.reddit.com/r/MachineLearning/comments/qlbye5/p_texttoimage_models_rudalle_kandinsky_xxl_12/)

[ruDALL-E model is open-source \[P\]](https://www.reddit.com/r/MachineLearning/comments/qmzy8a/rudalle_model_is_opensource_p/)",1636771128.0,2021-11-13 03:38:48
[D] Nvidia Jetson Thoughts,2,qt1vuy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qt1vuy/d_nvidia_jetson_thoughts/,10,"Has anyone used any of the Nvidia Jetson models for production use cases? We're considering using them for a health application, but I haven't heard much about the developer experience one way or the other. The alternative would probably be running the application on a mobile device.",1636812177.0,2021-11-13 15:02:57
[D] Can a GIoU loss (generalized intersection over union) be used after an STN module (spatial transformer network)?,2,qt1pnz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qt1pnz/d_can_a_giou_loss_generalized_intersection_over/,0,"I have a model that uses an STN module for number detection and Mean Squared Error loss. But I would like to replace it for GIoU, because MSE doesn't take into account how much of the target area has been detected, only how close individual coordinates are close to the target. But I wonder if this makes sense. Has anyone tried it, or has some insight?",1636811639.0,2021-11-13 14:53:59
[P] AutoAI â€“ A framework to find the best performing AI/ML model,0,qt5dqt,MachineLearning,https://github.com/blobcity/autoai,0,,1636822978.0,2021-11-13 18:02:58
[D] A dilemma of an ML guy in industry,91,qseien,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qseien/d_a_dilemma_of_an_ml_guy_in_industry/,43,"I'm a Machine Learning Engineer in the healthcare sector and I've been thinking about this a lot. With the rapid as fuck advancements in research on AI, do I keep up with research more or should I focus on learning about engineering the solutions(pipelines, etc.). Example: reading a paper/using a new GAN vs. reading about a case study.",1636733388.0,2021-11-12 17:09:48
"[D] A quick history of GANs - 8 years of GAN evolution, and the intuition behind it explained by Casual GAN Papers",1,qt4k2c,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qt4k2c/d_a_quick_history_of_gans_8_years_of_gan/,0,"This tutorial covers the intuition behind:

* Variational Auto Encoder (VAE)
* The OG GAN
* StyleGAN
* VQGAN

Telegram post: [https://t.me/casual\_gan/184](https://t.me/casual_gan/184)

Blog post: [https://www.casualganpapers.com/history-of-gans-survey-of-popular-architectures/GAN-architectures-overview.html](https://www.casualganpapers.com/history-of-gans-survey-of-popular-architectures/GAN-architectures-overview.html)

https://preview.redd.it/2r5lfkxwxdz71.jpg?width=1024&format=pjpg&auto=webp&s=bed2b1c7a5b68f4cca8549e986cc77d6ef7d4ebd

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries and GAN tutorials!",1636820465.0,2021-11-13 17:21:05
[D] Are there any theoretical analyses on the success of AlphaGo zero?,5,qsv83w,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsv83w/d_are_there_any_theoretical_analyses_on_the/,1,I am wondering whether there is a theoretical guarantee for the convergence of the network used in AlphaGo zero or for the optimality of the searching algorithm?,1636784987.0,2021-11-13 07:29:47
[D] Causality research in ML is a scam (warning: controversial),188,qs7g4t,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qs7g4t/d_causality_research_in_ml_is_a_scam_warning/,146,"Don't get me wrong, causal inference are *the* most methods for application areas where we observe a bunch of random variable and want to figure out the causal relationship between them.

This rant is not about the method is itself, but how ML research is recently getting exploiting the term ""causality"" for the sake of the hype and citations. 

In ML we have two main paradigms: Supervised learning and RL.

Work on causality (e.g., Bernhard SchÃ¶lkopf, Judea Pearl etc.) tells us that is impossible to determine the causal relationship between variables if we only observe them without performing any interaction. Therefore, with supervised learning we cannot learn a causal model but we need to impose one. Period.

Regarding RL, tabular Q-learning is guaranteed to converge to the maximum expected reward policy. Period. That's it, nothing else needs to be said about it.

However, despite these two fundamental statements, there is currently growing a hype in general ML research about causality. I am completely fine with causality research as long as it focuses on the application area mentioned in my first sentence. But this recent trend brings the concept into computer vision, NLP, etc. , where things become vague quite fast, exaggerated by the fact that research on causality can be already extremely vague and deeply philosophical (e.g., what's the practical implication of Newcomb's paradox).

In computer vision no causal model is known. Even the vision processing of humans or animals is very little understood. Moreover, CV tasks are inherently under-specified. For instance, is a cartoon drawing of an elephant still an elephant? Or is is out-of-distribution (OOD), or its own class, or multiple classes? Are we talking about the causal relationship of pixels, patches, or concepts? What makes an elephant ear an elephant ear?

This vagueness, combined with the general trend in ML of throwing a bunch of overly complex math statements into a paper to impress the reviewers, is really concerning.

I bet that there will be hundreds of papers on this topic be published in the next years that contribute very little to our understanding, but will create millions of (self-) citations.",1636708549.0,2021-11-12 10:15:49
[D] Discussion about fine tuning language models for generating SQL queries,0,qszlto,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qszlto/d_discussion_about_fine_tuning_language_models/,0,"Hey you all, hope you are doing well, I have a few things that I wanted to discuss about language modeling now I have a bunch of critical data stored in database format (for my company) I am working on something that can be used to train a language model to convert natural language to a SQL query. Currently we have a rule based system which has tons of if else conditions and, but it works, the only issue I see is that if a new database gets added, the currently existing rule based system will break.   

So I am thinking of a way to automate this NL2SQL using a machine translation model such as GPT-2, later followed by something bigger like GPT-J. I have looked at a few papers namely PICARD and the corresponding challenge called Spider  

I wanted to discuss a few queries that I have and the genral thoughts that people have about this problem.  

 Â Am I batshit cray, to think finetuning large language models with a new data set would work well?  

\- Â Any alternative's that you would suggest to this?  

\- Â How should I go about this? Just plain use the trainer API from Hugging Face?  

\- Â Other genral thoughts and opinions etc  

Thank you for taking your time to read this, I hope we can have a meaningful conversation about this.",1636803783.0,2021-11-13 12:43:03
"[P] Language model ruGPT-3 13B is apparently available for download. From an English translation: ""The ruGPT-3 13B model contains 13 billion parameters and is capable of continuing texts in Russian and English, as well as in programming languages.""",2,qstdsm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qstdsm/p_language_model_rugpt3_13b_is_apparently/,0,"[Download page](https://sbercloud.ru/ru/datahub/rugpt3family/rugpt-3-13b) ([English translation](https://sbercloud-ru.translate.goog/ru/datahub/rugpt3family/rugpt-3-13b?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=nui)). I did not sign up to try to download the file(s).

[Reference](https://habr.com/ru/company/sberbank/blog/550056/) ([English translation](https://habr-com.translate.goog/ru/company/sberbank/blog/550056/?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=nui)).",1636778046.0,2021-11-13 05:34:06
[D] How do I lower my standards for code quality to match my research team?,92,qs5igm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qs5igm/d_how_do_i_lower_my_standards_for_code_quality_to/,85,"I recently joined a lab working on research in robotics / reinforcement learning as a research intern. I graduated college recently, all my previous work experience is ML engineering but not active research, rather implementing things people have already done.

I've been in the lab a few months now and it's become clear to me that the standards for code in research are a lot lower than I expected. I am the only one on my team pushing for things like CI pipelines, refactoring, documentation, regression tests, etc. And when I do raise such concerns the usual response is that such things are not worth spending time on. My supervisor once told me that the job of a researcher is not to write good code, it's to find a correct problem formulation, describe it correctly, and then pass it to a software engineer whose job it is to create the high-quality implementation. They emphasised the importance of rapid iteration of the idea rather than going slowly because they're concerned about getting scooped in our current project. This is true to some extent because we are a small and resource-limited group whose experiments can take hours to run, working in a popular field (quadruped legged locomotion).

I struggle a lot with this mindset. I hate looking at the crappy code written by myself and by my teammates and I hate debugging it / trying to understand it. A lot of my time is wasted on things like checking which experimental configuration a model was trained from because we don't have automated logging of that. Aesthetically I also just dislike it, I have high personal standards of code quality (Uncle Bob's *Clean Code,* anyone?) which I feel like I'm constantly breaking when I have to write code for work.

I'm looking for advice on how I can learn to tolerate this better, or other similar resolutions to the problem. Thanks in advance.

==============================================================

Edit: Thank you all for the kind replies so far. I have received many more responses than I was ever expecting to get. It's going to be difficult to reply to each one but I will do my best to read all of them and consider. I will respond here to some general points I've seen made repeatedly.

1. I'm not arrogant enough to think that I know everything about research and coding after 3 months in a lab. And as many have rightly pointed out there's no need for research code to live up to the standards of long-term production code. I do recognize that in this case the problem likely lies (mostly) with me. Hence the title of 'how do I lower my standards' and the question of 'how do I learn to tolerate something I'm not comfortable with'. Seeing how popular this post is, I'm sure this struggle resonates with many others as well. As a result I hope for there to be less cynicism about my motives. The discussion so far has been largely productive and on-topic, I appreciate very much it staying that way.
2. To give some additional context, I am working right now as part of a group. I have a main project which I work on with 1 other person (also a recent graduate). My supervisor actually doesn't look at the code we write at all and as far as I can tell does not care much about code. They seem to implicitly trust that we are able to correctly implement things according to the description (which I believe is true for both of us). At our weekly meetings, we present our progress in the form of videos / graphs / slides. This means that I and my teammate are solely responsible for maintaining the quality of code in our current project, and as mentioned we have philosophical disagreements about how much code quality matters. (I know I am not necessarily right.)
3. The project I am working on is very much not a 'one-off experiment'. Our main codebase is inherited from somebody else who is no longer in the lab (which was itself an iteration on some open-source implementation). We are iterating various elements of the robot's sensor setup in addition to the reinforcement learning algorithm and training curriculum. With well-known off-the-shelf envs like those in MuJoCo I'd be pretty happy accepting that it was working as intended. With code I write myself or code written by other people which is not tested, it's a lot harder to have that confidence.
4. As some have correctly pointed out, using a standard logging tool like W&B would be an easy fix to the time spent on resolving experimental configurations. That issue in particular could be resolved easily by setting up the appropriate pipelines and would likely be a net plus to my team. More generally, I could benefit from critically examining the practices I am used to and seeing which have concrete benefits in my current workplace, and then propose those to the team.
5. I am a strong proponent of unit tests. Unit tests save my butt from making simple mistakes. It's already incredibly difficult to figure out why experiments fail sometimes. I can at least eliminate some of the possible causes by unit-testing code where easy and appropriate. Also, when I do fix a bug, I write a unit test to enforce the fix, and then I can mostly forget about it. That mental real estate can go to things that can't be automated as easily.
6. Similarly, I am a big fan of simple CI pipelines like Github Actions. Perhaps the term 'CI' comes with a lot of associated bells and whistles which are (rightfully) not used in research code, but to me CI is simply a way to automate the manual running of unit tests. With a bit of know-how, Github CI workflows are easy to set up from a cookiecutter template and they save a lot of time as well as mental worry. I like to include linters as well but it's not necessary to the core functionality.",1636699761.0,2021-11-12 07:49:21
[R] DeepMindâ€™s One Pass ImageNet: A New Benchmark for Resource Efficiency in Deep Learning,13,qse6gc,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qse6gc/r_deepminds_one_pass_imagenet_a_new_benchmark_for/,3,"A DeepMind research team presents the One Pass ImageNet (OPIN) problem, designed to study the space and compute efficiency of deep learning in a streaming setting with constrained data storage and to develop model training systems where each example is passed to the system only once. 

Here is a quick read: [DeepMindâ€™s One Pass ImageNet: A New Benchmark for Resource Efficiency in Deep Learning.](https://syncedreview.com/?p=31386&preview=true&_thumbnail_id=31387)

The paper *One Pass ImageNet* is on [arXiv](https://arxiv.org/abs/2111.01956v1).",1636732499.0,2021-11-12 16:54:59
[D] What simulation software would you use to train a custom robot in?,6,qskjq5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qskjq5/d_what_simulation_software_would_you_use_to_train/,7,"Say I want to use Reinforcement Learning to train a custom, virtual robot to stand. What simulation software would you recommend?
The requirements are the following...  

 * Good training times for a billions steps. I would want the step to restart if the robot hit the ground hard. I would like to get lots of training steps in as fast as possible.  
 * Inputs and outputs to and from Python. To observe the state of the simulation and take actions at every one of the robots joints. Position, balance, and velocity observations will be needed as well.  
 * Ability to observe visual sensors on the robot  
 * Very fine construction of the virtual robot in terms of size and weight of components. Exact positioning of the components. Precise force of the robot's motors at each joint.

I see Gazebo and Mujoco are popular, but I'm not sure they can do what I need. If there isn't an option maybe I will write my own physics engine.",1636750267.0,2021-11-12 21:51:07
"[R] PhD & postdoc positions at UT Austin: ML for complex systems (chaotic time series, cellular automata, & fluid dynamics)",193,qrygiv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrygiv/r_phd_postdoc_positions_at_ut_austin_ml_for/,17,"Hi! Iâ€™m looking for PhD students interested in the intersection of machine learning and physics, particularly chaos and fluid dynamics.
I'm also informally looking for postdocs (official ad coming soon).

### About

We are based in the physics department at UT Austin, and are affiliated with the Oden Institute for Computational Engineering & Sciences. Here is a link to [the lab website](https://gilpinlab.github.io/?utm_source=en_us_der)

Projects are pretty flexible based on curiosity and mutual interest; thereâ€™s room for more algorithm-focused time series mining projects, as well as pencil-and-paper dynamical systems and control theory problems. As far as applications go, weâ€™re particularly interested in projects that can eventually be used for biological data or fluid dynamics. Weâ€™re super open to applicants from uncommon academic or personal backgrounds

Here are some recent examples:
+ â€œChaos as an interpretable benchmark for forecasting and data-driven modellingâ€ (NeurIPS 2021) https://arxiv.org/abs/2110.05266
+ â€œDeep reconstruction of strange attractors from time seriesâ€ (NeurIPS 2020) https://arxiv.org/abs/2002.05909  
+ â€œCellular automata as convolutional neural networksâ€ (Phys Rev E 2019) https://arxiv.org/abs/1809.02942

### Applying

For grad students, feel free to apply to any of these grad programs at UT Austin:
+ The physics department (due 12/1)
+ The Oden CSEM program (due 12/15). 
+ Other departments (CS, EE) are probably possible, too

For postdocs, please reach out to me informally.

Our physics PhD program does not require physics GRE, normal GRE, or a physics undergrad degree. There are only four core courses, and we have previously had students with undergrads in CS, engineering, bioinformatics, etc. Our quals are research talks, not written exams

If any of this sounds interesting, feel free to email/DM me or chat with me at NeurIPS or APS",1636674961.0,2021-11-12 00:56:01
"[P] Questions regarding self-supervised learning for music (DINO, MoCo, ...)",5,qshm5b,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qshm5b/p_questions_regarding_selfsupervised_learning_for/,6," 

Hello everyone! First of all, I am pretty inactive on Reddit, so I hope that this is the right place for this post. I am a computer science graduate student focusing on machine learning, working on an interdisciplinary research project regarding the analysis of music.

**TL;DR** 

I try to extract general and descriptive music features on various different levels. 

* Which self-supervised methods can you recommend for limited resource capacities? 
* Which data augmentations to use for music? 
* General tips and tricks for SSL with music? 
* Is it a good idea to use a pretrained backbone in order to get away with a small dataset and compute (e.g. OpenAI's JukeBox VQ-VAE)? 
* Why is my DINO setup not converging?

&#x200B;

**General idea** Rather than solving some specific MIR task (e.g. genre classification), my goal is to extract generic, interpretable and descriptive music features. In other words, I want a model that â€œperceivesâ€ and â€œunderstandsâ€ music in general without giving it a specific goal. I would then further analyze extracted features, e.g., to find relations to human music perception, or use them for downstream tasks. Ideally, the outcome is a model that can describe music on various different levels (e.g., beat, rhythm, harmony), for example by features extracted from different neural network layers (shallow: low-level, deeper: higher-level). I know that this is by far no easy task, but it is worth investigating the possibilities and limitations in my opinion.

**Which self-supervised learning method?** After some research, self-supervised learning (SSL) seems the way to go here. SSL is a research area that gained momentum over the last two years or so and there are multiple proposed methods, however applied mostly in the computer vision area (images). Additionally, SSL methods seem to be rather data hungry and as you might imagine my resources are quite limited. I have a GTX 1060 available locally, but I am also willing to pay in order to train a model on GPU cloud services (e.g. Lambda GPU). Since my budget is low, Iâ€™d like to do all testing locally or on other free alternatives such as Colab in order to find the right method, hyperparameters, etc. for the actual training on a payed server.

**Idea: Using pretrained JukeBox backbone** My idea is to use the pretrained VQ-VAE from OpenAIâ€™s JukeBox (see below) as a backbone and hope that it extracts useful intermediate features, which I can forward to my (comparably small) model. With this I hope to get away with a small dataset (60 hours of music?) and relatively low resource usage, while still achieving reasonable results. Does anyone have experience with such setups? Now to the SSL method itself (ignoring generative models): On the one hand, there are contrastive methods such as CPC, SimCLR or (the less resource hungry?) MoCo. On the other hand, there are methods which do not explicitly formulate a contrastive loss such as BYOL or DINO. Especially the latter one seems interesting. Unfortunately, I have no experience in training such models at all, which is why I wanted to ask the community for some tips and feedback suited for my problem. Preferable methods are those which do not require much hyperparameter tuning (time and compute limitations) or much compute during training (e.g., large batch sizes with SimCLR), but still achieve good results.

**DINO not working properly** Currently I am testing the DINO method on a very limited setup locally: My dataset consists of a very small portion of the FMA dataset (90 minutes of music) with a sample size of 4 seconds and a batch size of 14 using the LARS optimizer. Additionally, I have adopted the audio augmentation strategy from a paper that applies SimCLR to music (CLMR, see below). I am very happy about ideas for other/better music augmentations for my problem, though. However, the model does not seem to converge properly. The loss decreases quite quickly after a few epochs. After a while however the model seems to collapse, and the loss increases rapidly staying high over the remaining epochs. I figure this has something to do with the momentum hyperparameter (i.e., how much of the studentâ€™s weights get transferred to the teacher after each iteration). When increasing this number, the collapse effect is minimized or is non-existent at all, if large enough, but the loss does not really decrease much either. I have logged everything and computed stats for several epochs, if anyone is interested. When splitting the loss up into the teacher entropy and the KL divergence between the student and the teacher, one can see that the collapse is caused by the entropy part. Does anyone have experience with DINO or similar methods and might have a clue why this is happening? As mentioned, I can give more details about the hyperparameters, logs, statistics, etc. Is it possible that this phenomenon is due to the small dataset and batch size on my local machine, and it would diminish on the cloud with a larger dataset and/or batch size? Though they mentioned in the paper that they have successfully tested their method even with a batch size of 8.

I know this is a lengthy post, but I wanted to share as much detail as possible about my goal and resulting problems. Hope anyone might give some feedback.

**Papers** JukeBox: [https://arxiv.org/abs/2005.00341](https://arxiv.org/abs/2005.00341) CLMR: [https://arxiv.org/abs/2103.09410](https://arxiv.org/abs/2103.09410) DINO: [https://arxiv.org/abs/2104.14294](https://arxiv.org/abs/2104.14294)",1636741959.0,2021-11-12 19:32:39
[D] Best/Favorite format for writing without a conference in mind?,4,qsh3z0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsh3z0/d_bestfavorite_format_for_writing_without_a/,4,"Usually, I have ideas, do some experiments, draft a manuscript, etc., and then afterwards edit it into a format for a particular conference or journal. But I'm wondering, among the community, what's your preferred format for writing something if you don't know where you're sending it yet? If something's just on arXiv, what format is visually the easiest to read?

I kinda like the JMLR format, pretty plain, pretty basic, but I can appreciate that it wastes a lot of space on the author list. The NeurIPS seems like the default, definitive format, but I find the bars around the title kind of ugly. The IEEE formats always present visually as being just completely fucking impenetrable. Not a fan of the ICLR format putting all the titles in caps.",1636740560.0,2021-11-12 19:09:20
[R] prune-then-quantize or quantize-then-prune for post-training optimization of computer vision models.,3,qsi0u2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsi0u2/r_prunethenquantize_or_quantizethenprune_for/,3,"As a student that is interested in improving the efficiency (model size, throughput, energy) of pre-trained deep learning models in the computer vision domain, I was wondering if there was a clear winning approach on in what order we should prune and quantize a pre-trained model.

So basically if you were given a pre-trained deep learning model (e.g. Resnet), would one of the following approaches lead to better solutions, in terms of the accuracy to efficiency ratio (e.g. model size, throughput, energy consumption):

- quantize-then-prune (with finetuning/retraining after every stage)
- prune-then-quantize (with finetuning/retraining after every stage)

I have trouble finding related works that answer this kind of question. To me it seems to be valid question, but maybe I'm missing something obvious. 

Any input would be appreciated.",1636743107.0,2021-11-12 19:51:47
[R] NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework,18,qs5cr7,MachineLearning,https://arxiv.org/abs/2111.04130,6,,1636699070.0,2021-11-12 07:37:50
[D] Micro-Grants,0,qss5os,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qss5os/d_microgrants/,0,Are there micro-grant sources besides AI Grant and Unitary Fund?,1636773748.0,2021-11-13 04:22:28
[D] Can Unity3D ML-Agents use GridSearch?,1,qsjngz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsjngz/d_can_unity3d_mlagents_use_gridsearch/,1,"Hello,

I am trying to find a way to do gridsearch for hyperparameter tuning for reinforcement learning under the Unity3D machine learning platform.

I googled but it feels like no options are available right now.

If there is alternative ways, please share! Thank you, guys!",1636747703.0,2021-11-12 21:08:23
[N][CfP] AI for Design and Manufacturing Workshop (ADAM) @ AAAI 2022 (Deadline Extended),2,qsebrh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsebrh/ncfp_ai_for_design_and_manufacturing_workshop/,0,"Hello [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)!

The deadline for submitting to the AI for Design and Manufacturing Workshop (ADAM) @ AAAI 2022 has been extended for a week due to multiple requests. If you're working in the intersection of AI and design, manufacturing, scientific computing, and geometric modeling, do consider submitting a 4-page workshop paper. We invite paper submissions on the following (and related) topics:

* New theory and fundamentals of AI-aided design and manufacturing,
* Novel AI-based techniques to improve modeling of engineering systems,
* Integration of AI-based approaches with engineering prototyping and manufacturing,
* Novel methods to learn from scarce/sparse, or heterogenous, or multimodal data,
* Novel ML methods in the computational material and physical sciences,
* Novel ML-accelerated optimization for conceptual/detailed system design,
* Novel AI-enabled generative models for system design and manufacturing,
* ML-guided rare event modeling and system uncertainty quantification,
* Development of software, libraries, or benchmark datasets, and
* Identification of key challenges and opportunities for future research.

Workshop website:Â [https://adam-aaai2022.github.io/](https://adam-aaai2022.github.io/)

Submission website:Â [https://openreview.net/group?id=AAAI.org/2022/Workshop/ADAM](https://openreview.net/group?id=AAAI.org/2022/Workshop/ADAM)

Submission deadline: November 19th, 2021",1636732892.0,2021-11-12 17:01:32
[D] Why do CNN Kernel weights reach high values?,3,qse2ov,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qse2ov/d_why_do_cnn_kernel_weights_reach_high_values/,2,"I've recently read a bunch of literature about network pruning. A common criteria in the field is to select kernels that are to be removed by their L1 Magnitude (e.g. \[1\], \[2\]), as the heuristic apparently catches relevant kernels quite well.

Most often the time the CNNs are trainied with some form of weight decay. This is (IMHO) intended to regularize the model and prevent single kernel weights from dominating the entire set of parameters and distribute relevance across multiple channels. Also it is normal to add BatchNorm to the architecture as it stabilizes the training procedure.

I'd argue that:

1. The only relevant thing for detecting a pattern is the relative weight between the kernel weights. The absolute value does not matter as it will only change the value range. (CNN Kernels are a Matrix multiplication and therefore a single scaling factor would do the same as scaling the entire thing.).
   1. As the kernel is followed by a BatchNorm the values get automatically scaled and zero-meaned before getting scaled and shifted, so there is less reason for that. (When using Conv-Bn-ReLU ordering)
2. With any weight penalty this should lead to continuously & slowly decreasing values of the kernel weights (Except in the BatchNorms which is way more ""penalty efficient"" than a kernel.

The only reason I could think of is that one will run into some numeric stability issues when one approaches the minimum resolution of the data format (i.e. float16/32). Maybe this introduces some sort of noise into the optimization process?

However as the evidence shows that we get higher weights I have to be wrong and would be happy to get shown where my thought process breaks down.

\[1\] [Comparing Rewinding and Fine-tuning in Neural Network Pruning](https://arxiv.org/pdf/2003.02389.pdf)

\[2\] [Learning efficient convolutional networks through network slimming.](https://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf)",1636732211.0,2021-11-12 16:50:11
[Discussion] Is your data quality suffering because of a first-mile reliability problem?,0,qsigso,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsigso/discussion_is_your_data_quality_suffering_because/,0,"If your data product is fueled by tens to hundreds of external data sources, then this may be relevant to you. When schema changes, volume anomalies, late-deliveries plague the first mile, they go on to infect your downstream warehouse tables and business processes. When the reliability of all those data sources are questionable, they cascade into points of failure that are out of your data teamâ€™s control & awareness.

If you'd like to learn how to improve your data's first-mile reliability, check out [**our latest blog post here**](https://databand.ai/blog/data-supply-chain/?utm_source=forum&utm_medium=r&utm_group=ml)**.**",1636744328.0,2021-11-12 20:12:08
[P] Create semantic search applications with machine-learning workflows,5,qs98gv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qs98gv/p_create_semantic_search_applications_with/,0,"&#x200B;

https://i.redd.it/i3nfnfbsg5z71.gif

Create semantic search applications with machine-learning workflows. The demo above shows how various NLP pipelines can be connected together to build a semantic search application.

txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications. txtai has support for processing both unstructured and structured data. Structured or tabular data is grouped into rows and columns. This can be a spreadsheet, an API call that returns JSON or XML or even list of key-value pairs.

Some example workflows:

* Summarize news articles
* Summarize and translate research papers
* Load and index data via a CSV
* Schedule a recurring job to query an API and index results for semantic search

References:

[Live Demo](https://huggingface.co/spaces/NeuML/txtai)  
[GitHub](https://github.com/neuml/txtai)  
[Article](https://towardsdatascience.com/run-machine-learning-workflows-to-transform-data-and-build-ai-powered-text-indices-with-txtai-43d769b566a7)  
[Notebook](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb)  
[Notebook](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/22_Transform_tabular_data_with_composable_workflows.ipynb)",1636716646.0,2021-11-12 12:30:46
[D] Adversarial Loss understanding in Total Relighting: Learning to Relight Portraits for Background Replacement paper,2,qsaonk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsaonk/d_adversarial_loss_understanding_in_total/,0,"Hi. I hope to get some help understanding Google's [Total Relighting: Learning to Relight Portraits for Background Replacement](https://augmentedperception.github.io/total_relighting/total_relighting_paper.pdf) paper. I'm stuck on the adversarial loss. In 4.1 Paper says:

>we add an adversarial loss on the face region to help the network learn to plausibly remove high-frequency shading effects from the input image while maintaining image detail. We use a least squares discriminator \[Mao et al. 2017\] disc^(alb) to add a loss between a crop of the face from the ground truth albedo ð´crop\_gt and a matching crop of the face from the predicted albedo ð´crop

Predicted albedo ð´crop is the face crop of the output of U-Net like network. No other details provided and I'm struggling to understand the setup here. Since in the paper they're using both ground truth and predicted albedo as the inputs for the loss, I can imagine two scenarios:

1. Use the loss from the original LSGAN paper and train the discriminator during the model training, which seems counterintuitive to me
2. Use L1 or L2 distance between pretrained discriminator's output for ground truth and prediction, but that is not really adversarial loss I guess.

I have no experience with such discriminator usage and therefore can't choose between these two or come up with something else reasonable. 

Is there a common way to use GAN discriminator for the loss calculation of ""non-GAN"" networks? Because from the paper it sounds like something that doesn't require deeper explanation",1636722057.0,2021-11-12 14:00:57
[R] Palette: Image-to-Image Diffusion Models,24,qry8i4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qry8i4/r_palette_imagetoimage_diffusion_models/,1,"website: [https://iterative-refinement.github.io/palette/](https://iterative-refinement.github.io/palette/)

paper: [https://arxiv.org/abs/2111.05826](https://arxiv.org/abs/2111.05826)

Samples:

&#x200B;

https://preview.redd.it/n7cseo2ty1z71.png?width=1172&format=png&auto=webp&s=f0a4068a59a2426ce01313e21698a03aede0cb7c

&#x200B;

https://preview.redd.it/glyaejury1z71.png?width=1172&format=png&auto=webp&s=9f70bc1640c487d607db98f8b62b45ede6af14f9",1636674082.0,2021-11-12 00:41:22
"[R] Invitation to participate in study ""The Labour of Ethical AI""",0,qsdnc2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsdnc2/r_invitation_to_participate_in_study_the_labour/,0,"This is an invitation to participate in a study titled â€œThe Labour of Ethical AIâ€. My name is James Steinhoff and I am a postdoctoral fellow at the Institute of Communication, Culture, Information and Technology at the University of Toronto Mississauga. The aim of this research is to understand the labour that goes into ethical AI research (research intended to promote ethical, responsible, democratic, human-centered, non-profit or socially beneficial AI), the organizations in which such work is conducted, the problems facing workers in this field, and how the field is connected to industry, academia and government. The goal of the study is to interview people who work, study or intern in ethical AI in order to gain empirical insight into the working conditions in this sector. 

If you have experience working in the sector, you could provide great insight into some of the challenges and opportunities facing workers. If you are willing, your participation would involve meeting me for an online interview that will last approximately one hour, where I will ask you questions about your working conditions, why you chose the ethical AI sector and the promises and problems facing that sector. Participation in this project will be confidential. I will supply potential participants with an informed consent form that outlines in greater detail the project and the parameters of participation.

I am happy to answer any questions or concerns you might have.

Thank you for your time and consideration.

Sincerely,

James Steinhoff, PhD @ j.steinhoff@utoronto.ca

Postdoctoral Fellow

Institute of Communication, Culture, Information and Technology

CCT Building 3071

3359 Mississauga Rd.

Mississauga, ON

L5L 1C6",1636731023.0,2021-11-12 16:30:23
[p] Alphafold 2.1.1 (Without Docker),15,qryaz4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qryaz4/p_alphafold_211_without_docker/,4,"Want to fold monomers AND multimers using Alphafold2...without the need for Docker? Look no further than my fork of DeepMind's Alphafold repository that removes all Docker dependencies. Enjoy! #AlphaFold #docker 

https://github.com/amorehead/alphafold\_non\_docker",1636674314.0,2021-11-12 00:45:14
"[D] Can data scientists still make a better predictive model using their talents, by the evidence? Will script kiddies increasingly take over because of great software and AI that does more and more of it very well?",0,qsc6y3,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qsc6y3/d_can_data_scientists_still_make_a_better/,7,"Having studied and applied data science and machine learning since 2014, I am startled by the high quality free tools that are coming online in the past week, month, and year or two, that I have tried. They are that good.  I am talking about automatic model selection, automatic tuning, and NLP that is just breaking away from the old limitations and old ways of doing projects.

See, on youtube lately I am watching all these ""data scientists"" posting videos where all they do is run the software that Microsoft and Google made, and call it a day.  They don't even change the default settings and they add nothing of their own talents (if they have them).  

And the results are pretty good, by golly. That's the thing. 

 Automatic feature engineering is one of the hallmarks of the top deep neural networks. People have spent careers in linguistics and computer vision, hand-making parts and hand-curating mathematical techniques that are now often completely bypassed by today's ML techniques baked into free software, and that's just one example of many.

It's not a trivial question. 

There's more to data science than making a predictive model on a canned, fixed public dataset like Iris, Titanic, Jewellery, or even Higgs Boson. There's also MLOps, exploratory data analysis, study design, visualization, data wrangling, data quality assurance, data life cycle, and many other areas. Data science has wide scope and many specialties.  Predictive model making is only about 5 percent of the whole ML and DS job now, according to a presentation I saw yesterday.

But modeling might be about to be fully automated soon. Can you as a ML engineer or data scientist, bring your wide and deep talents to bear, to actually show that you can make a model better than a script kiddie on predictive performance on any -- any at all -- well known open dataset?

Is making predictive models a fully automated task now?",1636726869.0,2021-11-12 15:21:09
[D] Calling out the authors of 'Trajformer' paper for claiming they published code but never doing it,529,qrbkc7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrbkc7/d_calling_out_the_authors_of_trajformer_paper_for/,94,"I read a paper from NeurIPS 2020 titled 'Trajformer: Trajectory Prediction with Local Self-Attentive Contexts for Autonomous Driving'. I found it interesting and the authors claim multiple times in the paper that 'we release our code at '[https://github.com/Manojbhat09/Trajformer](https://github.com/Manojbhat09/Trajformer)'. Turns out they never did, fine, I thought perhaps they will in the future and starred the repo to check it out later.

Many others raised issues asking for update on code release and they never replied. Finally, it April they update the readme to say that they will release the code and that's been the last update.

I know this is a common trend in ML papers now, but what sucks is that I emailed the authors (both the grad student and the PI) multiple times asking for an update an they never replied. Their paper is literally based on empirical improvements and without working code to replicate the results it is their word against mine.

I strongly think things have to change, and I believe they only will if we call them out. I waited long enough, and made significant effort to contact the authors with no response. I mean I don't mind them not releasing their code, but at least don't claim that you did in the paper/review phase and then disappear. An undergrad in my lab asked why she should take time to clean up the code and document it before release while others just move on to the next interesting project and I don't have an answer. ",1636600691.0,2021-11-11 04:18:11
[D] What must every PhD doing ML know before graduating,8,qrwuvy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrwuvy/d_what_must_every_phd_doing_ml_know_before/,4,"I am a 3rd year PhD who finally finished all program requirements (classes etc...) and am fully focused on research. My question is, what are some things that your average ML PhD should be good at at this point? I know its subjective and depends on their research field but what are some common things that I should be well versed in?",1636669986.0,2021-11-11 23:33:06
[R] A Survey on Green Deep Learning,1,qs5f6h,MachineLearning,https://arxiv.org/abs/2111.05193,4,,1636699358.0,2021-11-12 07:42:38
[D] Why do we have to discretize the data before we use mask prediction for representation learning?,6,qrte64,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrte64/d_why_do_we_have_to_discretize_the_data_before_we/,0,"In fields as vision or speech, a lot of recent papers learn representations by masking parts of the input and predicting the original. To do this, they often refer to masked language modeling (MLM) in BERT-style pre-training and say that we must first discretize the continuous input into discrete tokens  (e.g. using a VQ-VAE) before making predictions (i.e. classification-task over possible tokens in the dictionary).

One of the argument is that the predicting discrete tokens allows the model to learn high-level concepts whereas making prediction in the original input-space (e.g. raw pixels) will force the model to learn high-frequency/low-level details that are not useful for representation learning/compression, and is also computationally prohibitive (since the original input-space is likely very high-dimensional). However my question is why can't we do regression in a continuous latent space for the masked positions (e.g. predicting the latent representation of a learned continuous VAE for the masked positions) instead of classification in a discrete latent space (e.g. predicting the discrete tokens of a learned VQ-VAE for the masked positions)? Is there any theoretical advantage to using discrete tokens instead of continuous latents?",1636660160.0,2021-11-11 20:49:20
[R] Generalization in Dexterous Manipulation via Geometry-Aware Multi-Task Learning,1,qrynlx,MachineLearning,https://arxiv.org/abs/2111.03062,3,,1636675554.0,2021-11-12 01:05:54
[P] Integrate your ML model with your favorite apps from a single Python file,7,qrpx36,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrpx36/p_integrate_your_ml_model_with_your_favorite_apps/,4,"Hi all,

Many here can build a simple Machine Learning model to predict whether a customer will *churn* if they get a nice `pandas.DataFrame` with customer data. However, it gets really complicated if you want this model **deployed** and **integrated** in production, say a procedure as such:

1. Pull data from a new customer from **Shopify**
2. Predict for this customer whether they will *churn*
3. If we predict `CHURN == True`
4. Send a discount code to this customer with **Mailchimp**

Suddenly we have to code data integrations, ETL pipelines, deploy our original Machine Learning solution, spin up an HTTP server etc.; a huge pain indeed...

We are building [a framework](https://flow.magicsheets.io) that takes care of exactly all the boring stuff described above. We really believe this will **bridge the gap between research and real-world ML**.

Super excited to share this with all of you, please let me know if you have comments or feedback!",1636650680.0,2021-11-11 18:11:20
[R] Gradients are Not All You Need,47,qre2vo,MachineLearning,https://arxiv.org/abs/2111.05803,13,,1636609206.0,2021-11-11 06:40:06
[P] Fine-tuning and running GPT-J made easy,1,qs0eup,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qs0eup/p_finetuning_and_running_gptj_made_easy/,1,"Have you guys seen Eleutherâ€™s GPT-J for NLP yet? I feel like itâ€™s on par with OpenAIâ€™s Curie. It's pretty good overall for generic language generation, but you still need to fine-tune it for custom tasks, so I ended up putting together this project to simplify fine-tuning and deployment to production after.

Both can be done through a web interface. Also, I added a default pre-trained GPT-J to use through an interface or API too. Please, check it out and give me feedback if you can. Thanks!

Project: https://www.tensorbox.ai",1636681321.0,2021-11-12 02:42:01
[D] How much VRAM and RAM do I need for NLP transformer models?,5,qrnozu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrnozu/d_how_much_vram_and_ram_do_i_need_for_nlp/,10,"I'm a PhD student looking for a new desktop because my current (personal) PC has an AMD GPU.

When I train a pre-trained BERT model using my CPU (which takes forever) I assume that it is using RAM. Online, I often read about transformer models using VRAM.

So my question: does training transformer models exclusively use VRAM, or do I also need sufficient RAM? And how much would be needed (minimum) to work with such models?

(I am aware of Google Collab but it is not suitable for my work)",1636644422.0,2021-11-11 16:27:02
[D] Article: An introduction to Language Models in NLP,0,qrw5pb,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrw5pb/d_article_an_introduction_to_language_models_in/,0,"Hey! We're working on an intro series to language models. Would love any feedback on this first installment :)

[https://www.surgehq.ai/blog/an-introduction-to-language-models-in-nlp-part-1-intuition](https://www.surgehq.ai/blog/an-introduction-to-language-models-in-nlp-part-1-intuition)",1636667997.0,2021-11-11 22:59:57
"[P] Cedille, the largest French language model (6b), released in open source",167,qqzuh0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqzuh0/p_cedille_the_largest_french_language_model_6b/,25,"## [ðŸ“ DEMO](https://app.cedille.ai) / [ðŸ“˜ REPO](https://github.com/coteries/cedille-ai)

We have spent the last 3 months of our lives, teraFLOPs of compute and gone through 300gb of text to bring you Cedille:

> **Ce que j'aime quand je mange une baguette, c'est** quand celle-ci est craquante.
Je ne saurais dÃ©finir le terme ""craquant"" mais je sais que lorsque c'est le cas, je peux Ãªtre sÃ»re que la baguette est bonne.

The entirety of French spirit captured in measly 6B parameters! ðŸ‡«ðŸ‡·ðŸ¥–

More seriously, we are super excited to share Cedille, the so far largest French language model: https://en.cedille.ai

You can play with it right now on our playground (as long as servers hold ðŸ˜…) : https://app.cedille.ai

We are proponents of â€œopen AIâ€ and as such have released a checkpoint for the world to use (MIT license) : https://github.com/coteries/cedille-ai

Another aspect we had fun with is dataset filtering. We have run the [whole C4 French dataset](https://github.com/allenai/allennlp/discussions/5265) through the Detoxify classifier to clean it up ðŸ¤¬

Some acknowledgements :

* Cedille is based on GPT-J, the 6b model developed by the wizards at EleutherAI: https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/
* Cedille was also generously supported by the Google TFRC program: https://sites.research.google/trc/about/",1636566313.0,2021-11-10 18:45:13
[D] Handling bound constraints in CMA-ES,1,qrvasc,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrvasc/d_handling_bound_constraints_in_cmaes/,1,"Hi all,

Apologies if the question is a bit naive as I only have passing familiarity with ML (full stack developer by trade). I'm really just looking for more insight into the implementation and consequences of bound constraints with CMA-ES optimization. I did come across ([Biedrzycki, 2019](https://staff.elka.pw.edu.pl/~rbiedrzy/publ/RBSWEVO_CMA_BCHMs.pdf))  which discusses this topic, and a lot of the paper's findings at least come across as fairly natural, but I'm not finding a lot out there and I'm just wondering whether anybody else has any recommendations for further reading or personal stories of potential pitfalls. In the problem I'm dealing with specifically, each of the coordinates in my mutant/search point vectors need to be bound within the unit interval and I wanted to make sure I wasn't walking into any easily preventable mistakes.

**Background Context**:  I'm currently working on a personal project (inspired by this popular [Geijtenbeek, et al., 2013](https://www.goatstream.com/research/papers/SA2013/index.html) paper) where I've implemented a Hill-type muscle model and rigged up a humanoid model with some 200+ ""muscles"" approximating human skeletal muscle function. Now I'm beginning muscle control experiments with inspiration from this paper ([Wochner, et al., 2020](https://www.frontiersin.org/articles/10.3389/fncom.2020.00038/full)), except I'm plugging in CMA-ES instead of the Bayesian optimization used in the paper while utilizing the objective function insights.",1636665512.0,2021-11-11 22:18:32
[D] replacement for SoftMax when you want to activate multiple samples equally?,3,qrn5qb,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrn5qb/d_replacement_for_softmax_when_you_want_to/,4,"Softmax has been used for activating/normalizing a representation in way that the most important sample will have the largest value between \[0,1\] and the rest will be close to 0.

I am wondering what if we want to activate multiple samples equally in the representation. Lets say I have a vector of \`1x10\` dimension, and I know 5 of them are equally important and I want them to have same weight after activation, so softmax wont work here, one option is to use sigmoid  and then use softmax on the top of it to make sure the sum is one. but I am not sure if this is the smartest approach. So I was wondering if people here have any other suggestion ?",1636642863.0,2021-11-11 16:01:03
[D] Fast CC Taylor Transform for Computer Vision: Potentially train NeRF in matter of minutes,59,qr3b6w,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr3b6w/d_fast_cc_taylor_transform_for_computer_vision/,14,"Hello!

We just released our latest paper on arXiv. We explore the idea of fast summation algorithms (equivalent of the Fast Fourier Transform but with Taylor series instead) in the context of Computer Vision. In terms of computational complexity, our approach disentangles the number of model parameters (N) and model evaluations (M), i.e. our approach is in O(N+M) instead of O(NM). This allows us to reduce the number of FLOPs required for training and inference 150-200x depending on the problem.

Unfortunately, our current implementation is very FLOP inefficient and a FLOP efficient implementation requires a good bit of custom CUDA code. We are working on it :)

Video abstract: [https://www.youtube.com/watch?v=e6gXoMA5te4](https://www.youtube.com/watch?v=e6gXoMA5te4)

Paper link: [https://arxiv.org/abs/2111.00110](https://arxiv.org/abs/2111.00110)

Let me know if you have any questions!",1636575898.0,2021-11-10 21:24:58
[Research] .wav dataset for morse code,1,qrpwsn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrpwsn/research_wav_dataset_for_morse_code/,6,"Does anyone know where to obtain a dataset containing morse code .wav files.  I checked Kaggle, there was a competition once, but the data is no longer available. \[Research\]",1636650660.0,2021-11-11 18:11:00
[D] Landmark annotations in Blender,3,qriz01,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qriz01/d_landmark_annotations_in_blender/,5,"I am building a synthetic dataset of images for a landmark prediction task and I'm using Blender.

Having looked through the main data generation libraries available for Blender on Github (vision\_blender, BlenderProc, zpy...) I can't find any that support landmarks. Before I go and implement this myself, does anyone have any pointers that I'm missing?

Thanks

*Update*

The following script will write out the coordinates of vertices in a rendered image

```
import bpy
scene = bpy.data.scenes['Scene']
camera = bpy.data.objects['Camera']
obj = bpy.data.objects['Cube']

matrix = camera.matrix_world.normalized().inverted()
"""""" Create a new mesh data block, using the inverse transform matrix to undo any transformations. """"""
mesh = obj.to_mesh(preserve_all_data_layers=True)
mesh.transform(obj.matrix_world)
mesh.transform(matrix)

"""""" Get the world coordinates for the camera frame bounding box, before any transformations. """"""
frame = [-v for v in camera.data.view_frame(scene=scene)[:3]]

lx = []
ly = []

for v in mesh.vertices:
    co_local = v.co
    z = -co_local.z

    if z <= 0.0:
        """""" Vertex is behind the camera; ignore it. """"""
        continue
    else:
        """""" Perspective division """"""
        frame = [(v / (v.z / z)) for v in frame]

    min_x, max_x = frame[1].x, frame[2].x
    min_y, max_y = frame[0].y, frame[1].y

    x = (co_local.x - min_x) / (max_x - min_x)
    y = (co_local.y - min_y) / (max_y - min_y)

    lx.append(x)
    ly.append(y)
    
coords = [f""({x}, {y})\n"" for x, y in list(zip(lx, ly))]
with open(""log.txt"", ""w"") as f:
    f.writelines(coords)

```",1636628976.0,2021-11-11 12:09:36
[D] What is the current state of deep learning theory?,23,qr6hhb,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr6hhb/d_what_is_the_current_state_of_deep_learning/,9,"As I assume many people on this subreddit are, I'm interested in understanding why currently state of the art models work, and if we can use the understanding of the underlying princples of these models to create better more powerful ones. I mean this in the sense that physics has theory or classical computer science has one, in that both develop a mathematical framework to understand and predict their subject (there is of course a difference between them as one assumes laws from experiences and onr is more firmly rooted in math, but both still use a mathematical approach). 

I sometimes see some papers on this subreddit that look very relevant to this vmatter, and while I try to read some of them, given my current math education, which is somewhere between a practioneer and a BA student I do not believe I can truly understand and therefore judge them.

So for those of you who are experts in this, do we have an accepted predictive theory for deep learning models? If not, what do we have?",1636584690.0,2021-11-10 23:51:30
[D] Paper Explained - Autoregressive Diffusion Models (Full Video Walkthrough),20,qr5per,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr5per/d_paper_explained_autoregressive_diffusion_models/,1,"[https://youtu.be/2h4tRsQzipQ](https://youtu.be/2h4tRsQzipQ)

Diffusion models have made large advances in recent months as a new type of generative models. This paper introduces Autoregressive Diffusion Models (ARDMs), which are a mix between autoregressive generative models and diffusion models. ARDMs are trained to be agnostic to the order of autoregressive decoding and give the user a dynamic tradeoff between speed and performance at decoding time. This paper applies ARDMs to both text and image data, and as an extension, the models can also be used to perform lossless compression.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

3:15 - Decoding Order in Autoregressive Models

6:15 - Autoregressive Diffusion Models

8:35 - Dependent and Independent Sampling

14:25 - Application to Character-Level Language Models

18:15 - How Sampling & Training Works

26:05 - Extension 1: Parallel Sampling

29:20 - Extension 2: Depth Upscaling

33:10 - Conclusion & Comments

&#x200B;

Paper: [https://arxiv.org/abs/2110.02037](https://arxiv.org/abs/2110.02037)",1636582575.0,2021-11-10 23:16:15
[D] What are your favourite annotation platforms?,0,qrknk3,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrknk3/d_what_are_your_favourite_annotation_platforms/,4,"I have tried a few annotations platforms:

-Redbrick.AI

-V7 Labs

-Supervise.ly


None of them quite hit the sweet spot for my use-case. I mainly care about uploading pre-annotations of a semantic segmentation network to be double checked by human annotators.

The aforementioned platforms do offer this service but it involves fiddling with their respective SDKs and doesn't always work that well.

Can anyone share their favourite annotation platforms and why?",1636635174.0,2021-11-11 13:52:54
"[R] Microsoft India Proposes Varuna: Scalable, Low-Cost Training of Massive Deep Learning Models",45,qqxcgt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqxcgt/r_microsoft_india_proposes_varuna_scalable/,8,"A Microsoft Research India team presents Varuna, a system for training massive deep learning models on commodity networking that eliminates the need for specialized hyperclusters and alleviates the cost, scale, and resource utilization challenges of deep learning model training. 

Here is a quick read: [Microsoft India Proposes Varuna: Scalable, Low-Cost Training of Massive Deep Learning Models.](https://syncedreview.com/2021/11/10/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-142/)

The Varuna code has been open-sourced and is available on the projectâ€™s [Github](https://github.com/microsoft/varuna). The paper *Varuna: Scalable, Low-cost Training of Massive Deep Learning Models* is on [arXiv](https://arxiv.org/abs/2111.04007).",1636559400.0,2021-11-10 16:50:00
[D] Is anyone working on code generation other than OpenAI?,4,qravhd,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qravhd/d_is_anyone_working_on_code_generation_other_than/,7,Are there any other works in code generation models like Codex from OpenAI? I have seen some open source version of Codex but no variants of that model.,1636598475.0,2021-11-11 03:41:15
[P] List of ICLR 2022 Papers with Review Scores,20,qqzpf6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqzpf6/p_list_of_iclr_2022_papers_with_review_scores/,2,"ICLR 2022 reviews are publicly available now. We have compiled a list of papers sorted by the review scores (weighted by confidence).

**Link:** [https://papers.labml.ai/papers/iclr\_2022?sort\_by=conference\_score&dsc=0](https://papers.labml.ai/papers/iclr_2022?sort_by=conference_score&dsc=0)

**Mean review score is  4.9**

https://preview.redd.it/zhr30oj7xsy71.png?width=1676&format=png&auto=webp&s=1406196f52d50eb8d49b5d36b9b726704771576a",1636565909.0,2021-11-10 18:38:29
[D] What are the advances on encryption?,0,qrhh89,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrhh89/d_what_are_the_advances_on_encryption/,7,"I'm new to the topic, I understand that it's easy for AI find a valid result from a predictable algorithm like a basic letter+letter encryption with a small dataset (I saw it as an example in a lecture)

But how about more complicated encryption algorithms? 16bits? 32bits? 256bits? TLS? How big would the dataset get to be able to easily predict a correct unencrypted text from encrypted text?

Can AI be used to test if an algorithm is actually safe by solving it without really knowing how exactly the data was encrypted? For example if an encryption standard that looks very safe to experts but has an unknown mathematical shortcut AI could find it, is that possible? 

Where can I find more about this topic?",1636622767.0,2021-11-11 10:26:07
[D] What are simple projects I can do with OpenAI?,0,qrm0o7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qrm0o7/d_what_are_simple_projects_i_can_do_with_openai/,2,I'm thinking about making a SQL query generator from English language. Any other ideas are welcome. Thanks.,1636639462.0,2021-11-11 15:04:22
[D] Advice on buying PC,44,qqqzmu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqqzmu/d_advice_on_buying_pc/,76,"I'm doing a PhD in ML and have decided I need a better set-up computationally. Currently I'm running most scripts either locally on my Dell XPS 13 (no GPU) or on Colab, but evidently both are insufficient. As there's a budget in my PhD studentship for equipment, I'm thinking of buying a desktop that I can SSH into from my laptop. Does anyone have experience with this, and if so, recommendations (e.g. for GPU/CPU specs)?

Info:

* Area of research not yet set, though I will definitely be working with generative models (though not huge models; only tabular and time-series data). Work often involves data preprocessing steps that might mainly require CPU power.
* No fixed budget, though I'll need to justify my purchase

It seems buying a moderately good gaming PC with Nvidia GPU is financially and computationally interesting.

Any help is appreciated!

&#x200B;

EDIT: Budget-wise I was thinking below 2000Â£ to be able to justify it to my funding body, but if this is too limiting please let me know.",1636538365.0,2021-11-10 10:59:25
[D] How to avoid CPU bottlenecking in PyTorch - training slowed by augmentations and data loading?,9,qr0rck,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr0rck/d_how_to_avoid_cpu_bottlenecking_in_pytorch/,20,"Hello

My colleague and I are training models on a few workstations and we are noticing some bottlenecks that are not leveraging all our GPUs and stopping us from reaching full performance. We are curious what techniques folks use in Python / PyTorch to fully make use of the available CPU cores to keep the GPUs saturated, data loading or data formatting tricks, etc.

Firstly our systems:

* 1 AMD 3950 Ryzen, 128 GB Ram 3x 3090 FE - M2 SSDs for Data sets
* 1 Intel i9 10900k, 64 GB Ram, 2x 3090 FE - M2 SSDs for Data Sets

We notice that both of our systems take the same amount of time per epoch - ie - we get no gains with 3 GPUs vs 2 GPUs, which is frustrating.

Some things we are observing:
* CPUs on both systems spike to 100% CPU on occasion but aren't always utilized
* Disk throughput via IOTOP shows around 50 - 55 MB/s max read, which is way below SSD speeds. Surprisingly low.
* GPU usage is very spikey. 

[Here's an image of NVTop and HTop for both systems](https://imgur.com/a/MhFuISB)

Some things we are doing: 

* We are using PyTorch 1.10
* Pillow-Simd and the latest Nvidia NGC containers. We also use PyTorch Lighting for training. 
* [We follow most of the best practices here](https://tigress-web.princeton.edu/~jdh4/PyTorchPerformanceTuningGuide_GTC2021.pdf)
* We are setting to gradient to none instead of zero grad for performance small improvements
* We are setting cu-DNN auto-benchmark to true
* We are using the Distributed Data Parallel accelerator 
* We are using Pinned Memory.
* We are using num_workers = 8
* We see this behavior of low GPU usage / without augmentations.
* We've reduced batch size as an experiment to see where the issues lie. We are at 1/3rd max possible batch size - and we see maybe a 10-20% difference in performance 

Some things we have observed:

* We get intermittent crashing if we increase num workers above 8 - 
* We've noticed GPU 0 on our 3 GPU system is sometimes idle (which would explain performance differences). However its unclear to us why that may be. [Similar to this issue](https://github.com/PyTorchLightning/pytorch-lightning/discussions/2701)

Our guess is image loading and pre-processing appear to be the issue? We aren't entirely sure if we are diagnosing this correctly.

How are folks getting around issues like these? Should we be pre-processing our data set somehow and storing it in a more optimal format? We are relying on Pillow-Simd for image reading, decoding and copying to tensors.

Are there any good pragmatic guides to optimizing training?

Thank you.",1636568821.0,2021-11-10 19:27:01
[R] MIT AI Researchers Introduce â€˜PARPâ€™: A Method To Improve The Efficiency And Performance Of A Neural Network,94,qqmhiz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqmhiz/r_mit_ai_researchers_introduce_parp_a_method_to/,9,"Recent developments in machine learning have enabled automated speech-recognition technologies, such as Siri, to learn the worldâ€™s uncommon languages, which lack the enormous volume of transcribed speech required to train algorithms. However, these methods are frequently too complicated and costly to be broadly used.

Researchers from MIT, National Taiwan University, and the University of California, Santa Barbara, have developed a simple technique that minimizes the complexity of a sophisticated speech-learning model, allowing it to run more efficiently and achieve higher performance.

Their method entails deleting unneeded components from a standard but complex speech recognition model and then making slight tweaks to recognize a given language. Teaching this model an unusual language is a low-cost and time-efficient process because only minor adjustments are required once the larger model is trimmed down to size.

# [Read The](https://arxiv.org/pdf/2106.05933.pdf) [Paper](https://arxiv.org/pdf/2106.05933.pdf) | [Checkout The](https://people.csail.mit.edu/clai24/parp/) [Project](https://people.csail.mit.edu/clai24/parp/) | [5 Min Read](https://www.marktechpost.com/2021/11/09/mit-ai-researchers-introduce-parp-a-method-to-improve-the-efficiency-and-performance-of-a-neural-network/) | [MIT Blog](https://news.mit.edu/2021/speech-recognition-uncommon-languages-1104)

&#x200B;

https://preview.redd.it/rigqmwvd8py71.png?width=2495&format=png&auto=webp&s=5cf9af3adbcbaad5983cffe67f01c9785a278e9b",1636519924.0,2021-11-10 05:52:04
"[D] Mel Spectrum, is it useful for non-speech recognition classification tasks?",4,qr1pex,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr1pex/d_mel_spectrum_is_it_useful_for_nonspeech/,7,"I am working on a long-term project involving the development of a process monitoring program for an additive manufacturing process. I have read a good deal on different techniques used in audio classification, speech recognition, etc. From what I understand, most machine learning models involving audio either use features extracted from the time and frequency representations of the signal or spectrograms as inputs.

When looking into audio feature extraction, I have noticed it is very common to extract frequency-domain features from the mel spectrum. Obviously, the mel spectrum is very useful in speech recognition tasks since it is intended to align with how a human ear perceives sound. My question: is the mel spectrum useful for audio classification or anomaly detection tasks that do not involve human speech? If so, what is the reason it would be more useful than say the standard frequency scale spectrum for a non speech recognition task?

(literature references would be helpful)",1636571402.0,2021-11-10 20:10:02
[D] Can a trained discriminator in Gan be used for multi class classification ?,3,qr44bi,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr44bi/d_can_a_trained_discriminator_in_gan_be_used_for/,1,I have been thinking of using discriminator as a classifier after it is trained once with gan. But I have no idea on how to make discriminator into classifier because discriminator only says if it is fake or real . Any leads please,1636578184.0,2021-11-10 22:03:04
"[R] Rebooting ACGAN: A new GAN that achieves SOTA results and harmonizes with various architectures, adversarial losses, and even differentiable augmentations (Neurips 2021).",12,qqu6xh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqu6xh/r_rebooting_acgan_a_new_gan_that_achieves_sota/,1,"A research team from Pohang University of Science and Technology introduces a new type of ACGAN: the Rebooted Auxiliary Classifier GAN (ReACGAN) to overcome unstable training and poor generation performance of ACGAN.Â 

Here is a quick summary of the paper

* Gradient exploding in the classifier of ACGAN can cause an undesirable training collapse.
* Simply normalizing feature embeddings can resolve the problem.
* Using the normalization technique, we propose the Rebooted Auxiliary Classifier GAN (ReACGAN).
* ReACGAN achieves state-of-the-arts generation results on benchmark datasets.Â 
* ReACGAN harmonizes with various GAN architectures (DCGAN, ResNet, Big ResNet, StyleGAN2), adversarial losses, and differentiable augmentations (ADA, DiffAugment).  


arXiv: [https://arxiv.org/abs/2111.01118](https://arxiv.org/abs/2111.01118)  


https://preview.redd.it/s6zmab0ijry71.png?width=981&format=png&auto=webp&s=2741879d0fb0edc743617caa492fe92403d48612

https://preview.redd.it/0m2zos4zjry71.png?width=1244&format=png&auto=webp&s=b3b43dd51e2ca68569ce44536bab55eaac51bfb8",1636550132.0,2021-11-10 14:15:32
[discussion] Plaforms/Frameworks for Backtesting and Regression Testing Lots of Models?,2,qr6bu6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr6bu6/discussion_plaformsframeworks_for_backtesting_and/,2,"I have a need to test lots of models submitted by different teams.    There will be baseline ""curated"" datasets but there will also be updates as new data comes in from the field.   Models may have different preprocessing requirements. 

  We'll need to retrain models on the updated data, evaluate the models, and archive the reports and models (""configuration management"").    Most of this will probably need to be queued up (something like slurm) along with the need to asynchronously monitor performance of multiple DGX servers.

Are there commercial tools to manage testing of a fleet of models and the data?    (must work offline)",1636584255.0,2021-11-10 23:44:15
[P] Machine Learning tutorial in R,0,qr9ndf,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr9ndf/p_machine_learning_tutorial_in_r/,0,"Does anybody know of a good tutorial that would help me do this in R:

* A model to predict the probability of a home run for a given ball in play
* An explanation of your chosen model features
* A visualization of your model outputs
* Identify the home run that was least likely to be a home run, and the non-home run that was most likely to be a home run. Describe why you think your model classified these plays less accurately than others.

Advice for how to get started/model to choose would be helpful as well but I do not want answers here I want to learn. So far I have a model that has an overall prediction accuracy of 98% (compared to 90% for completely random), and a prediction accuracy of 63% when at least one of the predicted or actual result is home run (3% accuracy for random model). The models I have been using are KNN and random forest",1636594461.0,2021-11-11 02:34:21
"[N] Modernizing Gov Finance Data & Creating Infrastructure for Fed AI Adoption with Justin Marsico, BFS CDO Thursday, November 18, 2021 at 11:30 AM ET",6,qqvsu0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqvsu0/n_modernizing_gov_finance_data_creating/,1,"Hi r/MachineLearning!

I wanted to share an upcoming webinar with you. Below are the details from the website:

**Featured Guest Speaker: Justin Marsico, Chief Data Officer  and Deputy Assistant Commissioner at the Bureau of the Fiscal Service!**

During this presentation learn how the US Treasury's Bureau of the  Fiscal Service is building a better public understanding of federal  finance as they continue to modernize their management of data and data  sharing. Justin Marsico, Chief Data Officer for the Fiscal Service  shares unique opportunities around data at the federal level, what it  entails to create the infrastructure for AI adoption, how they are  recruiting data scientists, as well as the challenges and opportunities  in data governance, security, ownership and related data areas. By  offering clear, accessible information, citizens can see how their  taxpayer dollars are spent and learn about the federal budget. Justin  will give a live demonstration of the latest online resources so  attendees can learn how the Fiscal Service is enhancing data education  and building and enhancing public trust through greater transparency.

Come join us for this great presentation around areas related to:

* Enhancing AI Skills in the Government Workforce/Recruiting Data Scientists
* Finding the most appropriate use cases
* Creating the Infrastructure for AI Adoption

and stick around for Q&A with Justin at the end.

**Agenda:**

11:30-12:30pm: Featured Presentation

12:30-13:00pm: Your Q&A and interaction

Link to website: [https://events.cognilytica.com/CLNDMzMXwxOA](https://events.cognilytica.com/CLNDMzMXwxOA)",1636554957.0,2021-11-10 15:35:57
[P] New open-source vector search solution,7,qqssgq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqssgq/p_new_opensource_vector_search_solution/,2,"Meet our new open-source vector search solution - Vektonn.

We offer an opportunity for product teams and data scientists to solve the problem of reliable vector data storage, scalability, and undisturbed availability.

We store embeddings and their attributes, which are more interesting to users since they can use real-world objects. For example, they can identify objects using their real identification.

We support changing indexes as new data arrives (delete, change, or add data to the index) parallel with search queries.

You can expand multiple indexes over a single data source (vectors and attributes) and seamlessly transition to new versions of indexes. You can expand different indexes with different parameters of the same data.

You can work with vectors of any type. For example, you can use bag-of-words to solve word processing problems and load appropriate sparse vectors into Vektonn.

We'd appreciate any feedback or suggestions for the project and welcome GitHub stars to join in if, of course, you find it interesting. ðŸ™‚

Learn more - [https://vektonn.io/](https://vektonn.io/)

See what we have done - [https://github.com/vektonn](https://github.com/vektonn)",1636545435.0,2021-11-10 12:57:15
[R] Partition and Code: learning how to compress graphs,14,qqpfye,MachineLearning,https://arxiv.org/abs/2107.01952,2,,1636531492.0,2021-11-10 09:04:52
[N] NVIDIA GTC 2021,0,qr5t70,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qr5t70/n_nvidia_gtc_2021/,0,Check out OmniSciâ€™s session at the NVIDIA GTC 2021 for FREE! Learn how BIDMC Dept of Endocrinology is leveraging OmniSciâ€™s GPU accelerated analytics platform to explore massive amounts of transcriptomic data and how that has advanced their research processes. Register here! https://reg.rainfocus.com/flow/nvidia/nvidiagtc/ap2/page/sessioncatalog?search=%22A31341%22&ncid=ref-spo-444344,1636582860.0,2021-11-10 23:21:00
[D] Why does AMD do so much less work in AI than NVIDIA?,358,qq21s6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq21s6/d_why_does_amd_do_so_much_less_work_in_ai_than/,98,"Or is the assumption in the title false?

Does AMD just not care, or did they get left behind somehow and can't catch up?

&#x200B;

I know this question is very vague, maybe still somebody can point to a fitting interview or something else",1636458809.0,2021-11-09 12:53:29
[D] How to train GANs really fast - Projected GANs Converge Faster explained (5-minute summary by Casual GAN Papers),25,qqfe6y,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqfe6y/d_how_to_train_gans_really_fast_projected_gans/,9,"Despite significant progress in the field training GANs from scratch is still no easy task, especially for smaller datasets. Luckily Axel Sauer and the team at the University of TÃ¼bingen came up with a Projected GAN  that achieves SOTA-level FID in hours instead of days and works on even the tiniest datasets. The new training method works by utilizing a  pretrained network to obtain embeddings for real and fake images that the discriminator processes. Additionally, feature pyramids provide multi-scale feedback from multiple discriminators and random projections better utilize deeper layers of the pretrained network.

Full summary: [https://t.me/casual\_gan/181](https://t.me/casual_gan/181)

Blog post: [https://www.casualganpapers.com/data-efficient-fast-gan-training-small-datasets/ProjectedGAN-explained.html](https://www.casualganpapers.com/data-efficient-fast-gan-training-small-datasets/ProjectedGAN-explained.html)

[ProjectedGAN](https://preview.redd.it/vqyv3adpeny71.png?width=1730&format=png&auto=webp&s=f1788f97b3e8b9dc2266792da101c235d48ffa5d)

UPD: I originally included the wrong links  
[arxiv](https://studios.disneyresearch.com/app/uploads/2021/04/Adaptive-Convolutions-for-Structure-Aware-Style-Transfer.pdf) / [code](https://github.com/RElbers/ada-conv-pytorch)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",1636497858.0,2021-11-09 23:44:18
[D] Good Advertising Papers Using RL or DL,3,qquog8,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qquog8/d_good_advertising_papers_using_rl_or_dl/,2,"Hey, everybody

I'm currently working in AdTech and I'm searching for innovative products in advertising (CTR prediction, Digital Inventory Pricing,  Online learning for costumer segmentation or other stuff).

If you can help me, please suggest me a good paper in the comments or where I'd be able to find good papers in those subjects (I've looked into fb research and arxiv mainly).",1636551616.0,2021-11-10 14:40:16
[D] How do you choose an Optimizer? And why are there so many?,52,qq75zu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq75zu/d_how_do_you_choose_an_optimizer_and_why_are/,29,"Choosing an optimizer for the training of ANNS is one of the most critical design choices. Because ANNS are black boxes, the theoretical guidelines on the overall design are very limited. They are mostly anecdotally and strongly depend on the developers experience.

&#x200B;

When it comes to optimizer, there are hunderts available, from SGD to Adam to very specific ones. It feels like there is a custom-tailored optimizer for every problem and every architecture.

&#x200B;

Why is it so hard to come up with something more general? Why are there this many optimizer? And, how do you choose an optimizer for your project?",1636474726.0,2021-11-09 17:18:46
[Discussion] ICLR 2022 submission statistics,8,qqkwas,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqkwas/discussion_iclr_2022_submission_statistics/,0,"The statistics of ICLR2022 submission can be found here:

[https://guoqiangwei.xyz/htmls/iclr2022\_stats.html](https://guoqiangwei.xyz/htmls/iclr2022_stats.html)

https://preview.redd.it/sp68f1sgsoy71.png?width=1918&format=png&auto=webp&s=583d679ac168070e706a508ffba1f232e034dc56

**1.**  **number of reviewers for each submission:**

[**https://guoqiangwei.xyz/htmls/iclr2022\_stats\_number\_of\_reviewers.html**](https://guoqiangwei.xyz/htmls/iclr2022_stats_number_of_reviewers.html)

* **Submissions with maximum # reviewers:** 2 item in total, each with 7 reviewers

https://preview.redd.it/fknneqvqlqy71.png?width=1612&format=png&auto=webp&s=782f04a03ba15489287c9544ee95d30cacad039e

* **Submissions with minimum # reviewers:** 20+ item in total, each with 2 reviewers

https://preview.redd.it/u7ehgnwulqy71.png?width=1766&format=png&auto=webp&s=f26beb116dc909962719fe7c865026bda9208308

**2. Rating variance**

[**https://guoqiangwei.xyz/htmls/iclr2022\_stats\_variance.html**](https://guoqiangwei.xyz/htmls/iclr2022_stats_variance.html)

* **Submissions with highest rating variance**

https://preview.redd.it/ckbkh5zbmqy71.png?width=1266&format=png&auto=webp&s=46b3d25fa01925e5ee9c9a54b9d581ed52db91b9

* **Submissions with highest rating gap (max - min)**

https://preview.redd.it/mggqs2uemqy71.png?width=1702&format=png&auto=webp&s=ee601ab534eb1bdc9b9199a6164b5480b77821e3

* **Submissions with lowest rating variance**

https://preview.redd.it/adx9lgehmqy71.png?width=1914&format=png&auto=webp&s=b4c2452bbeb1cbe30d0d9fff67b9637edcb5ebfe

&#x200B;

&#x200B;

More data can be found here:

[https://github.com/weigq/iclr2022\_stats](https://github.com/weigq/iclr2022_stats)",1636514526.0,2021-11-10 04:22:06
[N] Message from r/MLOps: Announcing Our first AMA!,16,qqckfw,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqckfw/n_message_from_rmlops_announcing_our_first_ama/,2,"Hi, mod of r/MLOPs here. We finally managed to book our first AMA, which should interest some of the 2M members here.

The following is the content of the sticky over at our sub.
Would you please help us grow our community of MLOps enthusiasts by not burying this post å…«(ï¼¾â–¡ï¼¾*)
------------------------------------------------------------------------------

I don't know if you remember, but we were going to have AMAs here to celebrate the fact that there are so many of us! 
Naturally, since this is a niche subreddit, it wasn't as if top-tier mlops superheroes were lining up to post an AMA here...


But then, a miracle happened! 



I am delighted to announce that [Alessya Visnjic](https://twitter.com/zalessya/status/1457763397106241540?s=20) will be doing an AMA. Here, this Thursday! So spread the word, and let's make this AMA be the first of many successful ones.


And just in case you are not as immersed in the MLOps ecosphere as I am, here is her bio:


>Alessya Visnjic is the CEO and co-founder of WhyLabs, the AI Observability company on a mission to build the interface between AI and human operators. Before WhyLabs, Alessya was a CTO-in-residence at the Allen Institute for AI (AI2), evaluating the commercial potential for the latest advancements in AI research. Earlier in her career, Alessya spent 9 years at Amazon leading Machine Learning adoption and tooling efforts. She was a founding member of Amazon's first ML research center in Berlin, Germany. Alessya is also the founder of Rsqrd AI, a global community of 1,000+ AI practitioners committed to making AI technology Robust & Responsible. 

Of course, there's always an ulterior motive. Alessya will be focusing on the recent announcements by WhyLabs - their round of funding and their new SaaS solution called AI Observatory. 

Personally, I think their corner of the MLOps tooling space is super exciting, and WhyLabs are doing some hard opensource groundwork. Additionally, their marketing is not spammy, so it's an honor to host them on the sub.",1636489878.0,2021-11-09 21:31:18
[D] Motivation for a 3D bounding-box in ADAS,0,qqwa9u,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqwa9u/d_motivation_for_a_3d_boundingbox_in_adas/,0,"One of the common test-sets for ""car recognition"" is called KITTI. Until 2017 they had only a 2D bounding-box test-set, but around that year they moved to a 3D bounding-box test-sets. Can anyone explain what is the motivation behind a 3D bounding-box for ADAS and its features?",1636556399.0,2021-11-10 15:59:59
"""[D]"" Interesting bit of info from HTC Keynote - Nvidia Selene 500 node superpod trained GPT3 in 11 days",8,qqhct3,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqhct3/d_interesting_bit_of_info_from_htc_keynote_nvidia/,1,"I'm not sure the cost to run such a machine, but I'd imagine that represents a big cost and time reduction vs 1 year ago.",1636503654.0,2021-11-10 01:20:54
[D] Virtual MLOps Round Table,35,qq6jgo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq6jgo/d_virtual_mlops_round_table/,1,"I'm putting together an MLOps roundtable focused on peer learning. ML and MLOps practitioners can share how they utilize MLOps to automate and scale their ML processes and learn about how other teams structure theirs.

It works like this: we break people up into small groups of 5-7 people based on their team size and what they are working on. 

There is absolutely no selling or pitching. The focus is pure peer learning.

You can sign up here if you're interested: [https://www.eventbrite.com/e/199371063217](https://www.eventbrite.com/e/199371063217)

Let me know if you have any ideas, thoughts, or feedback.",1636472979.0,2021-11-09 16:49:39
[P] GPT-3 in the style of Shel Silverstein,12,qqdkeo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqdkeo/p_gpt3_in_the_style_of_shel_silverstein/,0,"I've recently been playing with the OpenAI API beta for GPT-3, and used their fine-tuning API to create a model that has been fine-tuned on a corpus of all of Shel Silverstein's poetry. The resulting model generates whimsical poems based on a prompted title.

Here's one example I liked, ""Walking On a Whale""

> I am walking on a whale,

> I feel it move and swell,

> I feel the mist come in and float,

> I feel the rain and the cold.

> But I don't mind at all-

> It's just like walking on the ground.

While the model is quite good at understanding semantics, and even has a flair for the vaguely metaphorical, it didn't pick up any notion of rhyming. It seems like this would be a hard thing for the model to learn given the training data it's seen, since only very rarely does the rhyme scheme affect the conditional likelihood of a word.

Question for any commenters: if you had a rhyming dictionary where you could simply look up whether two words rhyme instead of trying to infer this as some latent attribute, how could you update a large NLP model like this to take into account such declarative knowledge?

More examples and discussion here: http://dean.dog/shel-silverstein-gpt3/",1636492686.0,2021-11-09 22:18:06
[D] ICLR2022 review stats,33,qq5c51,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq5c51/d_iclr2022_review_stats/,0,"I crawled the ICLR2022 preliminary reviews with some help of another repo and uploaded the crawled raw data (crawled today around 2PM UTC+1) .You can also find some quick stats like

* distribution of mean scores etc..
* best paper by mean/median score
* most controversial paper by std of scores

in the following notebook:

[https://github.com/VietTralala/ICLR2022-OpenReviewData/blob/master/analyze\_reviews.ipynb](https://github.com/VietTralala/ICLR2022-OpenReviewData/blob/master/analyze_reviews.ipynb)

Feel free to play around with it âœŒ  

# Excerpt of the data

## best 10 paper by median score 
| paper_id    | title                                                                            | link                                        | keywords                                                                                                                                                               |    mean |   max |   min |      std |   median |   num |
|:------------|:---------------------------------------------------------------------------------|:--------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------:|------:|------:|---------:|---------:|------:|
| LdlwbBP2mlq | Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond       | https://openreview.net/forum?id=LdlwbBP2mlq | Local SGD, Minibatch SGD, Shuffling, Without-replacement, Convex Optimization, Stochastic Optimization, Federated Learning, Large Scale Learning, Distributed Learning | 8       |     8 |     8 | 0        |        8 |     3 |
| iMSjopcOn0p | MT3: Multi-Task Multitrack Music Transcription                                   | https://openreview.net/forum?id=iMSjopcOn0p | music transcription, transformer, multi-task learning, low resource learning, music understanding, music information retrieval                                         | 8       |     8 |     8 | 0        |        8 |     4 |
| BrPdX1bDZkQ | DemoDICE: Offline Imitation Learning with Supplementary Imperfect Demonstrations | https://openreview.net/forum?id=BrPdX1bDZkQ | imitation learning, offline imitation learning, imperfect demonstration, non-expert demonstration                                                                      | 7.33333 |     8 |     6 | 0.942809 |        8 |     3 |
| sOK-zS6WHB  | Responsible Disclosure of Generative Models Using Scalable Fingerprinting        | https://openreview.net/forum?id=sOK-zS6WHB  | Generative models, fingerprinting, responsible disclosure, deep fake detection and attribution                                                                         | 6.4     |     8 |     3 | 2.05913  |        8 |     5 |
| bVvMOtLMiw  | DIVA: Dataset Derivative of a Learning Task                                      | https://openreview.net/forum?id=bVvMOtLMiw  | Leave one out cross validation, AutoML, dataset optimization                                                                                                           | 7       |     8 |     5 | 1.41421  |        8 |     3 |
| lrocYB-0ST2 | Approximation and Learning with Deep Convolutional Models: a Kernel Perspective  | https://openreview.net/forum?id=lrocYB-0ST2 | kernel methods, deep learning theory, convolution, approximation, generalization                                                                                       | 7.5     |     8 |     6 | 0.866025 |        8 |     4 |
| siCt4xZn5Ve | What Happens after SGD Reaches Zero Loss? --A Mathematical Framework             | https://openreview.net/forum?id=siCt4xZn5Ve | SGD, implicit bias, generalization, deep learning, implicit regularization, manifold                                                                                   | 8       |    10 |     6 | 1.41421  |        8 |     4 |
| 0DLwqQLmqV  | NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy                       | https://openreview.net/forum?id=0DLwqQLmqV  | neural architecture search, AutoML                                                                                                                                     | 7.5     |     8 |     6 | 0.866025 |        8 |     4 |
| K0E_F0gFDgA | The MultiBERTs: BERT Reproductions for Robustness Analysis                       | https://openreview.net/forum?id=K0E_F0gFDgA | Pre-trained models, BERT, bootstrapping, hypothesis testing, robustness                                                                                                | 7.33333 |     8 |     6 | 0.942809 |        8 |     3 |
| fKv__asZk47 | Learning Similarity Metrics for Volumetric Simulations with Multiscale CNNs      | https://openreview.net/forum?id=fKv__asZk47 | metric learning, PDEs, numerical simulation, physical modeling                                                                                                         | 6.33333 |     8 |     3 | 2.35702  |        8 |     3 |

---

## 10 most controversial papers by std of scores

| paper_id    | title                                                                             | link                                        | keywords                                                                                                                                                                                   |    mean |   max |   min |     std |   median |   num |
|:------------|:----------------------------------------------------------------------------------|:--------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------:|------:|------:|--------:|---------:|------:|
| p0rCmDEN_-  | Visual hyperacuity with moving sensor and recurrent neural computations           | https://openreview.net/forum?id=p0rCmDEN_-  | visual system, convolutional neural networks, recurrent neural networks, active vision, active sensing, ocular drift                                                                       | 4.75    |    10 |     3 | 3.03109 |        3 |     4 |
| FPGs276lUeq | Palette: Image-to-Image Diffusion Models                                          | https://openreview.net/forum?id=FPGs276lUeq | machine learning, artificial intelligence, computer vision                                                                                                                                 | 4.75    |    10 |     3 | 3.03109 |        3 |     4 |
| SC6JbEviuD0 | White Paper Assistance: A Step Forward Beyond the Shortcut Learning               | https://openreview.net/forum?id=SC6JbEviuD0 | Shortcut Learning, Bias, Classification, Imbalanced Classification, Robustness                                                                                                             | 3.75    |     8 |     1 | 2.94746 |        3 |     4 |
| 7IWGzQ6gZ1D | Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates  | https://openreview.net/forum?id=7IWGzQ6gZ1D | reinforcement learning, lifelong learning, transfer learning, successor features                                                                                                           | 6       |    10 |     3 | 2.94392 |        5 |     3 |
| JGO8CvG5S9  | Universal Approximation Under Constraints is Possible with Transformers           | https://openreview.net/forum?id=JGO8CvG5S9  | Constrained Universal Approximation, Probabilistic Attention, Transformer Networks, Geometric Deep Learning, Measurable Maximum Theorem, Non-Affine Random Projections, Optimal Transport. | 7       |    10 |     3 | 2.94392 |        8 |     3 |
| 3ILxkQ7yElm | Learning Continuous Environment Fields via Implicit Functions                     | https://openreview.net/forum?id=3ILxkQ7yElm | Continuous Scene Representation, Implicit Neural Networks                                                                                                                                  | 5       |     8 |     1 | 2.94392 |        6 |     3 |
| V1MBgNBx5E  | Mask and Understand: Evaluating the Importance of Parameters                      | https://openreview.net/forum?id=V1MBgNBx5E  | influence function, interpretability, model pruning, feature importance ranking                                                                                                            | 4       |     8 |     1 | 2.94392 |        3 |     3 |
| TQ75Md-FqQp | Efficient and Modular Implicit Differentiation                                    | https://openreview.net/forum?id=TQ75Md-FqQp | implicit differentiation, bilevel optimization, autodiff, jax                                                                                                                              | 6.33333 |    10 |     3 | 2.86744 |        6 |     3 |
| MeMMmuWRXsy | Robust Robotic Control from Pixels using Contrastive Recurrent State-Space Models | https://openreview.net/forum?id=MeMMmuWRXsy | contrastive learning, model-based RL, distractions, predictive coding                                                                                                                      | 4.66667 |     8 |     1 | 2.86744 |        5 |     3 |
| kxARp2zoqAk | Information-Aware Time Series Meta-Contrastive Learning                           | https://openreview.net/forum?id=kxARp2zoqAk | Information-Aware Time Series Meta-Contrastive Learning                                                                                                                                    | 6.33333 |    10 |     3 | 2.86744 |        6 |     3 |",1636469545.0,2021-11-09 15:52:25
[P] Community sourced Open Audio Datasets â€“ Hacktoberfest 2021,21,qq5hrq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq5hrq/p_community_sourced_open_audio_datasets/,2,"Hey r/ML! A month ago, [I posted here](https://www.reddit.com/r/MachineLearning/comments/q574t3/d_supporting_hacktoberfest_for_ml_datasets/) about [DagsHub](https://DagsHub.com) supporting Hacktoberfest for ML Datasets. We wanted to do something that was geared towards the ML community, and we decided to create an open-source catalog of ðŸ”Š  audio datasets.

The response has been truly amazing! We received 40 dataset contributions, which are now publicly available, and viewable on DagsHub. They cover various tasks, languages, and sizes, and you can use them all for your projects.

If you want to check out the list of datasets: [https://dagshub.com/blog/hacktoberfest-2021-open-source-audio-datasets/](https://dagshub.com/blog/hacktoberfest-2021-open-source-audio-datasets/). I can't wait to see what everyone builds with these.

A huge **THANK YOU** to everyone who participated! You are what made this possible! The fact that Hacktoberfest is over doesn't mean you can't continue contributing. We'd love to see more datasets, both in the audio domain and others.",1636469988.0,2021-11-09 15:59:48
"Alibaba DAMO Academy Creates Worldâ€™s Largest AI Pre-Training Model, With Parameters Far Exceeding Google and Microsoft (10T parameters) [N]",164,qpuax4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpuax4/alibaba_damo_academy_creates_worlds_largest_ai/,35,"> [According to the company, the M6 has achieved the ultimate low carbon and high efficiency in the industry, using 512 GPUs to train a usable 10 trillion model within 10 days.](https://pandaily.com/alibaba-damo-academy-creates-worlds-largest-ai-pre-training-model-with-parameters-far-exceeding-google-and-microsoft/) Compared to the GPT-3, a large model released last year, M6 achieves the same parameter scale and consumes only 1% of its energy.

Thoughts? The pace of foundational models is starting to get scary, seems like a bigger and bigger model is pushed out every week.",1636427107.0,2021-11-09 04:05:07
[D] ICLR 2022 reviews,23,qq2rbm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq2rbm/d_iclr_2022_reviews/,46,Share your rants.,1636461419.0,2021-11-09 13:36:59
[D] Google AutoML's prices,0,qqopdj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqopdj/d_google_automls_prices/,0,"I'm trying to understand Google AutoML's pricing and I have three questions:

1. What is ""price for forecasting"" here  [https://cloud.google.com/vertex-ai/pricing#tabular-data](https://cloud.google.com/vertex-ai/pricing#tabular-data) ?
2. How much will I pay for having an endpoint available 24/7 to which I can post data and execute previously trained model (assuming simple numerical data with classification)?
3. Can I upload my own model and have it ready for predictions?

Thanks",1636528458.0,2021-11-10 08:14:18
[N] AMD launches MI200 AI accelerators (2.5x Nvidia A100 FP32 performance),230,qphg92,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qphg92/n_amd_launches_mi200_ai_accelerators_25x_nvidia/,69,"Source: https://twitter.com/IanCutress/status/1457746191077232650

More Info: https://www.anandtech.com/show/17054/amd-announces-instinct-mi200-accelerator-family-cdna2-exacale-servers

> For todayâ€™s announcement, AMD is revealing 3 MI200 series accelerators. These are the top-end MI250X, itâ€™s smaller sibling the MI250, and finally an MI200 PCIe card, the MI210. The two MI250 parts are the focus of todayâ€™s announcement, and for now AMD has not announced the full specifications of the MI210.",1636389639.0,2021-11-08 17:40:39
[D] improving segmentation masks,8,qq18tc,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq18tc/d_improving_segmentation_masks/,2,"I have a dataset with some segmentation masks for objects  (or better polygons around the objects)  I am interested in, but the quality is not very good. The polygons around the objects are correct but very rough, a low number of edges with huge chunks of background in there. Is there some algorithmic way to improve those? I tried GrapCut but the performance is not very good, huge chunks of background are still included and stuff like hair is done very poorly.",1636455502.0,2021-11-09 11:58:22
[R] Intel Optimized Facebook DLRM with 8x speedup (Deep Learning Recommendation Model),0,qqcrbh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qqcrbh/r_intel_optimized_facebook_dlrm_with_8x_speedup/,1,"Intel leveraged SigOpt's Hyper Parameter Optimization platform to achieve a software speedup for DLRM. Additionally, Intel leveraged vertical split embedding, LAMB optimization, and parallelizable data loaders.",1636490443.0,2021-11-09 21:40:43
Reward Prediction for Representation Learning and Reward Shaping,5,qq1ae2,MachineLearning,https://www.scitepress.org/Papers/2021/106402/,1,,1636455663.0,2021-11-09 12:01:03
[R] The How and Why of Bayesian Nonparametric Causal Inference,14,qpuqtk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpuqtk/r_the_how_and_why_of_bayesian_nonparametric/,0,"A nice summary paper, at the cutting edge of Bayesian causal inference. 

Link: https://arxiv.org/abs/2111.03897

Abstract: ""Spurred on by recent successes in causal inference competitions, Bayesian nonparametric (and high-dimensional) methods have recently seen increased attention in the causal inference literature. In this paper, we present a comprehensive overview of Bayesian nonparametric applications to causal inference. Our aims are to (i) introduce the fundamental Bayesian nonparametric toolkit; (ii) discuss how to determine which tool is most appropriate for a given problem; and (iii) show how to avoid common pitfalls in applying Bayesian nonparametric methods in high-dimensional settings. Unlike standard fixed-dimensional parametric problems, where outcome modeling alone can sometimes be effective, we argue that most of the time it is necessary to model both the selection and outcome processes.""",1636428613.0,2021-11-09 04:30:13
Landing AI gets $57 million series A to build a data centric MLOps platform. [News],70,qpk1tt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpk1tt/landing_ai_gets_57_million_series_a_to_build_a/,13,"Are data centric MLOps tools about to take off?

[CNBC](https://www.cnbc.com/2021/11/08/google-brain-founder-andrew-ng-raises-57-million-for-landing-ai.html)

[TechCrunch](https://techcrunch.com/2021/11/08/landing-ai-machine-learning-operations-tools/)",1636396790.0,2021-11-08 19:39:50
[D] What does having more than three reviews mean on ICLR 22?,11,qpwdps,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpwdps/d_what_does_having_more_than_three_reviews_mean/,12,"ICLR 22 reviews are in. Hope you guys got good reviews. I noticed that some papers got three reviews, while others got four or five reviews. Why do some papers get more reviews and what does it signify?",1636434371.0,2021-11-09 06:06:11
[D] Evaluating the effectiveness of text generation,0,qq644r,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq644r/d_evaluating_the_effectiveness_of_text_generation/,0,"I'm using GPT3 to generate text based on a Q&A dataset (the data is domain specific, based on data scrapped from various internal company sources). The challenge I am facing is that the quality of the output is somewhat subjective.

This makes it hard to improve the model output - I've easily been able to move beyond outputting gibberish to something which works reasonably well. However, I am not finding it hard to evaluating the effectiveness of minor model changes (e.g. temperature, prompt design, tweaks to the dataset, etc ...)

I'm considering 'crowd sourcing' input from my colleagues, giving them model output (with various tweaks) and asking them to score the results. However, this has obvious limitations!

So, I was wondering if there are techniques that people have developed that make it easier to fine-tune models where the output has a subjective quality?",1636471763.0,2021-11-09 16:29:23
[R] Deep Shallow Fusion for RNN-T Personalization,1,qq4v0g,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qq4v0g/r_deep_shallow_fusion_for_rnnt_personalization/,0,"End-to-end deep learning models for Speech Recognition can produce highly accurate transcriptions, but they are a lot harder to personalize. This paper from Facebook's AI team walks through some methods that help increase the accuracy of proper nouns and rare words from end-to-end deep learning models which I found really interesting.

I made a summary of this paper that [you can read here](https://www.assemblyai.com/blog/deep-shallow-fusion-for-rnn-t-personalization/). 

And the link to the original paper from Facebook AI can be found here -> [https://arxiv.org/abs/2011.07754](https://arxiv.org/abs/2011.07754)",1636468155.0,2021-11-09 15:29:15
[Project] Google MoveNet (Real-Time Pose Estimation) Used To Control Nintendo Punch-Out!!,70,qpenkt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpenkt/project_google_movenet_realtime_pose_estimation/,8,"Hey AI fans, I hacked the original Nintendo Punch-Out!! so that you control it with actual punches! This is a boxing video game that now uses Google's MoveNet (real-time pose estimation) to track your movements and detect punches, blocks, and other moves and then sends those commands to the game.

You can check out the full video here with plenty of sweet MoveNet footage: [https://www.youtube.com/watch?v=07JibJJVNp8](https://www.youtube.com/watch?v=07JibJJVNp8)

And play it yourself here: [https://reallifepunchout.com](https://reallifepunchout.com)  


&#x200B;

https://reddit.com/link/qpenkt/video/swwt8jw9sdy71/player",1636381349.0,2021-11-08 15:22:29
[R] M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining,6,qpu5m7,MachineLearning,https://arxiv.org/abs/2110.03888,3,,1636426619.0,2021-11-09 03:56:59
[Project] JORLDY: OpenSource Reinforcement Learning Framework,108,qp9bra,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qp9bra/project_jorldy_opensource_reinforcement_learning/,14,"Hello WoRLd! We are Reinforcement Learning (RL) engineers at KakaoEnterprise in South Korea! We published an opensource RL framework and named it JORLDY (Join Our Reinforcement Learning framework for Developing Yours). JORLDY is opened for helping RL researchers and students who study RL. The features of JORLDY are as follows.

* 20+ RL Algorithms ([Pytorch](https://pytorch.org/)) and various RL environment are provided
* The algorithms and environments can be run with simple command
* Algorithms and environment can be easily added and customized
* Distributed RL algorithms are provided usingÂ [ray](https://github.com/ray-project/ray)
* Benchmark of the algorithms is conducted in many RL environment

JORLDY github link: [https://github.com/kakaoenterprise/JORLDY](https://github.com/kakaoenterprise/JORLDY)

As we mentioned, JORLDY is an ""open source"" RL framework. Accordingly, our team wants to work with many people to develop JORLDY into a better framework. We would be very grateful if you use it widely and give us a lot of comments about JORLDY.

Thank you!",1636360178.0,2021-11-08 09:29:38
[D] ML datasets for commercial use,0,qpzewi,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpzewi/d_ml_datasets_for_commercial_use/,3,"Hi all,

there are a ton of datasets to ML researchers stemming from different areas. But when looking more closely, the vast majority of them have very restrictive licensing, only allowing to be used for research purposes, but not in a commercial environment.

I am now wondering what the strategies are for obtaining high quality datasets for commercial purposes. So let's say I want to build a car object detection model for my company, one of the most well known detection use cases.

I can neither use any of the public datasets, as they do not allow commercial usage, nor can I use any of the pre-trained models for this task, as they have been trained on these datasets.

I could now:

* Collect my own data
* Pay crowd-annotators to annotate the data
* Buy data

I would be specifically interested in the last point, is there a way to acquire these types of datasets? Is there a market for it?

How are other handling this in their companies?",1636447234.0,2021-11-09 09:40:34
[D] Recursive ML strategies?,3,qpw3br,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpw3br/d_recursive_ml_strategies/,8,"I'm looking for ideas on how to use recursive ML strategies, possibly utilizing multiple individual models where one model uses the output of another model to make more accurate predictions.

For example, I use two sklearn `RandomForestClassifier` models to provide a simple signal about the direction of the stock market. The first takes `n` inputs and outputs a prediction. The second takes the original `n` inputs plus the output of the first to make a new prediction. It doesn't provide earth-shattering results, but it appears to be slightly better than only using the one model.

Random forests also provide the ability to use Out-of-Bag samples, which could also be used.

I'm just curious if there any established methods, papers I should look at, etc. that discuss meta or recursive strategies to get the most out of ML models.",1636433341.0,2021-11-09 05:49:01
[T] Procedural Generalization by Planning with Self-Supervised World Models,2,qpuh3x,MachineLearning,https://arxiv.org/abs/2111.01587,2,,1636427689.0,2021-11-09 04:14:49
"[P] HuBERT: How to apply BERT to speech, visually explained",41,qpbci5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpbci5/p_hubert_how_to_apply_bert_to_speech_visually/,2,"Recently Facebook AI released HuBERT, a BERT-like model for learning powerful speech representations. At first glance, this model looks similar to wav2vec 2.0, but the training process/objective is actually very different.

I made some detailed illustrations to visually explain the pre-training process of HuBERT and how it compares to wav2vec 2.0. Both of these models are already available in the HuggingFace Transformers library.

[https://jonathanbgn.com/2021/10/30/hubert-visually-explained.html](https://jonathanbgn.com/2021/10/30/hubert-visually-explained.html)

Hope this is helpful!",1636369620.0,2021-11-08 12:07:00
[P] Open-NSFW 2: TensorFlow 2 implementation of the Yahoo Open-NSFW model,70,qp8897,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qp8897/p_opennsfw_2_tensorflow_2_implementation_of_the/,13,"Detecting Not-Suitable-For-Work (NSFW) images, in particular pornographic images, is a high demand task in computer vision. The Yahoo Open-NSFW model originally developed with the Caffe framework has been a favourite choice, but the work is now discontinued and Caffe is also becoming less popular.

This Open-NSFW 2 project provides a TensorFlow 2 implementation of the Yahoo model, with references to its previous third-party TensorFlow 1 implementation.

Please take a look!

[https://github.com/bhky/opennsfw2](https://github.com/bhky/opennsfw2)",1636355084.0,2021-11-08 08:04:44
[D] Commercial Distribution of OpenAI Jukebox Songs,4,qplwld,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qplwld/d_commercial_distribution_of_openai_jukebox_songs/,1,Hello. What is the copyright process for music created by an artificial intelligence? I have made a song using OpenAI's Jukebox and am wondering if I can commercially distribute it in streaming platforms such as Spotify.,1636401921.0,2021-11-08 21:05:21
[R] Implicit Behavioral Cloning,1,qpljnm,MachineLearning,https://arxiv.org/abs/2109.00137,2,,1636400939.0,2021-11-08 20:48:59
[R] Introducing MetaICL: A Language Model Meta-Training Framework for Few-Shot In-Context Learning,3,qpf5t0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpf5t0/r_introducing_metaicl_a_language_model/,1,"A research team from the University of Washington, FacebookAI Research and the Allen Institute for AI introduces Meta-training for InContext Learning (MetaICL), a new meta-training framework for few-shot learning where an LM is meta-trained to learn in-context â€” conditioning on training examples to recover the task and make predictions.

Here is a quick read: Introducing MetaICL:[A Language Model Meta-Training Framework for Few-Show In-Context Learning.](https://syncedreview.com/2021/11/08/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-140/)

The MetaICL code and data will be made available on the projectâ€™s [GitHub](https://github.com/facebookresearch/metaicl). The paper *MetaICL: Learning to Learn In Context* is on [arXiv](https://arxiv.org/abs/2110.15943).",1636382856.0,2021-11-08 15:47:36
[D]What is something you took the time to learn that benefitted you the most?,248,qordhq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qordhq/dwhat_is_something_you_took_the_time_to_learn/,112,Saw a thread in cscareer questiosn and I thought it was a great question that could help a lot of people in machine learning since there is so much to learn in this field and could use some direction!,1636300735.0,2021-11-07 16:58:55
[RESEARCH] OpenAI's GPT-3: cases of misusage and failures,3,qpi381,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qpi381/research_openais_gpt3_cases_of_misusage_and/,5,"Hello everyone!
Name's Alex, 26yo from Italy and currently studying Marketing and A.I. at IULM University here in Milan.

I grew quite an interest when finally GPT-3 came out, and while discussing with one of my professors, the topic of my BD thesis came up. Long story short, I'm gonna talk about GPT-3.

One of the topics I'd love to cover, is a collection of known applications (aka Use cases).
While I found quite a decent number of (more or less) succesful cases, I can't find anything.
I also tried on Google (since I only refer to Google Scholar to find reliable sources), but still never managed to find anything.
So here I am, asking if you guys know of any case where a company tried to use GPT-3 in some ways but the whole thing didn't end up quite as they expected.

Thank you all!",1636391456.0,2021-11-08 18:10:56
[N] Maritime Grand Challenge - Abu Dhabi,0,qplveb,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qplveb/n_maritime_grand_challenge_abu_dhabi/,0,"Came across this Maritime Grand Challenge that I thought others might find interesting. It combines drones, robotics and AI and thereâ€™s a $2M first prize and $3M overall in prize money. Open to universities, research institutions, companies and individual innovators. 

Hereâ€™s a link: [https://www.mbzirc.com/abudhabi\_aspire\_launches\_mbzirc\_challenge.php](https://www.mbzirc.com/abudhabi_aspire_launches_mbzirc_challenge.php). 

And a video: 

[https://www.youtube.com/watch?v=1AdBJCn15zQ](https://www.youtube.com/watch?v=1AdBJCn15zQ)

The competition is to further development of real-world solutions to illegal fishing, piracy, smuggling and coastline security.  First deadline -- initial phase  includes white papers and registration â€“ is Dec. 31, 2021.

I found out about it through this story: 

[https://www.robotics247.com/article/mbzirc\_maritime\_grand\_challenge\_3m\_prize\_launches\_abu\_dhabi/](https://www.robotics247.com/article/mbzirc_maritime_grand_challenge_3m_prize_launches_abu_dhabi/)",1636401821.0,2021-11-08 21:03:41
"[R] A Unified View of Relational Deep Learning for Polypharmacy Side Effect, Combination Synergy, and Drug-Drug Interaction Prediction",6,qp9mnn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qp9mnn/r_a_unified_view_of_relational_deep_learning_for/,1,"&#x200B;

https://preview.redd.it/t09f9yun5cy71.jpg?width=2146&format=pjpg&auto=webp&s=eafb32e5538a16375d34a82a6c01e0d01d9a677b

**Git:** [https://github.com/AstraZeneca/polypharmacy-ddi-synergy-survey](https://github.com/AstraZeneca/polypharmacy-ddi-synergy-survey)

**Paper:** [https://arxiv.org/abs/2111.02916](https://arxiv.org/abs/2111.02916)

**Abstract:**

In recent years, numerous machine learning models which attempt to solve polypharmacy side effect identification, drug-drug interaction prediction, and combination therapy design tasks have been proposed. Here, we present a unified theoretical view of relational machine learning models which can address these tasks. We provide fundamental definitions, compare existing model architectures and discuss performance metrics, datasets, and evaluation protocols. In addition, we emphasize possible high-impact applications and important future research directions in this domain.

**The paper provides:**

\- A unified model of drug pair scoring models with a general architecture design recipe.  
\- Model design comparisons based on architecture and input modalities.  
\- Evaluation metrics used by the most important papers.  
\- Public datasets that are relevant.  
\- Evaluation regime designs for stratified splits

**The Github repo comes with:**

\- Paper links with implementations.  
\- Links to the datasets.",1636361620.0,2021-11-08 09:53:40
[D] Intuition for meaning behind magnitude of covariance,0,qphreq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qphreq/d_intuition_for_meaning_behind_magnitude_of/,4,"Covariance matrices are pretty essential to many ML algorithms and probabilistic models. When two variables have positive covariance, they are correlated, when they have negative covariance, they are inversely correlated and when the covariance is zero, they are not correlated. However, the degree of correlation cannot be read from the magnitude of the covariance value. 

My question follows: well, what *can be read* from this magnitude. What does it mean if two variables have a very large covariance value opposed to a small one?",1636390548.0,2021-11-08 17:55:48
[R] Check the blog that introduces our recent #NeurIPS2021 work on nonconvex stochastic optimization! We propose stochastic Anderson mixing with theoretical guarantees and promising results in training neural networks.,14,qp432r,MachineLearning,https://thumtblog.github.io/2021/10/30/stochastic-anderson-mixing/,2,,1636338894.0,2021-11-08 03:34:54
[R] StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Synthesis,10,qp5pk4,MachineLearning,https://arxiv.org/abs/2111.03133,3,,1636344674.0,2021-11-08 05:11:14
[N] State of AI 2021,55,qotk5u,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qotk5u/n_state_of_ai_2021/,18,"Forth edition: [https://www.stateof.ai/](https://www.stateof.ai/)

If you read it (or skimmed it), what was the most important information in your opinion?",1636306842.0,2021-11-07 18:40:42
"[R][P] 2021: A Year Full of Amazing AI papers - A Review [work in progress...] A curated list of the latest breakthroughs in AI by release date with a clear video explanation, link to a more in-depth article, and code.",98,qoox33,MachineLearning,https://github.com/louisfb01/best_AI_papers_2021,6,,1636293026.0,2021-11-07 14:50:26
[D] New in-depth AI interview episode out! Tristan Zajonc's AI startup just came out of stealth mode already doing incredible work.,4,qp7ooe,MachineLearning,https://youtu.be/i8gIHUbZmwY,0,,1636352694.0,2021-11-08 07:24:54
[R] [P] AnimeGANv2 Face Portrait v2,1878,qo4kp8,MachineLearning,https://i.redd.it/k25gkmonb0y71.gif,104,,1636218407.0,2021-11-06 18:06:47
[D] Measure the distance between two domains for transfer learning.,27,qoqpv2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qoqpv2/d_measure_the_distance_between_two_domains_for/,14,"I know there are distances are defined to minimise for the domain adaption.

While I want to know does there exist any distance measurement that can measure the difficulty of performing the domain adaption from the source domain to different target domainsï¼Ÿ",1636298714.0,2021-11-07 16:25:14
[D] What happened to Compressive Transformers?,5,qoyyy7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qoyyy7/d_what_happened_to_compressive_transformers/,4,"They promised to solve one of the Transformer architecuter's greatest weaknesses, it's lack of long term memory, but I can't seem to find any bigger experiment using them? Did they not work out?",1636322519.0,2021-11-07 23:01:59
[D] According to google and AWS these are very NSFW... I want it on a shirt!,846,qo1l35,MachineLearning,https://medium.com/@tom_25234/synthetic-abstractions-8f0e8f69f390,72,,1636209527.0,2021-11-06 15:38:47
[R] stl file data annotation,4,qoqqp2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qoqqp2/r_stl_file_data_annotation/,2,"First time using Reddit. 

Anyone know a software I can use to annotate a point cloud stl medical image file? I'm aware of several softwares for DICOM files but none seem to work with stl. 

Ideally free software but willing to pay if good.",1636298795.0,2021-11-07 16:26:35
[Discussion] MLops tool for image data management and exploration,5,qoqp4w,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qoqp4w/discussion_mlops_tool_for_image_data_management/,7,"Hi there,

&#x200B;

Large part of the current work of our developers is dedicated to manual work on data. This is done in several stages of the development:

1. In the beginning when we get the annotated data we go over it (thousands of images). For segmentation tasks we observe the annotated images to validate the annotation correctness and images relevance for our tasks. Some images may be correctly annotated, but not relevant to our task or use-case so we remove them.
2. We check statistics of the data to validate that the data isn't biased. For example that we have enough samples from each class, and that the distribution of instances sizes (e.g. number of pixels in the segmentation mask) within the classes is reasonable.
3. When we have a trained model, we go over images for which the model gives poor results, and try to find similarities to understand weaknesses of the model and find root causes. This often leads to change of the data and repeating stages 1,2 and eventually 3.

These are very time consuming tasks, and we are looking for MLops tools that will make this work more efficient. Optimally, the tool will also take care of other MLops aspects, like experiments orchestration, experiment tracking and data versioning. We can also combine several tools, but using few (one?) tools is preferred. Open source or proprietary paid product are both possible.

Any recommendations?

Thanks!",1636298647.0,2021-11-07 16:24:07
[R] A (rare) real example of a true time series anomaly discovered by an algorithm.,34,qoa1tv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qoa1tv/r_a_rare_real_example_of_a_true_time_series/,3," In spite of all the academic work on anomaly detection in time series, it is almost impossible to find a ***real*** example of a true anomaly captured in the wild. Here I present such an example.

A group from Texas A&M/USC has released a very nice large dataset relating to electric grids. Most of the data is *measured* (temp, voltage etc.), but some (Solar Zenith Angle etc.) is *computed*.

As a sanity check upon downloading the data, I ran the Matrix Profile \[a\], to look for any anomalies in the data. It found the highly significant anomaly shown in the attached figure.

Can you guess what it isâ€¦ (spoiler below)

It took me a few seconds, but I guessed it >!might be a Leap Year bug in the data generator,!< and indeed, after I reported it, I found that this was the case.

Moral of the story. Check your data, and, the Matrix Profile is a very useful tool.

More examples of time series anomalies at \[a\] and \[b\]

\[a\] [www.cs.ucr.edu/\~eamonn/MatrixProfile.html](https://www.cs.ucr.edu/~eamonn/MatrixProfile.html)

\[b\] [https://www.cs.ucr.edu/\~eamonn/MERLIN\_Long\_version\_for\_website.pdf](https://www.cs.ucr.edu/~eamonn/MERLIN_Long_version_for_website.pdf)

\[c\] [https://github.com/tamu-engineering-research/Open-source-power-dataset](https://github.com/tamu-engineering-research/Open-source-power-dataset)",1636235133.0,2021-11-06 22:45:33
FLOPS Calculation [D],1,qoptlo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qoptlo/flops_calculation_d/,6,"Please consider the following table:

&#x200B;

https://preview.redd.it/z1w61dlho6y71.png?width=441&format=png&auto=webp&s=eb502bec8b7116771f72719598884956fb6b44be

which is from FaceNet publication. What I am trying is to learn FLOPS calculations with the help of this model so I can calculate for the below ones. If I am not wrong, the first row should have (110\*110\*64 \* 7\*7\*3) 113,836,800 FLOPS which is a bit lesser than given value. Likewise the second convolution row conv2a (55\*55\*64 \* 64 = ) 12390400 which is again near but not exactly the same.

[https:\/\/sefiks.com\/2020\/06\/16\/face-recognition-with-deepid-in-keras\/](https://preview.redd.it/124c1x8fo6y71.png?width=564&format=png&auto=webp&s=7b20c0efff595511c2dff4bdfdefa0c1a318dd50)

&#x200B;

Am I on the right track? What am I doing wrong?

 Also [https://www.thinkautonomous.ai/blog/?p=deep-learning-optimization](https://www.thinkautonomous.ai/blog/?p=deep-learning-optimization) mentions multiplying by 2 after all those operations which further confuses me. ",1636295924.0,2021-11-07 15:38:44
"[Research] Looking for interesting machine learning papers to read over the weekend? Here is a curated list I made for 2020. (with video explanation, short read, paper, and code) - Stay tuned for 2021 at the end of December!",52,qo0f1y,MachineLearning,https://github.com/louisfb01/Best_AI_paper_2020,5,,1636205864.0,2021-11-06 14:37:44
[D] GPT-3 is No Longer the Only Game in Town,22,qo5in1,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qo5in1/d_gpt3_is_no_longer_the_only_game_in_town/,9,"Hey there, I just put out a little article that you might find interesting - [GPT-3 is No Longer the Only Game in Town](https://lastweekin.ai/p/gpt-3-is-no-longer-the-only-game). It catalogues the appearance of models akin to GPT-3 over the course of 2021, like  [HyperCLOVA](https://venturebeat.com/2021/06/01/naver-trained-a-gpt-3-like-korean-language-model/) and such. Hope you enjoy it!

*TLDR: Organizations face significant challenges in creating a model similar to OpenAIâ€™s GPT-3, but nevertheless a half dozen or so models as big or bigger than GPT-3 have been announced over the course  of 2021.*",1636221258.0,2021-11-06 18:54:18
Comparing deep models with different complexity and different accuracies [D],0,qojz5g,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qojz5g/comparing_deep_models_with_different_complexity/,3," I am working with a deep learning based system where complexity has to be reduced to a minimal. However, this is having some impact on the accuracy. I am having to ask the question in somewhat hypothetical scenario and hope that I am clear in my statement.

Say one system uses 1 billion FLOPS network to achieve 99% accuracy yet another uses 0.5 billion FLOPS network to give 95%. On equal accuracy we could say that the system with lower flops is better. likewise, on equal FLOPS we would have been able to say that the first system is better. Is there a direct way like a standard KPI to compare these two systems in the given scenario?

In short, can we compare two systems with different accuracies and different FLOPS?",1636271893.0,2021-11-07 08:58:13
[R] Unsupervised Learning of Compositional Energy Concepts,11,qo4set,MachineLearning,https://arxiv.org/abs/2111.03042,3,,1636219041.0,2021-11-06 18:17:21
"[P] League of Legends Patch 11.21 Game Playing AI (Reinforcement Learning, Supervised Learning) Dataset",182,qnktqk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnktqk/p_league_of_legends_patch_1121_game_playing_ai/,36,"This dataset is meant for anyone who would like to try to create a deep learning agent either using supervised or (offline) reinforcement learning to play League of Legends. The dataset contains 72 games from patch 11.21 (last patch) where the game ended in an early surrender. These games were chosen as the game lengths were guaranteed to be low which kept the dataset from being too large.
To download the dataset, go to [this](https://github.com/MiscellaneousStuff/tlol) GitHub link and click on the `Google Drive Link`. The dataset is stored as an SQLite database file and the schema should be relatively self-explanatory. Happy to answer any questions.

This is just a preliminary dataset which demonstrates that this is possible. Within the next few days the dataset will contain 1000s of replays which means 10,000s of champions worth of data (for each time a player plays a champion).

Edit: Database now contains all 191 early surrender games (games ending at or before 3.5 minutes) in the dataset.
This table shows the top 10 champion occurrences within the dataset.

| Champion     | No. |
| ------------ | --- |
| Nami         | 116 |
| Miss Fortune | 103 |
| Lucian       | 61  |
| Khazix       | 36  |
| Viego        | 35  |
| Lux          | 34  |
| Jhin         | 32  |
| Yone         | 30  |
| Camille      | 29  |
| Graves       | 29  |

Edit 2: Larger dataset containing 987 games targeting Miss Fortune in the early game (up to first 5 minutes) with the same schema and format as the first dataset. Also contains all game objects recorded 4 times a second. The games were chosen by getting the games where the MF player lived the longest. This gave a dataset where the players overall had a 64.4% win rate in roughly EUW Diamond II.

Edit 3: A further 728 games also targeting Miss Fortune in the early game (up to first 5 minutes) with the same schema and format as the first and second dataset. This brings the total number of games for the MF-Longevity datasets to 1,715 or
`1,715 games * (5 minutes * 60 seconds * 4 frames/second) := 2,058,000 frames in total.`
This should now be enough to at least create a deep learning agent which can play Miss Fortune for the first five minutes of a game at least to a basic level.

Edit 4: Another day another dataset. A further 773 games from the `MFLongevity` dataset have been uploaded. I have now also included a Jupyter Notebook to analyse the data from the `191-EarlyFF` dataset which works completely standalone from Google Colab. Feel free to also run it locally if you wish to.

[GitHub Link](https://github.com/MiscellaneousStuff/tlol)

[![Open Notebook In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MiscellaneousStuff/tlol/blob/main/League_of_Legends_Patch_11_21_(Reinforcement_Learning).ipynb)",1636146868.0,2021-11-05 22:14:28
[P] Is it necessary to manually label desired objects in each image for object detection?,3,qo8wvt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qo8wvt/p_is_it_necessary_to_manually_label_desired/,5,"For example i have a dataset of thousands of images of 5 different objects and each object images already stored in different folders and i train my model on it. 

Now i give an image as input which contains all 5 of the objects and i want my model to detect all 5 of these objects in the image and draw bounding boxes around them.

To be able to do this, when preparing my dataset do i have to label desired object in each image by drawing boxes around them and then train my model on it?

Is there any better way to do this than manually labelling data of thousands of images?",1636231637.0,2021-11-06 21:47:17
[D] Google MUM details?,8,qo03fo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qo03fo/d_google_mum_details/,5,"Google has [anounced](https://www.google.com/amp/s/blog.google/products/search/introducing-mum/amp/)   MUM several months ago. In the blog post it says that it uses T5 framework and is multimodal but that is about it. 
The name Multitask Unified Model does not help a lot.

Does anybody know how it is trained and how it combines text and images? The only thing i can think of is to train it to generate image captions and then finetune it on multiple tasks, something like bigscience T0pp?",1636204803.0,2021-11-06 14:20:03
"[D] ""Real-World Challenges for AGI"" by DeepMind",23,qnu4bg,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnu4bg/d_realworld_challenges_for_agi_by_deepmind/,4,"Either, I did not understand the article ([https://deepmind.com/blog/article/real-world-challenges-for-agi](https://deepmind.com/blog/article/real-world-challenges-for-agi)) or they just inserted the letters A, G, and I randomly in between their current progress with weather prediction and plasma control for fusion.",1636178538.0,2021-11-06 07:02:18
[R] Is CVPR really only for computer vision?,7,qnxu7h,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnxu7h/r_is_cvpr_really_only_for_computer_vision/,9,"I know [CVPR](https://cvpr2022.thecvf.com/) literally means ""Computer Vision and Pattern Recognition"", but does it really means that any submission in the ML field with no direct link with Computer vision will be discarded?  
Thanks!",1636195879.0,2021-11-06 11:51:19
GPTSD - Transfer of trauma from human to machine back to human via machine learning models [P],3,qo1sdh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qo1sdh/gptsd_transfer_of_trauma_from_human_to_machine/,1,"&#x200B;

https://preview.redd.it/tf7f8tl9nzx71.jpg?width=808&format=pjpg&auto=webp&s=2c9335110a8bc0931f5c706573ea692216609606

GPTSD is a series of images and text created with GPT2 which explores the transfer of trauma from human to machine back to human via machine learning models. By converting human portraits to text, GPT2 is able to recreate new text base portraits and dream recollections based off the dream diary of a Vietnam war veteren. [https://www.hicetnunc.xyz/gptsd](https://www.hicetnunc.xyz/gptsd)",1636210107.0,2021-11-06 15:48:27
[D] Help: Choosing right data/file format for Storing Text Data,4,qnznd4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnznd4/d_help_choosing_right_datafile_format_for_storing/,5,"Hello, I'm working on a NLP project which requires me to collect the data. As of now I have collected data and stored in to individual text file(one article into a single text file).

So for future analysis I have cleaned the data and converted into dict (article id and article text) and now I want to store this data into some specific file formate such as Json, hd5 etc. 

So I want your recommendation on this, which is the best format to store this kind of data. 

Please provide suggestions by keeping in mind the latency in loading data from drive( or faster the better).",1636203320.0,2021-11-06 13:55:20
[P] Problems with a Neural Networks's Output's Order of Magnitude,1,qo5fhz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qo5fhz/p_problems_with_a_neural_networkss_outputs_order/,12," 

I'm working on a project where I'm trying to train an airplane to control itself in a 2D setting.

The neural network that acts as the airplane's pilot has 2 outputs that are related to the control of the airplane's altitude (angle of attack and throttle, for those that are familiar).

Unfortunately, my network is outputing numbers that have a completely wrong order of magnitude. I tried writing my own activation function to limit their values, but I'm simply getting the max or min allowable value (since my actual output is off the scale).

Is there any way to control the order of magnitude of the outputs? I've tried a ton of things, from intitializing weights to be extremely small, to normalization of the input.",1636220978.0,2021-11-06 18:49:38
"[N] TF/Keras in Hugging Face, Datasets Edition",64,qnhf5f,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnhf5f/n_tfkeras_in_hugging_face_datasets_edition/,8,"Hi all, Tensorflow maintainer at Hugging Face here! I [posted here a few months ago](https://www.reddit.com/r/MachineLearning/comments/ok81v4/n_tf_keras_and_transformers/) about the big change we were making to the library to make everything Keras-native, and people seemed to like it, so I thought I'd give another update on what's changed since then. We've made a couple of big changes that reduce the amount of duplicate, boilerplate code in common scripts massively, and we'd love to get people using the new approaches and get feedback!

**What happened in last week's episode of Hugging Face?**

The story up until now is that all our models are now Keras models. You can still write your own training loop or use the models as a layer in a larger model; everything like that remains unchanged, but it's incredibly convenient to just load a model, then just immediately `compile()` and `fit()` it. I gave some examples in the post I linked above.

**""Last week"" usually refers to times less than four months ago.**

You aren't telling me anything that isn't already in my performance reviews, don't worry.

**At least you delivered eventually. What's new?**

So the first big new change is a really nice integration with ðŸ¤— Datasets. If you're unfamiliar, Datasets is the data equivalent to Hugging Face's model hub - you just load any uploaded dataset in one line of code the same way you load a pretrained model, with the `load_dataset()` function. 

It's not just for NLP - people are using Transformers for audio and vision and everything else these days, so there's [all kinds of data](https://huggingface.co/datasets) in there. You should check it out!

To see an example of `load_dataset()` in action, a standard workflow with Datasets and Transformers goes something like this:

    from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
    from datasets import load_dataset
    
    # Load a pretrained model and its tokenizer
    model_name = 'bert-base-cased'
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
    
    # Load a dataset - we'll use the COLA dataset from the GLUE benchmark
    data = load_dataset(""glue"", ""cola"")
    
    # Define a function to tokenize the data, then apply it to the dataset.
    # The tokenizer returns a dict, and map() will add keys from that dict
    # to the dataset as columns
    def tokenize_function(dataset):
        return tokenizer(dataset['sentence'])
    
    tokenized_dataset = data.map(tokenize_function)

So far, so good, but this is the point where problems start to arise, because it's really hard to get the tokenized data into your model. The data is often 1) quite large and 2) jagged, because different samples will tokenize to arrays of different lengths. As a result, if you want to load the data as a single dict of `np.ndarray` or `tf.Tensor`, you end up having to do huge amounts of padding, which bloats memory usage and massively slows down the model. The way to get good performance is to load random batches of samples and only pad that batch, not the entire dataset, but doing that basically required you to write a custom training loop, or at the very least a Python generator, before it would work with Keras. 

If anyone was using Transformers with TF before now I'd love to hear how you were solving this, because it was a huge recurring pain for me.

**So is there a solution now?**

There is!  The solution is that we added the method `to_tf_dataset()` to all our datasets. This basically wraps the dataset in a `tf.data.Dataset`, which will do the just-in-time data padding you want. We've also updated our `DataCollator` classes to work with this, so you can generate your dataset like so:

    from transformers import DataCollatorWithPadding
    
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=""tf"")
    
    tf_dataset = tokenized_dataset['train'].to_tf_dataset(
        columns=[""input_ids"", ""attention_mask"", ""labels""],
        batch_size=16,
        shuffle=True,
        collate_fn=data_collator
    )

Note how the data collator needs your model's `tokenizer` \- that's because every **god damned** research group in every **god damned** university in every **god damned** country handles their data in a slightly different way, and so there's no universal approach to padding that works for all of the hundreds of different models out there.  We do guarantee, though, that the `tokenizer` that comes with a given `model` will have a `pad()` method that works for that model, and that's what the `data_collator` will use. You have no idea how much pain you're being saved with that method.

**Okay calm down, what do I do with this tf\_dataset?**

That bit's easy! A lot of people aren't that familiar with `tf.data`, but it's actually really cool. Once you have a `tf.data.Dataset`, you can pass it straight to `model.fit()` or just iterate over it in a for loop to get batches.

**Won't I need to compile this model before I can fit() it? What loss should I use?**

That's a great question! And that brings me to the second big change we've made. Our models now **automatically compute losses that are suitable for their task in a way that's accessible to Keras.** In other words, if you use `TFAutoModelForSequenceClassification`, that model will now compute a loss appropriate for sequence classification tasks (i.e. crossentropy) for you! Don't know what loss you need to train GPT-2 with? No problem - `TFAutoModelForCausalLM.from_pretrained('gpt2')` will do it for you.

**Wait, stop! I'm an advanced user and I want my loss, not your loss!**

Don't panic! You can still use whatever loss you want, and all old code will work exactly as it did before. The only change is that if you `compile()` your model without a loss, it'll interpret that as you wanting the default internal loss. If you specify a loss argument to `compile()`, then it'll use that and not the internal loss. In addition, this only applies when using the Keras API, like `fit()`. If you're writing manual training loops or using the model as a layer in a larger model, none of this is relevant to you. This is just a convenience, and it's easy to disable!

**So I just... skip the loss argument?**

Exactly! If we continue on the code samples from above, all you need to do is:

    from tensorflow.keras.optimizers import Adam
    optimizer = Adam(3e-5)  # Transformers work much better with lower LRs
    
    model.compile(optimizer=optimizer)  # No loss argument!
    
    model.fit(tf_dataset)

And that's it! Dataset loaded, tokenized and trained on. With changes to a few lines almost any NLP task from translation to token classification or summarization can be handled in a similar way.

If you want to see more, we have [a bunch of example notebooks in both TensorFlow and PyTorch](https://huggingface.co/transformers/notebooks.html), and all the TF examples should be up-to-date with these new methods.",1636136930.0,2021-11-05 19:28:50
[R] Arch-Net: A Family Of Neural Networks Built With Operators To Bridge The Gap Between Computer Architecture of ASIC Chips And Neural Network Model Architectures,9,qntbjh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qntbjh/r_archnet_a_family_of_neural_networks_built_with/,1,"**Key Takeaways**

* As it turns out, the Arch-Net is actually building a bridge that translates between computer architectures of ASIC chips and neural network model architectures by changing existing floating-point DNNs into hardware-friendly quantized Arch-Net.
* The structure of ArchNet is made up of five operators: 3Ã—3 Convolutions, Batch Normalization, Concatenation, 2Ã—2 Max-pooling, and Fully-Connected layers.
* The conversion to Arch-Net is much simpler without labeled data as researchers employ Blockwise Model Distillation on feature maps.
* Researchers did extensive experiments on image classification and machine translation tasks to confirm that Arch-Net is both effective, efficient and fast.

# [Paper](https://arxiv.org/pdf/2111.01135v1.pdf) | [Github](https://github.com/megvii-research/Arch-Net)

https://preview.redd.it/eqbzv5m1rwx71.png?width=1536&format=png&auto=webp&s=85cd91db543a521b189f5578a985f24629a6c91a",1636175091.0,2021-11-06 06:04:51
[D] Professors and research groups in Neural program synthesis,2,qo3704,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qo3704/d_professors_and_research_groups_in_neural/,5,"I want to collect a list of professors and research groups that work in neural program synthesis or program induction.
All you can find with a simple search is groups at Microsoft, Google Brain, ETH Zurich and MIT.
Does any one know other groups that work in this topic especially outside USA.

PS. I couldn't find any groups or professors in Germany
If you have any helpful tips how to do PhD in this topic would be nice",1636214319.0,2021-11-06 16:58:39
[R] Adversarial Intrinsic Motivation for Reinforcement Learning,3,qnwqed,MachineLearning,https://arxiv.org/abs/2105.13345,2,,1636190576.0,2021-11-06 10:22:56
[P] optimization of Hugging Face Transformer models to get Inference < 1 Millisecond Latency + deployment on production ready inference server,184,qn8com,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qn8com/p_optimization_of_hugging_face_transformer_models/,32,"Hi,

I just released a project showing how to optimize big NLP models and deploy them on Nvidia Triton inference server.

source code: [https://github.com/ELS-RD/triton\_transformers](https://github.com/ELS-RD/triton_transformers)

project description:  [https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c?source=friends\_link&sk=cd880e05c501c7880f2b9454830b8915](https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c?source=friends_link&sk=cd880e05c501c7880f2b9454830b8915)

Please note that it is for **real life large scale NLP model deployment**. It's only based on open source softwares. It's using tools not very often discussed in usual NLP tutorial.

Performance have been benchmarked and compared with recent Hugging Face Infinity inference server (commercial product @ 20K$ for a single model deployed on a single machine).

Our open source inference server with carefully optimized models get better latency times that the commercial product in both scenarios they have shown during the demo (GPU based).

Don't hesitate if you have any question...",1636108867.0,2021-11-05 11:41:07
"[P] Open-source project to collect data from multiple databases, apps, SaaS tools and prepare for ML tasks",4,qnq3m4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnq3m4/p_opensource_project_to_collect_data_from/,2,"We have lots of business data scattered across different databases and apps. [Rudderstack](https://github.com/rudderlabs/rudder-server) can integrate data from various sources and then activate this data in your warehouse or business tools for ML operations. This unlocks the potential applications of the data which was hard to do before Rudderstack e.g. UX personalization, business analytics, etc.

I'm seeking feedback, what can I improve and how would you want to use it? AMA",1636163429.0,2021-11-06 02:50:29
[R] RLiable: Better Evaluation for Reinforcement Learningâ€”A Visual Explanation,13,qnevvp,MachineLearning,https://araffin.github.io/post/rliable/,0,,1636129838.0,2021-11-05 17:30:38
"[R] Google & UC Berkeleyâ€™s Data-Driven Offline Optimization Approach Significantly Boosts Hardware Accelerator Performance, Reduces Simulation Time by More Than 90%",14,qnchpo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnchpo/r_google_uc_berkeleys_datadriven_offline/,0,"A research team from Google Research and UC Berkeley proposes PRIME, an offline data-driven approach that can architect hardware accelerators without any form of simulations. Compared to state-of-the-art simulation-driven methods, PRIME achieves impressive performance improvements of up to 1.54Ã— while reducing the total required simulation time by up to 99 percent. 

Here is a quick read: [Google & UC Berkeleyâ€™s Data-Driven Offline Optimization Approach Significantly Boosts Hardware Accelerator Performance, Reduces Simulation Time by More Than 90%.](https://syncedreview.com/2021/11/05/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-139/)

The paper *Data-Driven Offline Optimization for Architecting Hardware Accelerators* is on [arXiv](https://arxiv.org/abs/2110.11346).",1636122952.0,2021-11-05 15:35:52
[D] How do you actually serve your models in production?,8,qnfjdc,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnfjdc/d_how_do_you_actually_serve_your_models_in/,14,"Do you use some sort of a built-in tool with the technology stack that you are using (Amazon or Azure for example)?

Do you use other tools to create a Docker image for inference like mlflow or azureml?

Do you DIY it and make your own image and your own service/microservice?",1636131653.0,2021-11-05 18:00:53
ruDALL-E model is open-source [P],75,qmzy8a,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmzy8a/rudalle_model_is_opensource_p/,4,"Sberbank submitted an open-source ruDALL-E model, inspired by OpenAI's DALLÂ·E, for Russian.

The model with 1.3 billion parameters is available under Apache 2.0 license. The pipeline includes image generation, ranging results with ruCLIP and super-resolution.

The large model (12 billion parameters) will be available in the cloud. ruDALL-E is the biggest neural network project in the history of Russia, taking more than 20,000 GPU-days of Nvidia V100 to train it.

Github: [https://github.com/sberbank-ai/ru-dalle](https://github.com/sberbank-ai/ru-dalle)

Model: [https://huggingface.co/sberbank-ai/rudalle-Malevich](https://huggingface.co/sberbank-ai/rudalle-Malevich)

Demo (Russian): [https://rudalle.ru/](https://rudalle.ru/)

&#x200B;

Here are some pictures generated with it: (cherry-pick by authors)

[Avocado in the style of Malevich](https://preview.redd.it/uzwim0cshox71.jpg?width=1024&format=pjpg&auto=webp&s=014c5cfba1b905b07d83c82e5f449199382dad67)

[Cat looks at food](https://preview.redd.it/891829cshox71.jpg?width=1024&format=pjpg&auto=webp&s=ef530332306cf308821e15ae4f5e5b42ed43712d)

[Anime chan](https://preview.redd.it/w91ngfcshox71.jpg?width=1024&format=pjpg&auto=webp&s=c9266443ac9469ae80cf60e77c4ae302cee93c51)

[Trump hides the pain](https://preview.redd.it/zivbitcshox71.jpg?width=1024&format=pjpg&auto=webp&s=1d7b611ab31b6d6381c72ee0262f755df4812457)

[Mystery forest](https://preview.redd.it/nlm020cshox71.jpg?width=1024&format=pjpg&auto=webp&s=864caaa7fa49e32d0af1e62e11fc1454fe2e6147)

[Salvador Dali picture](https://preview.redd.it/pa9ag2cshox71.jpg?width=944&format=pjpg&auto=webp&s=9596c8691ef55ed1314281a05285c5feeb00b73a)

[Beautiful chan](https://preview.redd.it/olmhn8cshox71.jpg?width=1024&format=pjpg&auto=webp&s=0bc411b70a9ab0437d564dfd81034fd2d528a5f1)

[Pepe frog](https://preview.redd.it/g0nlqicshox71.jpg?width=1024&format=pjpg&auto=webp&s=02e56b776a338cbf78efce41432d559a3809fbf1)

[Grand Canyon](https://preview.redd.it/sgqn5ncshox71.jpg?width=1024&format=pjpg&auto=webp&s=ed9ddd11481f320ad2dd85da5d3135a47f0eeeda)",1636075281.0,2021-11-05 02:21:21
"[N] Isomorphic Labs just unveiled today, a new Alphabet company led by DeepMind's Demis Hassabis. Plans to tackle drug discovery using AI.",234,qmrglg,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmrglg/n_isomorphic_labs_just_unveiled_today_a_new/,57,"Even as an insider, I found the idea of a DeepMind offshoot pretty surprising -- curious what you folks think about it. What are the odds it'll succeed? Will Alphafold++ even be useful for drug discovery?  


Tweet unveiling the company: [https://twitter.com/demishassabis/status/1456283985554939907?s=20](https://twitter.com/demishassabis/status/1456283985554939907?s=20)  


Website: [https://www.isomorphiclabs.com/blog](https://www.isomorphiclabs.com/blog)",1636051415.0,2021-11-04 19:43:35
[D] CVPR: Policy for posting on arXiv,2,qnhn4s,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnhn4s/d_cvpr_policy_for_posting_on_arxiv/,1,"I am planning on submitting a paper to CVPR 2022, and I have some questions regarding the process; I am finding the information on the site a bit confusing.

What time-frame are we allowed to post preprint versions of our papers to arXiv? Are there certain considerations we need to know about? Thanks in advance!",1636137542.0,2021-11-05 19:39:02
[D] How do you structure your CV?,1,qniktz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qniktz/d_how_do_you_structure_your_cv/,7,"After I made my first CV I've been working on the same company for about two years and worked in multiple projects. Now I'm interested in looking out for a bit and reworking my CV and don't really know in what way should I display my projects and skills.

Any Senior AI-related Engineers want to share your CVs or suggestions?",1636140262.0,2021-11-05 20:24:22
[R] No One Representation to Rule Them All: Overlapping Features of Training Methods,17,qn2qbx,MachineLearning,https://arxiv.org/abs/2110.12899,4,,1636084592.0,2021-11-05 04:56:32
[R] Hierarchical Transformers Are More Efficient Language Models,102,qmm9z7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmm9z7/r_hierarchical_transformers_are_more_efficient/,9,"[https://arxiv.org/abs/2110.13711](https://arxiv.org/abs/2110.13711)

A team from Google, OpenAI, and University of Warsaw proposes a new Efficient Transformer architecture for language modeling, setting a new state-of-the-art on the imagenet32 for autoregressive models.",1636037469.0,2021-11-04 15:51:09
[D] Why do we need the random noise z in conditional GANs?,16,qmy3ir,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmy3ir/d_why_do_we_need_the_random_noise_z_in/,10,"Obviously, we need some kind of input for the neural net. But in the case of conditional GANs, we have another kind of input. Does the random noise z then only serve to introduce variety for a given condition (e.g. many different faces all with blonde hair)? If I didnâ€™t care about this variety, could I just do without the random noise? Or is there some other justification for why we need the random noise z (makes training easier, some theoretical reason, â€¦)?",1636069609.0,2021-11-05 00:46:49
[D] How does ACL rolling review work?,0,qnbrji,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnbrji/d_how_does_acl_rolling_review_work/,1,"Hi folks, I am going through the ACL rolling review process and have some doubts.

So, after submitting the paper at the open review website, 

1) if accepted the reviews, comments, and pdf on open review will be there?

2) Same for rejected papers; if rejected, can I withdraw my article from open review, or it is going to be there along with rejected reviews on open review website?

My main concern is if rejected and submitted to another conference, there will be plagiarism because the paper will be already on the open review website along with reviews, then it will affect the other submission.",1636120818.0,2021-11-05 15:00:18
[P] A place to create your online data science portfolio and browse the community data science projects,0,qnhtxj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnhtxj/p_a_place_to_create_your_online_data_science/,0,"Hey all! I'm a data scientist who has shifted career from the biomedical field - now working at a tech company. It was hard to learn data science skills, showcase them to my first employers and stand out. That's why I created [datascienceportfol.io](https://www.datascienceportfol.io/) You can create your own online portfolio, showcasing your projects and skills in an effective way! Also, you can get inspired by browsing projects created by the community!

Still early days and I'm now working on a section to browse projects of other people and get inspired!

Please, let me know what you think! any feedback or improvement ideas are very welcome! :D Thanks so much, Pasquale",1636138088.0,2021-11-05 19:48:08
"[D] In theory, could we make a code that bridges different music genres?",7,qn1erq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qn1erq/d_in_theory_could_we_make_a_code_that_bridges/,5,"I know little to no about coding, but I have been playing with VQGAN recently and this idea has popped into my head. In theory, could we write an AI code that gets feed stems (that is, each instrument track as a separate audio file) for two different songs and rebuilds the first song in the style of the second one?   
The code would look for relevant information from the first song such as melodies, rhythms, structure and maybe even allow the user to feed in the lyrics to the vocals. Then, it would study how these elements are employed in the second song and rebuild the first song using the second song's style. This code could rebuild a rock song as a rap song, for instance.",1636080051.0,2021-11-05 03:40:51
[D] Ethical concerns for ML to predict race & gender,41,qmm6uh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmm6uh/d_ethical_concerns_for_ml_to_predict_race_gender/,86,"Iâ€™m working on a data product that primarily uses image and name classifiers to identify race and gender. This means that someone who buys this product is now able to see race and gender data associated with people and/or companies in their database. The use case behind this is to report on and make decisions to improve diversity (i.e. an investment firm seeking to invest more in underrepresented groups, an HR company reporting on industry trends).

Iâ€™m looking for feedback on ethical/design/quality concerns in regards to some of the following factors:

-	We are primarily leveraging publicly available training data sets, models, and classifiers.
-	Gender classification includes only male/female options.
-	We use a publicly available photo for classifying each person.
-	None of the data we provide is self-reported, nor does the product communicate that to the customer. 
-	We do not yet provide a confidence score or any feedback or correction feature.
-	Race and gender/sex are legally protected classes in some cases. While we are not using these to make any decisions in our product, we are the ones generating this data, which our customers will use at their discretion.

Iâ€™m worried we are not doing enough due diligence for the intentional choices weâ€™re making and the unintended impact they may have. Any resources for designing fair systems, especially ones that attempt to generate rather than consume this type of data, would be appreciated.",1636037227.0,2021-11-04 15:47:07
[D] Buying a PC for training - Does it make sense in 2021?,0,qngw0u,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qngw0u/d_buying_a_pc_for_training_does_it_make_sense_in/,7,"I work with text data. I'd love to put \~$500 down to have a machine that can fine-tune the largest GPT-2 instance (the largest two are 774M, and 1.5B parameters). Does it still make sense to buy a computer to do it? I've spent a few hundred dollars on AWS credits, but knowing that I'm being throttled by the cost limits the experiments I can run.

I would love to hear from those who have bought machines or those who have decided against it. I would probably look for a used one on eBay, though I know very little about purchasing a PC.",1636135439.0,2021-11-05 19:03:59
[D] Why Jupyter notebook doesnt store requirements (require packages) in ipynb file?,0,qnh7w4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qnh7w4/d_why_jupyter_notebook_doesnt_store_requirements/,9,The ipynb file is a JSON file. List with required packages can be easily added there. Why there is a separate file for this?,1636136360.0,2021-11-05 19:19:20
[D] What's the difference between top-tier papers and others?,53,qmhthd,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmhthd/d_whats_the_difference_between_toptier_papers_and/,14,"Hello, guys.

Although, I read a various of papers that in top conferences such as ICCV, ECCV, ICML.

I don't know what's difference in the first, second and other tier papers.

From the top paper, All I can know it's very obvious in writing skills and technologies that I don't understand yet.

&#x200B;

To be specific, What's different in the way that distinguish them?",1636022465.0,2021-11-04 11:41:05
[D] What is the most effective way to mix scalar value(s) into CNN feature maps?,3,qn2jg5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qn2jg5/d_what_is_the_most_effective_way_to_mix_scalar/,5,"It feels like an easy task but I can't seem to recall or find much info on this.

What I'm trying to do is use a scalar value and try to mix it into the intermediate feature maps of a CNN.

I know typically you might just concatenate these kind of scalars after flattening the feature map and before a FC layer, but I want this value to be combined into the intermediate feature maps between convolutions, and not at the end of the whole CNN encoder.

If it were categorical information, I know I could use learnable embeddings and add/concat, but in my case this is a continuous scalar.

I've seen suggestions to treat this scalar like a bias term and simply add, but this doesn't look strong and I'm not quite convinced. I've also thought about copying this value to a h x w x 1 array to concat before the next convolution, but I'm not sure about this either.

What is the most effective method to do this?",1636083886.0,2021-11-05 04:44:46
[P] StyleGAN3 + Wav2Lip,11,qmqhgg,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmqhgg/p_stylegan3_wav2lip/,7,"&#x200B;

[due to the limit of my compute the quality suffers a bit.](https://reddit.com/link/qmqhgg/video/v5z7c4f9bmx71/player)",1636048748.0,2021-11-04 18:59:08
[P] Survey study examining practices in NLG evaluation,3,qmw9lr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmw9lr/p_survey_study_examining_practices_in_nlg/,1,"**Do you work or do research on natural language generation (NLG)?** If yes, we are interested in your participation in a **20-minute survey** about practices when evaluating NLG systems or models.

The participants should have experience with working with or on (any type of) natural language generation (NLG) systems and tasks. The purpose of this research is to uncover unnamed practices and assumptions made during the evaluation of NLG systems, applications, and tasks. We hope that by understanding such practices and assumptions we will be able to better unpack the ways they could lead to **unintended consequences related to fairness and inclusion**.  

**If you are interested, please fill out this form here**:  [https://forms.office.com/r/R9BL1szeei](https://forms.office.com/r/R9BL1szeei). Thank you so much for your consideration and help!",1636064370.0,2021-11-04 23:19:30
[Discussion] Applied machine learning implementation debate. Is OOP approach towards data preprocessing in python an overkill?,201,qm6ieq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm6ieq/discussion_applied_machine_learning/,86,"***TL;DR***:

* I am trying to find ways to standardise the way we solve things in my Data Science team, setting common workflows and conventions
* To illustrate the case I expose a *probably-over-engineered* OOP solution for Preprocessing data.
* The OOP proposal is neither relevant nor important and I will be happy to do things differently (I actually apply a functional approach myself when working alone). The main interest here is to **trigger conversations towards** **proper project and software architecture, patterns and best practices among the Data Science community.**

# Context

I am working as a Data Scientist in a big company and I am trying as hard as I can to set some best practices and protocols to standardise the way we do things within my team, ergo, changing the extensively spread and overused Jupyter Notebook practices and start building a proper workflow and reusable set of tools.

In particular, the idea is to define a common way of doing things (workflow protocol) over 100s of projects/implementations, so anyone can jump in and understand whats going on, as the way of doing so has been enforced by process definition. As of today, every Data Scientist in the team follows a procedural approach of its own taste, making it sometimes cumbersome and non-obvious to understand what is going on. Also, often times it is not easily executable and hardly replicable.

I have seen among the community that this is a recurrent problem. eg:

* [https://www.reddit.com/r/MachineLearning/comments/q7ey06/d\_tired\_of\_writing\_mundane\_data\_wrangling\_code/](https://www.reddit.com/r/MachineLearning/comments/q7ey06/d_tired_of_writing_mundane_data_wrangling_code/)

In my own opinion, many Data Scientist  are really in the crossroad between Data Engineering, Machine Learning Engineering, Analytics and Software Development, knowing about all, but not necessarily mastering any.  Unless you have a CS background (I don't), we may understand very well ML concepts and algorithms, know inside-out **Scikit Learn** and **PyTorch,** but there is no doubt that we sometimes lack software development basics that really help when building something bigger.

I have been searching general applied machine learning best practices for a while now, and even if there are tons of resources for general architectures and design patterns in many other areas, I have not found a clear agreement for the case. The closest thing you can find is cookiecutters that just define a general project structure, not detailed implementation and intention.

# Example: Proposed solution for Preprocessing

For the sake of example, I would like to share a potential structured solution for **Processing**, as I believe it may well be 75% of the job. This case is for the general **Dask** or **Pandas** processing routine, not other huge big data pipes that may require other sort of solutions.

\*\**(if by any chance this ends up being something people are willing to debate and we can together find a common framework, I would be more than happy to share more examples for different processes)*

&#x200B;

>*Keep in mind that the proposal below could be perfectly solved with a functional approach as well. The idea here is to force a team to use the same* ***blueprint*** *over and over again and follow the same* ***structure and protocol***, even if by so the solution may be a bit over-engineered. The blocks are meant to be replicated many times and set a common agreement to always proceed the same way (forced by the abstract class).  
>  
>IMO the final abstraction seems to be clear and it makes easy to understand whats happening, in which order things are being processed, etc... The transformation itself (`main_pipe`) is also clear and shows the steps explicitly.

In a typical routine, there are 3 well defined steps:

* Read/parse data
* Transform data
* Export processed data

Basically, an ETL process. This could be solved in a functional way. You can even go the extra mile by following `pipes` chained methods (as brilliantly explained here [https://tomaugspurger.github.io/method-chaining](https://tomaugspurger.github.io/method-chaining))

It is clear the `pipes` approach follows the same *parseâ†’transformâ†’export* structure. This level of cohesion shows a common pattern that could be defined into an `abstract class`. This `class` defines the bare minimum requirements of a **pipe**, being of course always possible to extend the functionality of any instance if needed.

By defining the `Base class` as such, we explicitly force a cohesive way of defining `DataProcessPipe` (*pipe* naming convention may be substituted by *block* to avoid later confusion with **Scikit-learn** `Pipelines`). This base class contains `parse_data`, `export_data`, `main_pipe` and `process` methods

In short, **it defines a formal interface that describes what any process block/pipe implementation should do.**

A specific implementation of the former will then follow:

    
    from processing.base import DataProcessPipeBase
    
    class Pipe1(DataProcessPipeBase):
    
        name = 'Clean raw files 1'
    
        def __init__(self, import_path, export_path, params):
            self.import_path = import_path
            self.export_path = export_path
            self.params = params
    
        def parse_data(self) -> pd.DataFrame:
            df = pd.read_csv(self.import_path)
            return df
    
        def export_data(self, df: pd.DataFrame) -> None:
            df.to_csv(os.path.join(self.export_path, index=False)
            return None
    
        def main_pipe(self, df: pd.DataFrame) -> pd.DataFrame:
            return (df
                     .dropnan()
                     .reset_index(drop=True)
                     .pipe(extract_name, self.params['extract'])
                     .pipe(time_to_datetime, self.params['dt'])
                     .groupby('foo').sum()
                     .reset_index(drop=True))
    
        def process(self) -> None:
            df = self.parse_data()
            df = self.main_pipe(df)
            self.export_data(df)
            return None

With this approach:

* The ins and outs are clear (this could be one or many in both cases and specify imports, exports, even middle exports in the `main_pipe` method)
* The interface allows to use indistinctly **Pandas**, **Dask** or any other library of choice.
* If needed, further functionality beyond the `abstractmethods` defined can be implemented.

Note how parameters can be just passed from a **yaml** or **json** file.

For complete processing pipelines, it will be needed to implement as many DataProcessPipes required. This is also convenient, as they can easily be then executed as follows:

    from processing.pipes import Pipe1, Pipe2, Pipe3
    
    class DataProcessPipeExecutor:
        def __init__(self, sorted_pipes_dict):
            self.pipes = sorted_pipes_dict
    
        def execute(self):
            for _, pipe in pipes.items():
                pipe.process()
    
    if __name__ == '__main__':
        PARAMS = json.loads('parameters.json')
        pipes_dict = {
            'pipe1': Pipe1('input1.csv', 'output1.csv', PARAMS['pipe1'])
            'pipe2': Pipe2('output1.csv', 'output2.csv', PARAMS['pipe2'])
            'pipe3': Pipe3(['input3.csv', 'output2.csv'], 'clean1.csv', PARAMS['pipe3'])
        }
        executor = DataProcessPipeExecutor(pipes_dict)
        executor.execute()

## Conclusion

Even if this approach works for me, I would like this to be just an example that opens conversations towards proper project and software architecture, patterns and best practices among the Data Science community. I will be more than happy to flush this idea away if a better way can be proposed and its highly standardised and replicable.

If any, the main questions here would be:

* Does all this makes any sense whatsoever for this particular example/approach?
* Is there any place, resource, etc.. where I can have some guidance or where people are discussing this?

Thanks a lot in advance

\---------

PS: this first post was published on StackOverflow, but was erased cause -as you can see- it does not define a clear question based on facts, at least until the end. I would still love to see if anyone is interested and can share its views.",1635979901.0,2021-11-03 23:51:41
[D] FUZZY C-MEANS Clustering on line graph data,1,qn0ynq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qn0ynq/d_fuzzy_cmeans_clustering_on_line_graph_data/,1,"Hi I've been trying to apply fuzzy c-means on data that can be represented as line graphs (hourly electrical load profiles). I understand that I must cluster the points on each hour. What I don't get is how do I relate the clustered points on each hour to the adjacent hour?  So that I can obtain the result which is a clustered line graph. 

Here is an example of an input and the desired output.

[Input: Electrical Load Profiles](https://preview.redd.it/icxk687mqox71.png?width=941&format=png&auto=webp&s=179848d6fe69c4f353eb925fad079538f09f8b1f)

&#x200B;

[Output: Clustered Results](https://preview.redd.it/v4y7969oqox71.png?width=992&format=png&auto=webp&s=6589adfb44f023dd23f8be00a1d3947c57a79819)",1636078590.0,2021-11-05 03:16:30
[D] Is there any way for GAN to generate arbitrary length of time series signal?,3,qmq7hb,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmq7hb/d_is_there_any_way_for_gan_to_generate_arbitrary/,4,"Hello, I'm working on using GAN to generate some signals. As I have viewed some related works, I found that most of them merely sample a latent vector from some distribution with fixed size *(e.g a latent vector of dim 64)*, and after some upsampling operation, they will get signals in a fixed window size *(e.g a signal of 4s \* 256Hz = 1024 points)*.

I want to get rid of the annoying limit of ""fixed window size"", and be able to generate continuous signal with arbitrary length.Â 

I tried to code by myself. I have designed a GAN framework , in which the generator takes an input of an arbitrary length of vector, and outputs a signal in same length as the input. The upsampling process is thrown away here, so the generator merely do some modification on the input signal instead of upsampling on it. As for discriminator, I use global average pooling in replace of the linear layers. However, my code failed to work. So I think maybe I need some new ideas.

I come to you guys for help. Do you know any paper that might be helpful for me? Or do you have any good idea?

Thanks!",1636047990.0,2021-11-04 18:46:30
[D] SageMaker Linear Learner,0,qmxns9,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmxns9/d_sagemaker_linear_learner/,2,"Hi Everyone,

I was just wondering.. does anyone know where there is a corresponding library in cran or scikit-learn for the linear learner in AWS sage maker?

I do not have access to AWS so I can't tell whether it is just a interface to different regressions  or something more sophisticated.

Enjoy your day,   
Fella",1636068276.0,2021-11-05 00:24:36
[R] Washington U & Google Study Reveals How Attention Matrices Are Formed in Encoder-Decoder Architectures,4,qmln6l,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmln6l/r_washington_u_google_study_reveals_how_attention/,0,"In the new paper Understanding How Encoder-Decoder Architectures Attend, researchers from the University of Washington, Google Blueshift Team and Google Brain Team propose a method for decomposing hidden states over a sequence into temporal- and input-driven components, revealing how attention matrices are formed in encoder-decoder networks. 

Here is a quick read: [Washington U & Google Study Reveals How Attention Matrices Are Formed in Encoder-Decoder Architectures.](https://syncedreview.com/2021/11/04/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-138/)

The paper *Understanding How Encoder-Decoder Architectures Attend* is on [arXiv](https://arxiv.org/abs/2110.15253).",1636035633.0,2021-11-04 15:20:33
[R] Koopman Q-learning: Offline Reinforcement Learning via Symmetries of Dynamics,17,qmc9mb,MachineLearning,https://arxiv.org/abs/2111.01365,1,,1635998223.0,2021-11-04 04:57:03
[R] Pruning for Self-Supervised Speech Recognition,1,qmsyf8,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmsyf8/r_pruning_for_selfsupervised_speech_recognition/,0,"MIT News: [https://news.mit.edu/2021/speech-recognition-uncommon-languages-1104?fbclid=IwAR0X7bTI1BIT9Lpq07Kb6evvDhgBdoSPEcT4f9IzwOCR3-IgybNvdtMZblo](https://news.mit.edu/2021/speech-recognition-uncommon-languages-1104?fbclid=IwAR0X7bTI1BIT9Lpq07Kb6evvDhgBdoSPEcT4f9IzwOCR3-IgybNvdtMZblo)

Paper (NeurIPS 2021): [https://arxiv.org/abs/2106.05933](https://arxiv.org/abs/2106.05933)",1636055444.0,2021-11-04 20:50:44
[R] Local Latin Hypercube Refinement for Multi-objective Design Uncertainty Optimization,3,qmj7tq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmj7tq/r_local_latin_hypercube_refinement_for/,3,"Many real world systems consist of input features with aleatoric (i.e. irreducible) uncertainties. In engineering design applications, such uncertainties may arise from production tolerances, operational conditions as well as other environmental factors. Thus, the distribution of these features can be measured and to some extent modified (e.g. by moving the mean value).

Design uncertainty optimization seeks to find the distribution parameters of input features, which optimize metrics such as the failure probability and the variance of key performance indicators besides the expected objective values. Since these often require uncertainty quantification of black-box functions, the whole process is computationally quite burdensome. In this work, we propose using machine learning methods in combination with sequential sampling to reduce the required amount of computation and accelerate the uncertainty optimization task. Due to the small data setting, we limit the investigation to GPR and SVR but argue that a more suitable model can exist depending on the problem, which could be used within the proposed framework instead.

[https://arxiv.org/abs/2108.08890](https://arxiv.org/abs/2108.08890)",1636028027.0,2021-11-04 13:13:47
[D] Pairprogramming/ pair analysis in ML development teams to improve harmony?,1,qmrr7t,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmrr7t/d_pairprogramming_pair_analysis_in_ml_development/,0,"Hi.

I'm gonna assemble a small team to work on an ML project, which might lead to an ML first software, and I was wondering about whether using pair programming can be beneficial in ML teams, especially in early stages, which is adventurous analysis and exploration.  What is your experience with pair programming in ML and in what stages do you think it can be good? With what strategy and etc.

We are all working remotely, so that's another aspect that is interesting. What is your experience, and how do you create harmony in a newly assembled team that is working remotely?",1636052233.0,2021-11-04 19:57:13
[P] iris - Open Source Photos Platform powered by PyTorch,90,qm06ct,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm06ct/p_iris_open_source_photos_platform_powered_by/,5,"This is my submission for PyTorch Annual Hackathon 2021! A self-hosted alternative to Google Photos. Currently it contains basic features built with short scope of hackathon. The team will be continuing to work by adding new features.

[Explore Section](https://preview.redd.it/cft3mb6o6fx71.png?width=2784&format=png&auto=webp&s=e204c8d6e20e98011f853938b5cf8ed02de61bc5)

[Smart Search](https://preview.redd.it/7w1xu76o6fx71.png?width=2784&format=png&auto=webp&s=b171397d4dcac4a9df88b8df6706e3560bdb9fd6)

Go check out and support the project now from below links!

YouTube: [https://www.youtube.com/watch?v=ZMG2rohochc](https://www.youtube.com/watch?v=ZMG2rohochc)

DevPost: [https://devpost.com/software/iris-7s3yna](https://devpost.com/software/iris-7s3yna)

GitHub: [https://github.com/prabhuomkar/iris](https://github.com/prabhuomkar/iris)

&#x200B;",1635962267.0,2021-11-03 18:57:47
[D] Best approach for noisy language detection,0,qmw43e,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmw43e/d_best_approach_for_noisy_language_detection/,0,"I need to predict the language (e.g. English, Portuguese, Russian) from a few words / sentences. This is noisy real-world data, meaning that the words might be misspelled, have poor grammar, emojis, language switching etc.

There's a bunch of different GitHub repos out there, but it's unclear which one works well, especially for noisy real-world data and not e.g. Wikipedia. I had hoped that websites like [https://paperswithcode.com/area/natural-language-processing](https://paperswithcode.com/area/natural-language-processing) would have solved this by now, but I don't see a relevant category for language detection. I don't have strong requirements for the solution to be particularly efficient although a transformer-approach seems overkill for this.

Any advice on a simple Python library to use?",1636063956.0,2021-11-04 23:12:36
[D] Feedback on our idea - a platform that turns code into a monetized API,0,qmq67b,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qmq67b/d_feedback_on_our_idea_a_platform_that_turns_code/,6," 

Hi,

we're building a platform that turns code into an API , hosted by us. infrastructure is paid by whoever calls your service (we charge the exact amount the infra costs us) if you choose to you can also monetize your service.

as a data scientist myself I'm really excited about the possibilities of easily sharing my models' capabilities as an API without dealing with dev-ops , but I'm biased because I love what we're building. I really want to know what do you think about using such a platform (not mentioning the name, not sure it's allowed)

how many of you both create ML models and know how to set up your own API (and monetize it)?

we want smart people to create smart solutions and be able to share them easily with anyone.

creating such a service would be free, signing up is free and as a creator you can only earn if someone is using your service.

any thoughts?

Offer",1636047892.0,2021-11-04 18:44:52
[R] [2110.13771] AugMax: Adversarial Composition of Random Augmentations for Robust Training,4,qmik2k,MachineLearning,https://arxiv.org/abs/2110.13771,3,,1636025531.0,2021-11-04 12:32:11
[R] Intermediate Layer Optimization for Inverse Problems using Deep Generative Models,5,qmfsnd,MachineLearning,https://arxiv.org/abs/2102.07364,1,,1636013147.0,2021-11-04 09:05:47
[D] Paper Explained - EfficientZero: Mastering Atari Games with Limited Data (Full Video Analysis),21,qm6l31,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm6l31/d_paper_explained_efficientzero_mastering_atari/,2,"[https://youtu.be/NJCLUzkn-sA](https://youtu.be/NJCLUzkn-sA)

Reinforcement Learning methods are notoriously data-hungry. Notably, MuZero learns a latent world model just from scalar feedback of reward- and policy-predictions, and therefore relies on scale to perform well. However, most RL algorithms fail when presented with very little data. EfficientZero makes several improvements over MuZero that allows it to learn from astonishingly small amounts of data and outperform other methods by a large margin in the low-sample setting. This could be a staple algorithm for future RL research.

&#x200B;

OUTLINE:

0:00 - Intro & Outline

2:30 - MuZero Recap

10:50 - EfficientZero improvements

14:15 - Self-Supervised consistency loss

17:50 - End-to-end prediction of the value prefix

20:40 - Model-based off-policy correction

25:45 - Experimental Results & Conclusion

&#x200B;

Paper: [https://arxiv.org/abs/2111.00210](https://arxiv.org/abs/2111.00210)

Code: [https://github.com/YeWR/EfficientZero](https://github.com/YeWR/EfficientZero)

Note: code not there yet as of release of this video",1635980120.0,2021-11-03 23:55:20
[N] Zillowâ€™s NN-based Zestimate Leads to Massive Losses in Home Flipping Business,662,qlilnf,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlilnf/n_zillows_nnbased_zestimate_leads_to_massive/,211,"Zillow announced that they are [laying off a quarter of their workforce](https://www.cbsnews.com/news/zillow-layoffs-closing-zillow-offers-selling-homes/) due to a $420 million loss incurred by Zillow Offers, the home flipping arm of their business. The business model was reliant on [Zestimate](https://www.zillow.com/z/zestimate/), a neural network-based model that forecasts housing prices.

This seems like a colossal misstep on their part. It begs the question, how can other companies avoid a similar fate if they are making large gambles based on machine learning models predicting market movements? Additionally, how much should consumers rely on market predictions like Zestimate when making financial decisions (speaking as someone who recently bought a home and researched the market on Zillow during the process)?",1635899869.0,2021-11-03 01:37:49
[D] AAAI 2022 Paper Reviews,61,qlt4cz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlt4cz/d_aaai_2022_paper_reviews/,84,"Now that AAAI 2022 reviews are out, I am creating a discussion thread for this year's reviews.",1635941607.0,2021-11-03 13:13:27
[Project] Discover ongoing ML/AI competitions,14,qm2ov5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm2ov5/project_discover_ongoing_mlai_competitions/,0,"***Looking for feature suggestions for mlcontests.com!***

*It's been two years since I posted here about my then-new project* [ML Contests](https://mlcontests.com)*. It was well received and since it's been a while I thought I'd post an update and ask for feedback!* 

[Main page \(mlcontests.com\)](https://preview.redd.it/tohjmaw91gx71.png?width=1565&format=png&auto=webp&s=5e96185b838570cac26ab9b245b5ca9d03a5cd1b)

The main page just lists ongoing competitions. There's also a newsletter where I occasionally send out updates about the competitive ML space, and a separate page which compares cloud GPUs for ML.

You can visit the site at [MLContests.com](https://mlcontests.com).

Traffic is steady, the newsletter is growing, there are no ads, and I'd like to figure out where to take it next. **I'd love to hear your thoughts on what you want from the site!**

PS: If you want to contribute, it's all open source: [https://github.com/mlcontests/mlcontests.github.io](https://github.com/mlcontests/mlcontests.github.io) :)",1635969222.0,2021-11-03 20:53:42
[R] Can Vision Transformers Perform Convolution?,21,qlxivf,MachineLearning,https://arxiv.org/abs/2111.01353,2,,1635955036.0,2021-11-03 16:57:16
Neurosymbolic models vs. large transformers in program synthesis,11,qm15le,MachineLearning,https://arxiv.org/abs/2111.01633,2,,1635964991.0,2021-11-03 19:43:11
[D] Python toolboxes for probabilistic graphical model inference,5,qm578v,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm578v/d_python_toolboxes_for_probabilistic_graphical/,2,"Hi folks,

which libraries or toolboxes would you recommend (ideally based on personal experience) for performing inference in probabilistic graphical models, for an actual practical application and not for academic toy examples?

Minimal requirements: must be able to specify a Bayesian network or factor graph consisting of categorical nodes, some of which are hidden and others observed, and use a set of observations to identify the factors / dependencies. (I think essentially any PGM toolbox will fulfill these requirements?)

Bonus points given for:- good documentation- maturity/stability- works efficiently with many factors and many datapoints- implements many different inference methods and modeling paradigms- simplicity of use

I do know of a few promising toolboxes such as [pgmpy](https://github.com/pgmpy/pgmpy), [pymc3](https://docs.pymc.io/en/stable/), and [pyro](http://pyro.ai/), but have not used either of them (for this purpose) and am at a bit of a loss picking one to start with.",1635976141.0,2021-11-03 22:49:01
"[R] Neural Program Generation Modulo Static Analysis, Mukherjee et al, 2021 NeurIPS Spotlight",4,qm5nfs,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm5nfs/r_neural_program_generation_modulo_static/,1,"Nice paper - using program analysis as a learning signal for program synthesis

(I am not the author - alas..)

[paper](https://www.cs.utexas.edu/~swarat/pubs/neurips21-nsg.pdf)  

[twitter 1/10](https://twitter.com/swarat/status/1455587818160508933)",1635977358.0,2021-11-03 23:09:18
[D] Zero-shot models as input features in NLP classification tasks?,6,qlxl01,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlxl01/d_zeroshot_models_as_input_features_in_nlp/,2,"So places like huggingface offer zero shot models that pretty decently as a zero-shot classifier, and are easy to implement.  I was thinking, what if I just took a few zero shot models and added them as features and then fit a classifier on top?

 So let's say that I'm trying to classify toxic tweets. I might use an embedding model or a verctorizer to turn the tweet into model features. And then fit some classifier on top of this to try to predict the label from these text features. 

But what if I add some simple zero shot binary features? is\_angry\_model1, is\_sad\_model1, is\_news model1 etc and do this for one or more models?  Or even more simply use, is\_toxic\_model1, is\_toxic\_model2 and use their predictions as input features into a classifier. 

I was planning to experiment a bit with this, but wanted to get some thoughts on it and if there has been previous similar work I can use for reference?

Thanks!",1635955196.0,2021-11-03 16:59:56
[D] AdaConv explained - Adaptive Convolutions for Structure-Aware Style Transfer (5-minute summary by Casual GAN Papers),1,qm5axp,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm5axp/d_adaconv_explained_adaptive_convolutions_for/,0,"Classical style transfer is based on Adaptive Instance Normalization,  which is limited to transferring statistical attributes such as color distribution and textures while ignoring local geometric structures in the image. But that is the stuff of the past, let me introduce to you  Adaptive Convolutions, a drop-in replacement, for AdaIN, proposed by  Prashanth Chandran and the team at Disney research. AdaConv is able to transfer the structural styles along with colors and textures in real-time.

Full summary: [https://t.me/casual\_gan/165](https://t.me/casual_gan/165)

Blog post: [https://www.casualganpapers.com/style-conditioned-image-to-image-style-transfer/AdaConv-explained.html](https://www.casualganpapers.com/style-conditioned-image-to-image-style-transfer/AdaConv-explained.html)

[AdaConv](https://preview.redd.it/jdz6ruh1cgx71.png?width=2292&format=png&auto=webp&s=d9bffa23beee943beaa16781be3bcddbbefa9f79)

arxiv: [https://studios.disneyresearch.com/app/uploads/2021/04/Adaptive-Convolutions-for-Structure-Aware-Style-Transfer.pdf](https://studios.disneyresearch.com/app/uploads/2021/04/Adaptive-Convolutions-for-Structure-Aware-Style-Transfer.pdf)

code: [https://github.com/RElbers/ada-conv-pytorch](https://github.com/RElbers/ada-conv-pytorch)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",1635976434.0,2021-11-03 22:53:54
"[D] ""Learn AI Together"" Discord community is looking for experienced people to share their projects and willing to help other AI enthusiasts by answering questions from time to time",7,qlrxzz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlrxzz/d_learn_ai_together_discord_community_is_looking/,2,"Hey everyone! We are looking for researchers, teachers, teacher assistants, or professionals willing to help and exchange with people learning AI by answering questions from time to time.

We are an AI-focused community of over 20'000 people where members can chat, ask questions, share resources and projects, share/find job offers, etc. We are now focusing on getting experts or advanced members to join us and help us help others. Everyone else in the field is welcome to join as well! 

More info about the community and to join us: [Learn AI Together](https://www.louisbouchard.ai/learn-ai-together/)

Excited to chat with you there!

*Note that this is not a paid opportunity and we won't be asking for hours or anything. Help whenever you can! :)*",1635937066.0,2021-11-03 11:57:46
"[P] Loss function that penalizes classification errors heavily, or should I just modify log-loss?",2,qlujgx,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlujgx/p_loss_function_that_penalizes_classification/,12,"I'm working on a classification problem where predictions that are ""good enough"" are.... actually good enough.  So I don't need my model to spend time optimizing a top-1 90% to become a 98% or a 99%.  

However, I really care when the model makes incorrect decisions.  I've built a family of models of different sizes/parameter counts using traditional log-loss as my loss function, and each model will make catastrophically bad decisions on occasion.  And for my domain, a few catastrophically bad classifications can ruin tens of thousands of good classifications.  I'm wondering if there's a different loss function I can use that penalizes errors even more heavily than log-loss (log-loss squared?).

The natural question to ask is if I have errors in my data pipeline or model configuration.  I've debugged it quite a bit and I'm sure that's not my problem.  My domain (chess) naturally has a manifold that is so complex and twisted with many sharp edges and saddle points... It's a really difficult space to work in so any model smaller than GPT-3 is understandably going to have trouble.",1635946346.0,2021-11-03 14:32:26
[N] Extended submission deadline â€” EvoMUSART 2022 conference,1,qm0upg,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qm0upg/n_extended_submission_deadline_evomusart_2022/,0,"Good news: **The submission deadline of EvoMUSART 2022 has been extended to November 24th!** ðŸ™Œ

You still have time to submit your work to the 11th International Conference on Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART).

If you work with Artificial Intelligence techniques applied to visual art, music, sound synthesis, architecture, video, poetry, design or other creative tasks, don't miss the opportunity to submit your work to EvoMUSART.

EvoMUSART 2022 will be held in **Seville, Spain**, between 20 and 22 April 2022. ðŸ’ƒðŸ‡ªðŸ‡¸

For more information, visit the conference webpage: [evostar.org/2022/evomusart/](http://www.evostar.org/2022/evomusart/)

https://preview.redd.it/pue6n3usafx71.png?width=2083&format=png&auto=webp&s=1b2bfa549576b75720ced846ceab7954fa792787",1635964139.0,2021-11-03 19:28:59
[Research] Towards the Generalization of Contrastive Self-Supervised Learning,17,qllc70,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qllc70/research_towards_the_generalization_of/,0,"Some interesting results of contrastive learning theory ([https://arxiv.org/abs/2111.00743](https://arxiv.org/abs/2111.00743)):

1. The generalization ability of contrastive self-supervised learning depends on 3 factors: strength of **data augmentations**, **alignment** of positive samples, and **divergence** of class centers.
2. Data augmentation enables self-supervised learning. Good algorithms (which optimize the alignment and divergence factors well) with weak data augmentation still have bad performance.
3. Barlow Twins, which aims to decorrelate the different vector components of the representation, implicitly optimizes the geometry of embedding space to satisfy the alignment and divergence factors actually.",1635908768.0,2021-11-03 04:06:08
"[N] Researchers From Seoul National University, NVIDIA and Microsoft Release â€˜ACAV100Mâ€™: An Automatically Curated Video Dataset For Self-Supervised Audio-Visual Learning",4,qlqqow,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlqqow/n_researchers_from_seoul_national_university/,0,"Audio-visual (AV) learning is defined by delivering and applying instructional content that includes both sound and visual information. The natural relationship between visual observations and their accompanying sounds has shown strong self-supervision signals for learning video representations. That is why the massive amount of online videos has become a valuable source for self-supervised learning among research communities.Â 

However, due to overdubbed audio, online videos frequently provide imperfectly aligned audio-visual signals. Therefore, the models trained on uncurated films have been shown to develop poorer representations as a result of the misalignment difficulties. The existing techniques typically rely on manually curated datasets with a predetermined taxonomy of semantic ideas, where the audio-visual connection is highly likely.

To overcome this gap, researchers from Seoul National University, NVIDIA and Microsoft have released an automatic dataset curation pipeline and a large video dataset for self-supervised audio-visual learning, termed ACAV100M (automatically curated audio-visual dataset). The dataset is made up of a massive number of uncurated web videos. The researchers took 140 million full-length videos and reduced them to 100 million segments with the best audio-visual correspondence.Â 

Checkout the [Paper](https://arxiv.org/pdf/2101.10803.pdf), [Codes](https://github.com/sangho-vision/acav100m), [Project](https://acav100m.github.io/), [Microsoft Blog](https://www.microsoft.com/en-us/research/blog/acav100m-scaling-up-self-supervised-audio-visual-learning-with-automatically-curated-internet-videos/), [Video Presentation](https://www.youtube.com/watch?v=VR0eh0iVH8Q) and a [Short Read from Us](https://www.marktechpost.com/2021/11/03/researchers-from-seoul-national-university-nvidia-and-microsoft-release-acav100m-an-automatically-curated-video-dataset-for-self-supervised-audio-visual-learning/?_ga=2.130414219.529287311.1635906011-1849463555.1635487993).

&#x200B;

&#x200B;

https://preview.redd.it/p3klq1yvmcx71.png?width=1024&format=png&auto=webp&s=750c51e058da29510d7bd1dcc44df17a24df7404",1635931558.0,2021-11-03 10:25:58
[D] Raspberry Pi and ML Frameworks - any benchmarks for training or inference?,5,qlq3rt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlq3rt/d_raspberry_pi_and_ml_frameworks_any_benchmarks/,1,"Hey folks!Do you know any benchmarks of available ML frameworks (TF, Pytorch, ...) for **training** (toy data data sets) **and inference** on a **Raspberry Pi** (or other edge devices)?",1635928436.0,2021-11-03 09:33:56
"[D] Are GNNs/GCNs viable for graphs with no node features, with only the unique node IDs? Are they different from DeepWalk at that point?",2,qlqls4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlqls4/d_are_gnnsgcns_viable_for_graphs_with_no_node/,3,"I started to dig into GNNs for the first time and I have trouble understanding its advantages over NLP inspired embedding methods like DeepWalk and node2vec. Do GNNs only shine with node features? Or can they handle IDs/giant one-hot vectors as well? Does the usual input for GNNs only consist of a vector of handcrafted features? Are GNNs used directly for tasks like link prediction or they are just embedding generators for other models?

I appreciate all and any explanations.",1635930857.0,2021-11-03 10:14:17
[R] Twitter Cortex Proposes LMSOC for Socially Sensitive Pretraining,0,qlwcgx,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlwcgx/r_twitter_cortex_proposes_lmsoc_for_socially/,1,"In the new paper LMSOC: An Approach for Socially Sensitive Pretraining, a Twitter Cortex research team proposes a simple but effective approach for learning both linguistically contextualized and socially sensitive representations in large-scale language models. 

Here is a quick read: [Twitter Cortex Proposes LMSOC for Socially Sensitive Pretraining.](https://syncedreview.com/2021/11/03/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-137/)

The LMSOC code is available on the projectâ€™s [Github](https://github.com/twitter-research/lmsoc). The paper *LMSOC: An Approach for Socially Sensitive Pretraining* is on [arXiv](https://arxiv.org/abs/2110.10319).",1635951638.0,2021-11-03 16:00:38
[P] Text-to-image models ruDALL-E Kandinsky (XXL) (12 billion parameters) and ruDALL-E Malevich (XL) (1.3 billion parameters). A demo for the latter is available.,37,qlbye5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlbye5/p_texttoimage_models_rudalle_kandinsky_xxl_12/,12,"[Technical report (Russian)](https://habr.com/ru/company/sberbank/blog/586926/).

[Technical report (translated to English by Google Translate)](https://habr-com.translate.goog/ru/company/sberbank/blog/586926/?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=nui).

[Demo for ruDALL-E Malevich (XL)](https://rudalle.ru/demo).

[GitHub repo for ruDALL-E Malevich (XL)](https://github.com/sberbank-ai/ru-dalle).

[More links from my other post](https://www.reddit.com/r/bigsleep/comments/ql9n81/comment/hj1d7zo/).",1635880933.0,2021-11-02 20:22:13
[D] Why do we apply batch normalization between layers,81,ql5hdb,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/ql5hdb/d_why_do_we_apply_batch_normalization_between/,25,"After batch normalization we are basically trying to get the unit  gaussian output. Initialising the data with unit gaussian seems to be a  good idea but doing so in between the network, how does that make sense?",1635863442.0,2021-11-02 15:30:42
[D]Using Perplexity for Evaluating Language Models,0,qlvjdy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlvjdy/dusing_perplexity_for_evaluating_language_models/,1," 

Hello,

Assume a training dataset has ten sentences, and they are used for constructing a probabilistic language model consisting of the Maximum Likelihood Estimates for all of the different combinations of bigrams for these ten sentences.Â  To test the accuracy of this bigram language model, a test dataset consisting of twenty sentences are used for calculating the probability for each of these test sentences using the bigram conditional probabilities that were calculated using all of the sentences in the training set.Â  To evaluate the accuracy of this bigram language model, there is a need to calculate the Perplexity measure for indicating the accuracy of the probabilistic language model.Â  Because of the test dataset having twenty sentences, is it correct to calculate the Perplexity value for each of these twenty sentences using their probability values and then taking the average of these twenty Perplexity values for calculating the overall Perplexity value for the test dataset or is it correct to calculate the probability for each of the twenty sentences and then find the simple average of these twenty probability values to give a simple probability average which is then used for calculating the Perplexity value for the test dataset?Â  Would this procedure apply for a test dataset having any number of sentences?Â  Would this procedure apply for a trigram model?Â  Would this procedure apply for any n-gram models like a quadrogram model?Â  The goal is to always have a probabilistic language model having the lowest Perplexity value.

Thanks.",1635949300.0,2021-11-03 15:21:40
[R] Neurips 2021 Accepted Paper List,47,ql9d4s,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/ql9d4s/r_neurips_2021_accepted_paper_list/,8,"List of accepted papers now appears to be public: [https://neurips.cc/Conferences/2021/AcceptedPapersInitial](https://neurips.cc/Conferences/2021/AcceptedPapersInitial)  


Spot any particularly interesting ones?",1635873997.0,2021-11-02 18:26:37
[D] What is the state-of-the-art for few-shot text classification?,1,qlugqf,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlugqf/d_what_is_the_stateoftheart_for_fewshot_text/,6,Say I have many text snippets that can be one of four classes. I also cannot get a large-scale labeled dataset (I have ~30-50 labeled examples per class). What methods are currently state-of-the-art for such settings?,1635946116.0,2021-11-03 14:28:36
[D] Which apps in the real world would you like to connect your ML models with?,1,qlsp3s,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlsp3s/d_which_apps_in_the_real_world_would_you_like_to/,5,"Hi all! As a side-project I am building a tool to **connect the ML models you make in your Jupyter Notebook to apps in the real world** with just a few lines of extra code.

As an example, think of building a model that predicts *Customer Churn* in a Jupyter Notebook. We make sure that it pulls new customers from **the Mailchimp account** of your company and **make it run as a Discord or Slack bot**! You can sign up for early access to the tool [here](https://flow.magicsheets.io).

I am building fast, and I wanted to ask **what apps** you think **would be awesome to integrate your Machine Learning models with**?

So far we have:

* Mailchimp
* Shopify
* Slack
* Google Sheets

Thanks in advance!",1635940037.0,2021-11-03 12:47:17
[D] What do Machine Learning Engineers at Facebook do?,202,qkyini,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkyini/d_what_do_machine_learning_engineers_at_facebook/,152,I was approached by an interviewer for this position and had a hard time grasping what they work on on a day-to-day basis. Thanks in advance!,1635836173.0,2021-11-02 07:56:13
[D] Neural architecture search and Neuroevolution,9,qlffsv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlffsv/d_neural_architecture_search_and_neuroevolution/,2,"Hi fellow readers,

I've come recently to a slight confusion about how to understand NAS and Neuroevolution. This is why I would like to hear your explanations.

My current understanding is as follows:

**NAS** \- technique which can be used to automate the process of designing/optimizing neural networks (NNs) \[[1](https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html#evaluation-strategy)\]. The technique is further divided into three components such as:

* Search space - defines types of layers, depth, type of connections
* Search strategy - defines the approach used to explore search space
* Evaluation strategy - evaluates the performance of build ANN from its design

**Neuroevolution** \- a technique that harnesses evolutionary algorithms (EA) to design/optimize NNs. It can augment the NN by changing the topology, connections and weights \[[2](http://www.scholarpedia.org/article/Neuroevolution)\] based on observed action in the environment.

Confusion:

Therefore, from the above description, I don't understand if NAS and Neuroevolution can be explained as applied techniques to design/optimize NN topologies. Where NAS can use any kind of algorithm to design NN topologies (RL, EA, Gradient descent) and Neuroevolution uses only EA?

**OR**

Neuroevolution can be just defined as a search strategy in NAS? This means Neuroevolution can be used in NAS (search strategy) like for example Reinforcement learning (RL) \[[3](https://arxiv.org/abs/1611.01578)\].

To simplify, I would like to understand, how can I think about NAS and Neuroevolution, when researching. My goal is to understand how I can put all the puzzles together when building an automated machine learning process for a specific task as anomaly detection. 

If I made any mistakes or silly comparisons please point them out in the comments, that future readers can grasp my mistakes and your knowledge. Any comments are more than welcome!

Thank you in advance.",1635890550.0,2021-11-02 23:02:30
[R] Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey,2,qlkg63,MachineLearning,https://arxiv.org/abs/2111.01243,1,,1635905816.0,2021-11-03 03:16:56
[D] Did anyone check ykilcher's video of Siraj Raval's interview,52,qkyyno,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkyyno/d_did_anyone_check_ykilchers_video_of_siraj/,60,"I love Yannic's video but I did not see any point of this interview. I mean even in the interview Siraj seemed like someone who has just started learning machine learning, when he mentions about ""Superintelligence digital organism god"", seems like he imagines ML as a hollywood movie (much like the general person).",1635838260.0,2021-11-02 08:31:00
[P] natural language only coding with co-pilot stream 11/2 10PM PST,1,qliy7s,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qliy7s/p_natural_language_only_coding_with_copilot/,2,"Kinda late to the party, but I just got access to github's copilot AI-backed code auto-complete tool, it can do some pretty impressive things. I have not played with it for more than an hour, and I'm pretty impressed.

At 10PM PST today, I will be streaming at [twitch.tv/evanthebouncy](https://twitch.tv/evanthebouncy) for about 1-2 hours, where I will be attempting to perform simple coding exercises by writing \_only comments\_ and letting co-pilot complete the code from my natural language inputs. It'll be fun if you can come and spam some ideas in case you haven't had a chance to play with it yet.

I will also be giving some commentary / reactions to it, as I work in program synthesis for a living, and this is a pretty cool piece of tech that will definitely change how people think about programming in the near future.

mod, if this is kinda spammy feel free to just delete the thread, idc.",1635900997.0,2021-11-03 01:56:37
[D] What's the best simple machine learning API service?,2,qlcj9r,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qlcj9r/d_whats_the_best_simple_machine_learning_api/,3,"I'm looking integrate machine learning into my application (for example, I want to classify images users are uploading to my site). I'm aware there's a lot of these SAAS machine learning companies but I was wondering if anyone here had recommendations as to which ones worked best for them? I basically just want to send all my data to a service, train a model, the be able to call an API to get an answer from the model.",1635882524.0,2021-11-02 20:48:44
[D] AAAI FastTrack 2021 Review Results,29,qkxatt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkxatt/d_aaai_fasttrack_2021_review_results/,34,Good luck everyone! Results gonna be out soon for AAAI 2022!,1635830970.0,2021-11-02 06:29:30
[D] Why hasn't BERT been scaled up/trained on a massive dataset like GPT3?,129,qklvfp,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qklvfp/d_why_hasnt_bert_been_scaled_uptrained_on_a/,41,"Both architectures can be trained completely unsupervised, so why has GPT been scaled up and not BERT? Is it a software limitation?",1635795508.0,2021-11-01 20:38:28
[P] Scientific Literature Review generation,7,ql2qjz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/ql2qjz/p_scientific_literature_review_generation/,4," Hello everyone,

I've developed an algorithm to automatically generate a literature review : [https://www.naimai.fr](https://www.naimai.fr/)  
Hopefully that could be useful for the PhDs (and the non PhDs) !

For those curious to understand how it works : [https://yaassinekaddi.medium.com/scientific-literature-review-generation-386f36b05eae](https://yaassinekaddi.medium.com/scientific-literature-review-generation-386f36b05eae)

I'll be thankful if you have any remarks about that :)

Cheers,",1635854811.0,2021-11-02 13:06:51
[R] Unsolved Problems in ML Safety,16,qkxtjo,MachineLearning,https://arxiv.org/abs/2109.13916,1,,1635833155.0,2021-11-02 07:05:55
[R] Sequence-to-Sequence Learning with Latent Neural Grammars,11,qkylaz,MachineLearning,https://arxiv.org/abs/2109.01135,1,,1635836498.0,2021-11-02 08:01:38
[D] how to correlate the results from an extremely imbalance data to performance relative to a random guess,6,qkwk6j,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkwk6j/d_how_to_correlate_the_results_from_an_extremely/,17,"Hi all,

At a project at work I handle with an extremely imbalance dataset - 699 cases of positive outcome while around 15,000 negative cases.

In addition, our cases are relatively hard to separate and in some cases domain knowledge experts are struggling with manual classification.

In that context, I was asked to explore different classifiers and present them in a POC report.

At first I tried a naive approach and dumped the data as it is (used a train, validation, test splitting with stratification option on). All the models predicted 0 all the time, to maximize accuracy.
Then, I used over sampling with smote package in python, and changed the criteria to the area under the precision-recall curve. In the text set, I predicted correctly 30% (21/70) and I had a false positive rate of around 12%.
Regardless of possible model modification or boundary (predict 1 if the model predict a higher score than 0.65, for example), I am having some problems in defining my metric to evaluate the results. Our data is imbalanced, so 30% is good? In addition, false negative is strongly worse than false positive.

In addition, today I presented the results to my manager and he asked me to prove him (or argue) that the results are better than random guessing. 

I thought about two things to evaluate my results:
- Randomly draw 1000 observations, with a prior of 5% equal to 1, the rest to 0. Than randomly guess 5% of them to be 1 and compare it to my results. Bootstrap this scheme to get a distribution, and check where is my model performance along the distribution.  

-Take the examples from the test set, assign the same number or normal observations, and give the hr experts to classify them manually. Then compare results.

I will be glad to hear what do you think about the problem and the suggestions.

Thanks :)",1635828087.0,2021-11-02 05:41:27
Plagiarism Case Detected @ ICLR 2022 [News][Discussion],168,qkb6ga,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkb6ga/plagiarism_case_detected_iclr_2022_newsdiscussion/,27,"[https://openreview.net/forum?id=EO4VJGAllb&noteId=Ks7TmTUsyXa](https://openreview.net/forum?id=EO4VJGAllb&noteId=Ks7TmTUsyXa)

&#x200B;

https://preview.redd.it/zwpndspxpyw71.png?width=1153&format=png&auto=webp&s=d1105dfd3ae9da5fb56c1e0fc8b9652a6c9a1cef

The submission was withdrawn by the authors before the Program Chairs posted a desk-reject citing a serious case of plagiarism? What is happening?

The figures and tables do look like they've been lifted straight from previous papers.",1635763106.0,2021-11-01 11:38:26
[D] How to generate images from text with CLIP + VQGAN (easy to follow 5-minute tutorial by Casual GAN Papers),11,qksq6q,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qksq6q/d_how_to_generate_images_from_text_with_clip/,0,"[Cartoon village in a mushroom valley trending on ArtStation](https://preview.redd.it/gmcb3yvf03x71.png?width=1024&format=png&auto=webp&s=f847b72dff915b0d30898bcefde7bb00efdb5bd5)

Hey everyone!

Have you been playing with GANs for a while and want to create something yourself? Do you want to try out those text-to-image google colabs for generative art you have seen on Twitter but are not sure where to get started? Then this tutorial is for you!

My name is Kirill, and I have been writing weekly ML paper summaries for almost 9 months over at [casualganpapers.com](https://casualganpapers.com), and while they are helpful to a lot of people already working in the generative modeling field, I realized the digests are not as interesting to those just starting their generative AI journey.

This is why I am starting a new series of posts focused on quickly getting you started in the world of generative art, 3D, AI-based image editing, and more. 

Check out the first post on how to use the popular CLIP + VQGAN colabs to create beautiful generative art in just 5-10 minutes (excluding the training time):

[https://www.casualganpapers.com/howto-clip-vqgan-text-guided-image-generation-explained/VQGAN-CLIP-tutorial.html](https://www.casualganpapers.com/howto-clip-vqgan-text-guided-image-generation-explained/VQGAN-CLIP-tutorial.html)

If you enjoyed the tutorial make sure to follow Casual GAN Papers on telegram ([https://t.me/casual\_gan](https://t.me/casual_gan)) or Twitter ([https://twitter.com/KirillDemochkin](https://twitter.com/KirillDemochkin)) to get notified when the next post is released!

Take care,  
Kirill",1635815260.0,2021-11-02 02:07:40
[R] Top 7 Books to Boost Your Data Driven Outlook,179,qkes8a,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkes8a/r_top_7_books_to_boost_your_data_driven_outlook/,13,"**In this post, I will cover the best 7 books for data analysts. These data analytics books will teach you about the power of big data and ways to harness it.**

I started my career as a Software Developer and switched to data science 8 years ago when big data software projects were difficult to predict and risky to conduct due to large volumes of unclassified data and many types of metrics. Using machine learning, data analysis, and visualization approaches was essential for facilitating informed decision-making throughout the software development and testing process.

Mastering data analysis was one of the most challenging experiences in my life. Wading through tons of books to figure out where to start and which methods and techniques to use in a particular case can be extremely daunting and time-consuming.

If you have been studying data analytics for some time, choosing the right educational resources is crucial to launching and advancing your career within this area.  

# 1.  Storytelling with Data: A Data Visualization Guide for Business Professionals

As a data analyst, your aim is not just to retrieve data but also to make it intelligible, which requires you to be able to present the data in a certain way.  However, presenting data does not imply dragging and dropping data fields into a chart. It entails creating a meaningful visual representation of the data. This book is based on real-life scenarios and will give you some idea on the difference between colorful visualization and intelligent visualization, explaining why you should closely examine each line and color on your visual interface.

This book provides excellent guidance, examines criteria, and presents examples of how to properly deal with data.

# 2. Mastering Tableau 2021: Implement advanced business intelligence techniques and analytics with Tableau, 3rd Edition  

As a business analytics practitioner, I search for publications that can simplify complicated topics in a manner that everyone can understand. The book contains several tips and techniques that will assist you in understanding when to utilize particular chart styles, at what data granularity, and with what sort of presentation for the end user. You will begin this fascinating trip by learning essential strategies for using sophisticated math to tackle challenging situations. These strategies involve the inventive use of several sorts of computations, such as row-level, aggregate-level, and others. Besides, you will get concise instructions on using Tableau to solve practically any data visualization problem by knowing the tool's (inner workings and thinking creatively about possibilities.) expanded capabilities,

After reading the book, you will be equipped with an arsenal of advanced chart types and methods that will allow you to display information to a range of audiences in an effective and engaging manner using clear, efficient, and engaging dashboards. Explanations and examples of effective and inefficient visualization approaches, well-planned and badly created dashboards, and compromise choices when Tableau users do not embrace data visualization, will expand your knowledge of Tableau, so that you get the most of this powerful tool.

# 3. Machine Learning with the Elastic Stack - Second Edition  

This book is a one-of-a-kind resource for users using Elastic search. I With actual case, it focuses on the substantial growth of machine learning technology in Elastic search providing actual case studies and extensive explanation. This book is similar to having a one-on-one conversation with a subject matter expert. If you need to refresh your practical skills in machine learning, the book offers examples of how to apply Elastic ML in your environment, get valuable insight into your data, and how you can turn machine learning from static to intelligent. If you want to understand not just how to build tasks but also tap into the underlying models and variables, Machine Learning with the Elastic Stack is the ideal option for you. 

# 4. Data Analytics Made Easy: Analyze and present data to make informed decisions without writing any code

With data literacy being such an important component of a data-driven mindset, this book is an excellent resource for data science students looking to obtain practical information and learn how to apply their analytical skills. The author does an excellent job of introducing readers to KNIME, a low-code data analytics framework that allows to instantly evaluate data. Furthermore, his presentation of machine learning is user-friendly, with an emphasis on theoretical knowledge and handling a variety of use cases. More significantly, De Mauro assists readers in comprehending the significance  of becoming a great data presenter, a vital talent to cultivate in order to influence decision-making.

# 5. Fundamentals of Machine Learning for Predictive Data Analytics, Second Edition: Algorithms, Worked Examples, and Case Studies  

Fundamentals of Machine Learning for Predictive Data Analytics is a detailed analysis of the most important machine learning methods used in predictive data analytics, encompassing both theoretical principles and actual implementations. Technical and mathematical knowledge is complemented with instructional practical examples, and case studies show how these models may be employed in a wider business setting.

Following a description of the journey from extracting data to gaining insights and making a prediction, the book delves into the most essential machine learning techniques: data-based learning, correlation-based learning, probability and error-based learning. Each of these strategies starts with a no- tech description of the core principle, followed by quantitative models and algorithms demonstrated with extensive practical examples.

The authors discuss the procedures in a straightforward and succinct way, without referring to any specific programming frameworks or languages. They do a fantastic job of introducing the main concepts before diving deeper into the complexities of the logic and math underpinning the algorithms.

# 6.  Analytics Stories: Using Data to Make Good Things Happen

Analytics Stories: How to Make Good Things Happen is a serious, intelligent, and entertaining look at how analytics can tackle real-world problems and situations. Analytics Stories fills the gap between data analytics and the particular challenges it solves, with topics ranging from sports to finance, politics, healthcare, and commerce.

The author does an outstanding job of conveying the notion of data storytelling to the reader. He develops around 50 business cases on topics ranging from education to sports. Dr. Winston mostly utilized MS Excel to interpret, analyze, display, and successfully convey the data.

# 7.  Data Pipelines Pocket Reference: Moving and Processing Data for Analytics

A data science pipeline is a set of procedures that transform raw data into meaningful business responses. Data science pipelines streamline data validation, extract, transform, load machine learning and modeling, revision, so their implementation is crucial for data analytics success. The difference between having data and truly deriving value from it is moving data from various sources and processing it to create context.

This helpful reference describes common pipeline failures and key decision factors like batches vs. streaming data input and building vs. purchasing. This book delves into fundamental concepts that apply to open source systems, consumer applications, and homegrown solutions, as well as the most common decisions made by experts.

Data Pipelines Pocket Reference is a precious resource for all of the everyday problems and activities you are likely to encounter, if you work in data analysis or a related field that will assist you in making data-driven decisions for many years to come.

# Conclusion

Having a thorough grasp of data analytics and knowing how to gain actionable data-driven insights are essential for a successful career in data science. Anyone interested in expanding their knowledge of data analytics can benefit from the books mentioned in this article, since they provide the most recent industry information illustrated by examples of best practices.",1635775707.0,2021-11-01 15:08:27
[D] To phd or not to phd?,33,qkfuzn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkfuzn/d_to_phd_or_not_to_phd/,49,"I made the following post on other subs too. Just posting it here to get the input from larger machine learning community.

Hi all,

I recently completed my research based masters in computer vision and currently working in a company as a computer vision researcher. My current role requires a lot of paper reading to improve the existing models. I really like doing research and am satisfied with my current role. I have the following questions 
1. If I decide to pursue a phd, I will not be able to save money for next 3 to 4 years. Which is better 4 years of PhD or 4 years of research job experience?
2. My long term goal is to get a job in big companies like google and Facebook. Most of the computer vision roles in big companies require a phd with multiple publications. Can i join such companies without a phd?
3. My company encourages publishing papers. Letâ€™s say if I publish some papers in next three to four years, would that help me in competing with phd degree holders? Or I would still need an official degree?
4. How hard is to get admission in a good uni after some years of research experience with no publication record?

I would be thankful if someone could comment on my questions.",1635778867.0,2021-11-01 16:01:07
[R] Can ICRA reviewer see the names of author,0,qkzi4q,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkzi4q/r_can_icra_reviewer_see_the_names_of_author/,8,"I submitted a paper to ICRA, but I forgot to put the names of the ourselves on the paper. Question: can the reviewer still see the author names through the system?",1635840752.0,2021-11-02 09:12:32
[R] Any movie dataset with movie summaries?,3,qkvy6l,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkvy6l/r_any_movie_dataset_with_movie_summaries/,2,"Do you know of a dataset that contains movie summaries?

Do you know if researchers are legally allowed to download IMDB movie summaries for research purposes?",1635825819.0,2021-11-02 05:03:39
[D] why isn't converting ML Models to plain code trivial?,0,ql0a61,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/ql0a61/d_why_isnt_converting_ml_models_to_plain_code/,27,"I've only done across one project, (m2cgen), for converting ml models to plain code.
Given that even complex models can be broken down to a series of nested functions, why is this not more commonly done?

Yes, training is very complex, but inference is just passing the input through, it's nothing dynamic. Sure, the performance will suffer, but for non-streaming applications it should be fine. Even a complex classification network isn't going to take long to run inference.

The Frameworks already parse the graph, or pipeline, or whatever it is they use to matrix multiplications, so why not export a plain code version. (I know it's not the same, but it's probably much easier for the framework Devs to implement this rather than someone external)

It'll take a bit of doing, but having completely portable computer code, with no hosts, model serialisation etc... Seems like a good thing?

As you might imagine, I'm thinking of how to make portable models for integrating with local software (a game engine), with as little hassle as possible.",1635844540.0,2021-11-02 10:15:40
[D] How does tensorflow perform on M1 Pro/Max?,14,qkfgxj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkfgxj/d_how_does_tensorflow_perform_on_m1_promax/,13,"Basically the title. Apple claims that tensorflow is optimized/native for M1 chips, but how does it actually perform?",1635777741.0,2021-11-01 15:42:21
[Project] These plants do not exist - Using StyleGan2,1223,qjpcut,MachineLearning,https://v.redd.it/jxy5m9bvcsw71,26,,1635686392.0,2021-10-31 14:19:52
[D] How is MLOps done in your current workplace?,68,qk5avf,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk5avf/d_how_is_mlops_done_in_your_current_workplace/,26,"I joined a startup recently where the the necessary backend to support ML deployment is pretty much non-existent. All we have are some simple templates for CI/CD modified from those designed for generic microservices. Currently it takes data scientists at least 3-5 working days (post R&D) for to put a model into production as a prediction end point with logging and observability. This excludes setting up the necessary data pipelines between the models and other backend services. Whole process can take as long as 2 weeks. 

My team and I are looking into setting up some framework and automation to cut the turn around time for putting models into production. Trying to establish some reasonable goals for this project and hope to get some insight from others have been through the same.

&#x200B;

* Which part of the production processes are automated by your MLOps teams and tools?
* How much effort do these tools help save and how much time does it currently take to put up a piece of R&D work into production?",1635736845.0,2021-11-01 04:20:45
[D] What tools exist to determine the most useful type for perdiction?,1,qksrhl,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qksrhl/d_what_tools_exist_to_determine_the_most_useful/,5,"I've messed around with IBM's and Google's AutoML frameworks, I can't remember which output a report of which data was most helpful in predictions.

If I'm not using the correct terminology, I'm sorry. Basically, if I train an AI model on data such as Car Steering Angle, Gyro, Acceleramotor, speed, etc, and ground truth it to a more precise car steering angle, I want to figure out which of these data types were most useful for a good prediction.

That way I can feed in a whole lot of data, train the model, and know which data sources are irrelevant. What tools exist out there for this?",1635815370.0,2021-11-02 02:09:30
TARS: Task-Aware Representation of Sentences for Generic Text Classification (Paper Summary) [D],7,qkdfwe,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkdfwe/tars_taskaware_representation_of_sentences_for/,4,"State-of-the-art approaches for text classification leverage a transformer architecture with a linear layer on top that outputs a class distribution for a given prediction problem. 

While effective, this approach suffers from conceptual limitations that affect its utility in few-shot or zero-shot transfer learning scenarios ðŸ”¥

--------------------
This paper proposes a novel formulation of text classification that addresses these limitations.

https://youtu.be/XT6acdzVRHM

Paper: https://aclanthology.org/2020.coling-main.285/",1635771542.0,2021-11-01 13:59:02
[R] Music Source Separation in the Waveform Domain,55,qk1kp6,MachineLearning,https://v.redd.it/k29qmktcgvw71,2,,1635723558.0,2021-11-01 00:39:18
100Circles - Words to Paintings via NightCafe VQGAN+CLIP [Project],474,qjn0vg,MachineLearning,https://v.redd.it/rjdmkmbmjrw71,29,,1635677190.0,2021-10-31 11:46:30
[R] BERMo: What can BERT learn from ELMo?,3,qkewlk,MachineLearning,https://arxiv.org/abs/2110.15802,1,,1635776077.0,2021-11-01 15:14:37
[Project] BERT Tokenizers NuGet Package for C#,9,qk90b9,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk90b9/project_bert_tokenizers_nuget_package_for_c/,0,"&#x200B;

https://preview.redd.it/bzj0uq6uuxw71.png?width=1200&format=png&auto=webp&s=e9ff2d6fd627478aac69482be9e154cc4096cfe9

 

Inspired by the challenges I faced with using BERT models with ML.NET, I have built a small open-source project and NuGet Package for easy tokenization in C# ðŸš€

With this package, you don't have to worry about different vocabularies and you can build input for BERT models quicker.

ðŸ‘‰GitHub: [https://github.com/NMZivkovic/BertTokenizers](https://github.com/NMZivkovic/BertTokenizers)

ðŸ‘‰Blog Post: [https://rubikscode.net/2021/11/01/bert-tokenizers-for-ml-net/](https://rubikscode.net/2021/11/01/bert-tokenizers-for-ml-net/)",1635752632.0,2021-11-01 08:43:52
[D] Does cuda latest version support all version of pytorch and tensorflow,4,qka6p0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qka6p0/d_does_cuda_latest_version_support_all_version_of/,3,"Greetings. sorry i could not think a better place to ask this question where i can get response regarding pytorch, tensorflow and cuda version.

I want to to know if i install cuda 11.5, will it support lower version tensorflow or torch packages such as tensorflow 2.4 or pytorch 1.71 which is supported by 11.0 cuda version. Actually i want to install both tensorflow and pytorch. The best option is to install cuda 11.0 or 10.1, but i want to know can i install latest version of cuda and whether will it support both frame works?",1635758557.0,2021-11-01 10:22:37
[D] Is there a good guide/roadmap on Deeplearning with large Datasets in Clouds?,1,qkef2i,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkef2i/d_is_there_a_good_guideroadmap_on_deeplearning/,4,"Is there a good guide/roadmap on Deeplearning with large Datasets in Clouds?

I have around 50-200GB of data in .npy format to feed into a tensorflow pipeline  
Preprocessing itself takes a few hours too. (Or should i completely do that offline and change the pipeline structure?)",1635774637.0,2021-11-01 14:50:37
[D] NLP model for chatbot for inference on 11 GB GPU?,4,qkbfst,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkbfst/d_nlp_model_for_chatbot_for_inference_on_11_gb_gpu/,5,"Hello everybody

Iâ€™ve just found the amazing Huggingface library. It is an awesome piece of work.

I would like to train a chatbot on some existing dataset or several datasets (e.g. the Pile). For training (or fine-tuning) the model I have no GPU memory limitations (48 GB GPU is available). For inference, I only have a GPU with 11 GB available. Inference should be feasible in real-time (i.e. below around 3 seconds) and the model should be adjustable, i.e. the source code should be available to change the structure of the model.

What model is best when taking into account these requirements? Probably one of the best models is GPT-J but I think for inference it needs more than 11 GB GPU.

Are the models in the Huggingface library fully customizable (i.e., layers etc.)?",1635764232.0,2021-11-01 11:57:12
[D] Has anyone else received an e-mail for the ICLR review?,1,qkf6mt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkf6mt/d_has_anyone_else_received_an_email_for_the_iclr/,0,I got an e-mail from openreview with a single review of the paper. I went to the openreview website to see that it was deleted. Anyone else with a similar experience?,1635776907.0,2021-11-01 15:28:27
[D] How does AI fit into the metaverse future?,6,qk6h7n,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk6h7n/d_how_does_ai_fit_into_the_metaverse_future/,7,"**What are popular applications and research topics of AI relevant for VR/AR?**

As an ML engineer, I am interested in learning more about ML topics that are / could be useful for building VR software and hardware. This is meant to be an open-ended question so all kinds of opinions and perspectives will be appreciated. Thanks!",1635741241.0,2021-11-01 05:34:01
[D] Reusing parts of an open source code for a potential publication and a new open source code,1,qkert8,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkert8/d_reusing_parts_of_an_open_source_code_for_a/,2,"I am currently developing a new method that builds up upon an existing work in the literature in order to address the limitations and provide reasonable improvements to what has already been done. Earlier, I reached out to the authors for possible academic collaboration, but I have not received a reply from them. Their work has already been published as a conference paper two years ago, and their code is available on github, is regularly maintained and has also been deployed as pypi package that can be installed using \`pip\`.

My question is clearly about how to use certain parts of their work without plaigarising or breaking any copyright agreements? To what extent is it acceptable to rely on other people's work for producing a new method (especially when it is open source). Since the method I am working on is largely inspired from the existing method, it seems that I am currently on track to adopt around 30% of their code and follow their general code structure and OOP layout.",1635775674.0,2021-11-01 15:07:54
[P] Model Performance Monitoring in Production,0,qkh9jg,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qkh9jg/p_model_performance_monitoring_in_production/,0,"We've recently introduced model performance metrics in Graphsignal. Basically, by just logging a label and prediction the model-specific metrics are automatically computed, visualized and can be monitored. Graphsignal is currently SaaS, so a free account is necessary. No raw data is sent, only statistics.

More details in the blog post [Monitoring Model Performance in Production](https://graphsignal.com/blog/monitoring-model-performance-in-production/).

And the logger repo is [https://github.com/graphsignal/graphsignal](https://github.com/graphsignal/graphsignal).

I hope it can be useful for those who need to monitor models in production and do not want to build own pipelines for continuously computing accuracy and other metrics, implementing alerting, etc.",1635782837.0,2021-11-01 17:07:17
[D] Thoughts on pathways by Google Research,4,qk7tuv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk7tuv/d_thoughts_on_pathways_by_google_research/,10,"I recently found out about this proposal called ""Pathways"" by Jeff Dean ([https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)). But the article seemed very obscure, there were just ideas and not a single hint of how that would be solved, whenever a question was posed the word ""Pathways"" was thrown at it. Is it another huge transformer from google :). Just wanted to know what everyone here thinks about it.",1635747014.0,2021-11-01 07:10:14
[D] 2D models on 3D tasks (convolutions): simple replace?,0,qka32i,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qka32i/d_2d_models_on_3d_tasks_convolutions_simple/,2,"2D tasks enjoy a vast backing of successful models that can be reused. For convolutions, can one simply replace 2D ops by 3D counterparts and inherit their benefits? Any 'extra steps' to improve the transition? Not interested in unrolling the 3D input along channels.

Pubs/code help.",1635758044.0,2021-11-01 10:14:04
[D] What makes Multi Armed Bandit Problems contextual,2,qk9uet,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk9uet/d_what_makes_multi_armed_bandit_problems/,3,"Hi everyone,

I dive straight into my problem. What makes a multi armed bandit problem contextual? I read on [TensorFlow agent tutorial](https://www.tensorflow.org/agents/tutorials/per_arm_bandits_tutorial#multi-armed_bandits_with_arm_features) that the agent receives the context vector, which is just the observation, at every step, that makes a bandit setting contextual. Isnt every agent in an bandit setting doing this? Since in the MAB problem the agent needs to know on which machine/bandit he is and how much he knows of the probability of the machine. So how does contextual MAB defer from the standard MAB ? Is it for example ""extra"" information ontop? For example he knows wether a machine/bandit has a higher probability if the wether is raining or not?

And the second part of my question is, I'm currently working with Stable Baselines 3. Is here the normal observation function the correlating observation function (context vector) from tf and using the observation in every step making it contextual? Couldnt find any information in the SB3 documentation how the contextual settings work.

To be more specific, my ""extra"" context in my MAB problem is a state machine the bandit uses and each state is an one armed bandit.

I hope this isnt a beginner question and I am tolerated here.",1635756922.0,2021-11-01 09:55:22
[D] Machine Learning - WAYR (What Are You Reading) - Week 124,8,qjxfu9,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjxfu9/d_machine_learning_wayr_what_are_you_reading_week/,2,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|121-130|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)|[Week 121](https://reddit.com/pmzx3g)|||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)|[Week 122](https://reddit.com/pw14z5)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)|[Week 123](https://reddit.com/q5fi12)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)|[Week 116](https://reddit.com/odrudt)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)|[Week 117](https://reddit.com/omy345)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)|[Week 118](https://reddit.com/ovz52j)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)|[Week 119](https://reddit.com/p50knh)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)|[Week 120](https://reddit.com/pe2idh)||

Most upvoted papers two weeks ago:

/u/Icko_: [Patches Are All You Need?](https://papers.labml.ai/paper/dd638a442a9e11ec9e9dcba33be64600)

/u/CatalyzeX_code_bot: [Paper link](https://arxiv.org/abs/2012.09841)

Besides that, there are no rules, have fun.",1635710405.0,2021-10-31 21:00:05
[D] Using Movies to Improve Punctuation Prediction,13,qjuxax,MachineLearning,https://youtu.be/jxOpu4hXPJY,2,,1635703132.0,2021-10-31 18:58:52
[R] Physics Informed Neural Network suggestion and recommendation,0,qk79s9,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk79s9/r_physics_informed_neural_network_suggestion_and/,2,"Hi guys. I am learning about physics informed neural network, actual research focussing on Autoencoder. However, I just got in this new field and really want to get in-depth knowledge in this area. Would you recommend any related work or papers that I should read? Thanks a lot.",1635744547.0,2021-11-01 06:29:07
[P] StyleGAN3 + Cosplay Dataset. Happy Halloween! ðŸŽƒ,805,qj3uhj,MachineLearning,https://v.redd.it/imst817wvlw71,20,,1635607882.0,2021-10-30 17:31:22
[D] Projects to do with 100TB ASMR video dataset?,33,qjkoam,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjkoam/d_projects_to_do_with_100tb_asmr_video_dataset/,27,"I have been archiving ASMR videos for a number of years and have built up a 100TB collection of videos and thumbnails. About 11 billion individual frames. There is only so much ASMR I can listen to before I get bored, so I'm curious if there are any interesting ML projects that I can run against this data-set?

I think generating single frames or thumbnails using StyleGAN could be an option but not really that exciting. Audio and video generation seems considerably more immature. 

I was thinking of starting my training my own 4x upscaler (using the 4K videos as a dataset) to convert all 1080p videos to 4K. Keen to hear if anyone has fun ideas?",1635666250.0,2021-10-31 08:44:10
[R] current state of the art and novel research in support vector machines,2,qk2bj6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk2bj6/r_current_state_of_the_art_and_novel_research_in/,9,Title pretty much says it already. I m interested in the current state of art of svms/svrs and on which topics researchers currently work on in that area ( i guess optimization still being a big one). Would also appreciate any paper links posted here on not so much known svm extensions etc. Go nuts ! ;),1635726122.0,2021-11-01 01:22:02
[D] Anyone with powerful computers deploying locally?,1,qk144p,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qk144p/d_anyone_with_powerful_computers_deploying_locally/,4,"I have my own computer that has a pretty good cpu/gpu, I'd rather not spend more time to get a static ip to set up my computer as an official server through my ISP, or move my model and setup an expensive instance in the cloud. Is there an easier way to run an inference server on my machine that i am not aware of?",1635721988.0,2021-11-01 00:13:08
[D] State-of-the-art AI research,11,qjlmfe,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjlmfe/d_stateoftheart_ai_research/,10,"Hi everybody!

How/where do you check the state of the art of a topic? e.g. Whatâ€™s the state-of-the-art result for ""Transformer"" right now? Do you search for ""Transformer"" on Google scholar or Arxiv and you look for the last papers that came out?

Are there any tools available or some peer-review system that states where we are now with a specific topic?

Thanks in advance.",1635670613.0,2021-10-31 09:56:53
Training a model to recognize that a source/citation is needed in a text [R],3,qjtapm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjtapm/training_a_model_to_recognize_that_a/,3,"I'm looking into creating a tool that will analyze text and recognize where a source is needed for the claim or fact.

Some examples could be:
""Amazon was founded in 1997""
""Avarage conversion rates for eCommerce is 2%""
""Gold is up 20% in May 2021""

My question is if it would be possible to train a NLP model to understand this?",1635698317.0,2021-10-31 17:38:37
[R] ICCV2021 Oral -- Neural TMDlayer: Modeling Instantaneous flow of features via SDE Generators (with video explanation),1,qjx4k3,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjx4k3/r_iccv2021_oral_neural_tmdlayer_modeling/,0,"Our TMDlayer is inspired by stochastic differential equation (SDE) and aims to model the stochastic flow of features. In principle, it can be easily added on top of any DNN layer to bring the benefits. In addition, it immediately enables transductive inference once inserted into the model. 

Welcome to check out our video for a quick and easy understanding.

Video: [https://www.youtube.com/watch?v=vR3nrYJqcgQ&t=11s](https://www.youtube.com/watch?v=vR3nrYJqcgQ&t=11s)

Paper: [https://openaccess.thecvf.com/content/ICCV2021/papers/Meng\_Neural\_TMDlayer\_Modeling\_Instantaneous\_Flow\_of\_Features\_via\_SDE\_Generators\_ICCV\_2021\_paper.pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Meng_Neural_TMDlayer_Modeling_Instantaneous_Flow_of_Features_via_SDE_Generators_ICCV_2021_paper.pdf)

Code: [https://github.com/zihangm/neural-tmd-layer](https://github.com/zihangm/neural-tmd-layer)

&#x200B;

Our paper abstract: We study how stochastic differential equation (SDE) based ideas can inspire new modifications to existing algorithms for a set of problems in computer vision. Loosely speaking, our formulation is related to both explicit and implicit strategies for data augmentation and group equivariance, but is derived from new results in the SDE literature on estimating infinitesimal generators of a class of stochastic processes. If and when there is nominal agreement between the needs of an application/task and the inherent properties and behavior of the types of processes that we can efficiently handle, we obtain a very simple and efficient plug-in layer that can be incorporated within any existing network architecture, with minimal modification and only a few additional parameters. We show promising experiments on a number of vision tasks including few shot learning, point cloud transformers and deep variational segmentation obtaining efficiency or performance improvements.",1635709456.0,2021-10-31 20:44:16
[R] Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence,1,qjwdny,MachineLearning,https://arxiv.org/abs/2107.02173,2,,1635707275.0,2021-10-31 20:07:55
[D] What factors hinder people from studying causal inference in machine learning?,10,qjj70h,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjj70h/d_what_factors_hinder_people_from_studying_causal/,12,"Causal Inference has been widely studied and applied in many fields, but its impact seems to be relatively low in the machine learning community at the moment. I've listened to some talks, but people are only emphasizing the importance of causality (which seems to be axiomatic), with little discussion of the main technical challenges. Are there some factors that prevent researchers in machine learning from embracing causal inference? Or in other words, are some assumptions of causal inference difficult to hold in machine learning research problems?

Any kind of discussion is welcome. Thank you very much.",1635659459.0,2021-10-31 06:50:59
[R] A Closer Look at How Fine-tuning Changes BERT,5,qjim1u,MachineLearning,https://arxiv.org/abs/2106.14282,1,,1635656913.0,2021-10-31 06:08:33
[D] Sagemaker. Pros and Cons?,11,qjeslj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjeslj/d_sagemaker_pros_and_cons/,9," Hello everybody! I have a question: What are the pros and cons of using Sagemaker for a machine learning pipeline? I see many company conferences using custom solutions and architectures involving docker and other frameworks within the cloud, but never using Sagemaker. I have this doubt since it presents itself as a complete solution for end-to-end projects",1635642145.0,2021-10-31 02:02:25
[N] Jax now Supports Apple Silicon [CPU ONLY],22,qjarv1,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjarv1/n_jax_now_supports_apple_silicon_cpu_only/,6,"In case you guys didn't know this, I'm able to use JAX on my M1 MacBook Air.

Currently only CPU is supported, and I think they don't have any plans to support GPU yet.

Check this thread to install jaxlib: [https://github.com/google/jax/issues/5501](https://github.com/google/jax/issues/5501)

Issue for GPU support: [https://github.com/google/jax/issues/8074](https://github.com/google/jax/issues/8074)",1635628495.0,2021-10-30 23:14:55
[D] How to detect multiple number with decimal points?,0,qjopnm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjopnm/d_how_to_detect_multiple_number_with_decimal/,6,"I have images which have time displayed in the bottom corner with decimal points. I want to store those time stamp along with the image ID in a spreadsheet. I tried using PyTesseract but it doesn't work very well. What off the shelf alternatives could I use?

I know it's a very simple problem but I would really appreciate any help!",1635684146.0,2021-10-31 13:42:26
[P] StyleGAN3 - This shoe does not exist,126,qiwl21,MachineLearning,https://www.thisshoedoesnotexist.com,20,,1635579804.0,2021-10-30 09:43:24
[R] Measuring Disagreement in Science,2,qjhe3z,MachineLearning,https://arxiv.org/abs/2107.14641,1,,1635651922.0,2021-10-31 04:45:22
[D] Getting Started with Deep Learning in JAX with Treex in 16 lines,19,qj1d49,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qj1d49/d_getting_started_with_deep_learning_in_jax_with/,13,"If you are JAX-curious but don't want to stray too far from the Pytorch-way,

[Treex](https://github.com/cgarciae/treex) is here to save the day ðŸŒ³

&#x200B;

https://preview.redd.it/6cjnb94sflw71.png?width=1230&format=png&auto=webp&s=0c4a56734b0b8ced065014ee9bc425578dd2718d

Treex is a Pytree-based Module system for JAX that is simple and intuitive. Just like in Pytorch, Treex Modules hold their parameters and respect Object Oriented semantics. Thanks to their Pytree component Treex is fully compatible with all JAX functions \`jit\`, \`vmap\`, \`grad\`, etc, and 3rd party code that works with Pytrees.

**Note**: For a real use-case, you want to extract the loop's body into a jit-ed function for performance as shown next, however the previous code is more pedagogical.

&#x200B;

https://preview.redd.it/pzmux8rvflw71.png?width=1208&format=png&auto=webp&s=454e84fa003e557678f7dcb8f5cacc8bcf0f4f24",1635600045.0,2021-10-30 15:20:45
Composability in Julia: Implementing Deep Equilibrium Models via Neural ODEs [P],14,qizzhr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qizzhr/composability_in_julia_implementing_deep/,0,"https://julialang.org/blog/2021/10/DEQ/

This is just a fun blog post showing how to link two different ""implicit layer"" methods, Neural ODEs and Deep Equilibrium Models (DEQ), specifically by implementing a DEQ via Neural ODEs with events. It also showcases Julia and the SciML libraries as a nice research platform for these kinds of ideas as pieces like the optimized adjoints, GPU compatibility, etc. all come for free just by using the standard nonlinear and ODE solver libraries. We'll be doing a more serious and in-depth discussion of related methods in some upcoming papers, but for now enjoy a post that's more about building bridges and opening questions than anything else.",1635594987.0,2021-10-30 13:56:27
[News][Research] ADOP: Approximate Differentiable One-Pixel Point Rendering (Synthesize Smooth Videos from a Couple of Images),10,qj0yhj,MachineLearning,https://youtu.be/Jfph7Vld_Nw,2,,1635598673.0,2021-10-30 14:57:53
"[D] Interview w/ Siraj Raval - Stories about YouTube, Plagiarism, and the Dangers of Fame (by Yannic Kilcher)",0,qjptfe,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qjptfe/d_interview_w_siraj_raval_stories_about_youtube/,70,"I had a super interesting conversation with Siraj Raval about YouTube, being popular, plagiarism, chasing clout, and the perils of fame. I think there is definitely something in here for everyone. Have a listen:

[https://youtu.be/kEhEbVZQwjM](https://youtu.be/kEhEbVZQwjM)

OUTLINE:

0:00 - Intro

1:30 - Welcome

3:15 - Starting out: From Economics to YouTube

13:00 - More Views: Plagiarizing Video Content

23:30 - One Step Up: Copying A Research Paper

29:15 - Was there another way?

39:00 - Clickbait Course: Make Money with Machine Learning

50:30 - Rock Bottom and the Way Forward

1:01:30 - Advice for Future Generations

&#x200B;

Siraj's Channel: [https://www.youtube.com/c/SirajRaval](https://www.youtube.com/c/SirajRaval)",1635687949.0,2021-10-31 14:45:49
[D] What is a reasonable way to address a paper that was published and you consider to be dishonest or plain bogus?,220,qiea6g,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qiea6g/d_what_is_a_reasonable_way_to_address_a_paper/,61,"I have reasons to believe that some published work did not do an honest comparison with related work, and the analysis lacks merit, misrepresents the models it was tested against, and does not explain why the proposed framework works.",1635518300.0,2021-10-29 16:38:20
[D] Recommendation for pattern theory/similar literature?Iâ€™m interested in theoretical backing for texture recognition (image required),0,qj6rv4,MachineLearning,https://i.redd.it/frgzfby4mmw71.jpg,2,,1635616516.0,2021-10-30 19:55:16
[D] What do you think about my idea to boost skip connections in ResNets without performance loss?,8,qivb0y,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qivb0y/d_what_do_you_think_about_my_idea_to_boost_skip/,9,"Consider a small block that has skip connections which is part of some bigger NN.

Symbols: input x, output y, weight w âŠ‚ D(some distribution). (A very simplified situation for demonstration).

Block: y=x+wx

Suppose some loss L and a given matrix(single number in this case) of gradients dL/dy calculated via backpropagation.

dy/dw = x, dL/dw=dL/dy \* dy/dw = dL/dy \* x

The weight w will be modified as: w -= lr \* (dL/dy \* x).

We also compute **dL/dx=dL/dy \* w** for backprapagation to more early layers.

dy/dx=w, dL/dx=dL/dy \* dy/dx = dL/dy \* w

\------------------------

We can omit the skip connection by initializing the weights as a number v that is 1 bigger than w.

init: v=w+1,  w âŠ‚ D

Block: y=vx

In this case, dL/dv is computed consistently as the case with the original weight w.

dy/dv = x, dL/dv=dL/dy \* dy/dv = dL/dy \* x

However, the problem is that dL/dx is different because dy/dx=v=w+1 not w:

dy/dx=v=w+1,**dL/dx=dL/dy \* dy/dx = dL/dy \* (w+1)**

This will affect the gradient values of other layers. This is intuitively expandable to more complex operations e.g. matmul and convolution.

\-----------------------

Because of this inconsistency, removing residual connections with a modified initialization is known to yield different(worse) performance from the original ResNet architecture.

What if we initialize the weights with w+1 but define a skip connection function or layer with custom Autograd backward methods like the tutorial below to compute dy/dx as (w-1)? Would this yield exactly the same inference results and gradients compared to a typical skip connection in ResNet? What do you think?

The problem with such skip connections are that they increase the peak memory usage by x2 because we have to cache all the features of x while computing f(x), and they do impact inference time.

[https://pytorch.org/tutorials/beginner/examples\_autograd/two\_layer\_net\_custom\_function.html](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html)

This is just some stuff in mind that came up while reading the paper [*RepVGG: Making VGG-style ConvNets Great Again*](https://arxiv.org/pdf/2101.03697.pdf). Haven't done much research on it yet, but the DiractNet paper seems relevant, while it seems to degrade the performance without a solution to the gradient problem, it suggests that initializing W'=W+I(I is identity matrix) should yield the same results in linear algebra operations.

[Ïƒ\(x\) is a function combining nonlinearity and batch normalization](https://preview.redd.it/c9w8aosghjw71.png?width=1080&format=png&auto=webp&s=ff079e69474523f9018237a5eb36e94cdbae3d36)

Edit: the expansion to linear algebra might not be as simple :)",1635573972.0,2021-10-30 08:06:12
[D] How to truly understand attention mechanism in transformers?,95,qidpqx,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qidpqx/d_how_to_truly_understand_attention_mechanism_in/,33,"Attention seems to be a core concept for language modeling these days. However it is not that easy to fully understand, and in my opinion, somewhat **unintuitive**. While I know what attention does (multiplying Q and K, scaling + softmax, multiply with V), I lack an intuitive understanding of what is happening. What were some explanations or resources that made attention click for you?",1635516674.0,2021-10-29 16:11:14
[P] Large-scale language modeling tutorials with PyTorch,38,qiguvq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qiguvq/p_largescale_language_modeling_tutorials_with/,1,"&#x200B;

https://preview.redd.it/k0ggm13b3fw71.png?width=2592&format=png&auto=webp&s=c2611413d7598dd3743fea105ede0e069ef6cfee

Hello. I'm Kevin Ko, a machine learning engineer at TUNiB, Korean AI startup. Since last year, GPT3 has taken the lead, and large-scale language models have become mainstream of NLP research. However, from the point of view of a model scientist who has never learned distributed programming, these techniques may still feel very unfamiliar. I was one of them, and I'm still going through a lot of trial and error. ðŸ˜‚

So, I made a tutorial notebooks by organizing various techniques related to large-scale language modeling. I hope this material will be of some help to people like me who are interested in distributed programming and large-scale language models, but are afraid to start. I made all the materials in Korean, my native language, but if you use Google Translate, you will be able to study well. And when I have free time, I will translate all materials into English. Thank you !

[https://github.com/tunib-ai/large-scale-lm-tutorials](https://github.com/tunib-ai/large-scale-lm-tutorials)",1635525481.0,2021-10-29 18:38:01
[P] â€œAbstractified Multi-instance Learning (AMIL) for Biomedical Relation Extractionâ€,1,qiy1bn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qiy1bn/p_abstractified_multiinstance_learning_amil_for/,0,"https://arxiv.org/pdf/2110.12501v1.pdf

https://www.youtube.com/watch?v=vNBAaE_uaUg

https://akbc.apps.allenai.org/static/slides/13.pdf

https://www.akbc.ws/2021/assets/pdfs/VX0swzJEzpg.pdf

The idea is that a sentence with two entities is likely to express a relationship. The novelty of this paper is that they bunch sentences together by entity type.",1635586577.0,2021-10-30 11:36:17
[P] Complete End-to-End Machine Learning Portal (POC),0,qiytlg,MachineLearning,https://github.com/shreyas-jk/ML-Portal-FastAPI,1,,1635590041.0,2021-10-30 12:34:01
[R] Do Vision Transformers See like Convolutional Neural Networks?,9,qifv7i,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qifv7i/r_do_vision_transformers_see_like_convolutional/,0,"Do Vision Transformers work in the same way as CNNs? Do the internal representational structures of ViTs and CNNs differ? 

Podcast: [https://youtu.be/kJJoB19ayjg](https://youtu.be/kJJoB19ayjg)",1635522694.0,2021-10-29 17:51:34
[D] Google: Ondevice grammar error correction on gboard,4,qimdee,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qimdee/d_google_ondevice_grammar_error_correction_on/,0,"https://ai.googleblog.com/2021/10/grammar-correction-as-you-type-on-pixel.html

We are launching a grammar correction feature that is directly built into Gboard on Pixel 6 that works entirely on-device to preserve privacy, detecting and suggesting corrections for grammatical errors while the user is typing. Building such functionality required addressing a few key obstacles: memory size limitations, latency requirements, and handling partial sentences. Currently, the feature is capable of correcting English sentences (we plan to expand to more languages in the near future) and available on almost any app with Gboard.

Gboard suggests how to correct an ungrammatical sentence as the user types.",1635541499.0,2021-10-29 23:04:59
[D] How machine learning is changing immunology,181,qhy2gd,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhy2gd/d_how_machine_learning_is_changing_immunology/,21,"Hi, ML community.

This article shows some examples of how immunology researchers are using machine learning to improve our understanding of the immune system:
https://www.immunai.com/press/immunology-and-machine-learning-online-symposium

This article can spark some interest in these kinds of applications, because this is just the tip of the iceberg in terms of what is actually possible.",1635459560.0,2021-10-29 00:19:20
[D] Advice on Short (5 minute) Talks for Virtual Conferences,0,qiqcpp,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qiqcpp/d_advice_on_short_5_minute_talks_for_virtual/,1,"I had a first-author paper accepted for the virtual poster session at neurips. As part of that, I need to give a 5 minute talk (due November 1st; yes, I am a procrastinator).

This is my first time giving a talk at a non-workshop conference venue. The instructions provided from neurips are very sparse, with no advice on how the talk should be structured. Can anyone here give advice on what they've seen that's worked well in \~5 minute lightning technical talks?",1635554689.0,2021-10-30 02:44:49
"[D] Google Research: Introducing Pathways, a next-generation AI architecture",81,qi0act,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qi0act/d_google_research_introducing_pathways_a/,23,"**Blog Post URL**

[https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)

**Summary**

GShard and Switch Transformer are two of the largest machine learning models weâ€™ve ever created, but because both use sparse activation, they [consume less than 1/10th the energy](https://blog.google/technology/ai/minimizing-carbon-footprint/) that youâ€™d expect of similarly sized dense models â€” while being as accurate as dense models.

So to recap: todayâ€™s machine learning models tend to overspecialize at individual tasks when they could excel at many. They rely on one form of input when they could synthesize several. And too often they resort to brute force when deftness and specialization of expertise would do.

Thatâ€™s why weâ€™re building Pathways. Pathways will enable a single AI system to generalize across thousands or millions of tasks, to understand different types of data, and to do so with remarkable efficiency â€“ advancing us from the era of single-purpose models that merely recognize patterns to one in which more general-purpose intelligent systems reflect a deeper understanding of our world and can adapt to new needs.

**Intro**

Too often, machine learning systems overspecialize at individual tasks, when they could excel at many. Thatâ€™s why weâ€™re building Pathwaysâ€”a new AI architecture that will handle many tasks at once, learn new tasks quickly and reflect a better understanding of the world.

When I reflect on the past two decades of computer science research, few things inspire me more than the remarkable progress weâ€™ve seen in the field of artificial intelligence.

In 2001, some colleagues sitting just a few feet away from me at Google realized they could use an obscure technique called machine learning to help correct misspelled Search queries. (I remember I was amazed to see it work on everything from â€œayambic pitnamiterâ€ to â€œunnblevaiabelâ€). Today, AI augments many of the things that we do, whether thatâ€™s helping you [capture a nice selfie](https://ai.googleblog.com/2021/06/take-all-your-pictures-to-cleaners-with.html), or providing [more useful search results](https://blog.google/products/search/how-ai-making-information-more-useful/), or warning hundreds of millions of people [when and where flooding will occur](https://ai.googleblog.com/2020/09/the-technology-behind-our-recent.html). Twenty years of advances in research have helped elevate AI from a promising idea to an indispensable aid in billions of peopleâ€™s daily lives. And for all that progress, Iâ€™m still excited about its as-yet-untapped potential â€“ AI is poised to help humanity confront some of the toughest challenges weâ€™ve ever faced, from persistent problems like illness and inequality to emerging threats like climate change.

But matching the depth and complexity of those urgent challenges will require new, more capable AI systems â€“ systems that can combine AIâ€™s proven approaches with nascent research directions to be able to solve problems we are unable to solve today. To that end, teams across Google Research are working on elements of a next-generation AI architecture we think will help realize such systems.",1635466654.0,2021-10-29 02:17:34
[D] Can you apply mix-up to regression problems with multiple features?,0,qisr9d,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qisr9d/d_can_you_apply_mixup_to_regression_problems_with/,3,"So the equation for mix-up is usually

yk = L\*yi + (1-L)\*yj and xk = L\*xi+(1-L)\*xj

This pertains for feature vectors. Has there been much success applying this to a regression problem with multiple features? So a feature matrix in this case? It's hard for me to imagine since some regression problems can have weird features such as categorical so if you multiply by a lambda and add to another categorical with a lambda doesn't feel like it'd help, because then you're treating a categorical as a range instead. Has anyone had any experience with this? Does it work on non neural networks?",1635563542.0,2021-10-30 05:12:22
[D] EU AI Act - Overview and Roadmap for AI Manufacturers,17,qi6qtk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qi6qtk/d_eu_ai_act_overview_and_roadmap_for_ai/,5,"Couldnâ€™t make it last time? Were back with a new and improved free webinar!

The EU Commission is about to pass legislation that is going to profoundly change the way AI will be developed.

In the free webinar linked below, we will summarize the key requirements  of the AI Act and will present a roadmap to get your business ready for the upcoming challenges in time.

Register for this LinkedIn event to stay updated - the webinar link will be shared on time.

See you there!

[https://www.linkedin.com/events/euaiact-overviewandroadmapforai6855800886480957440/ ](https://www.linkedin.com/events/euaiact-overviewandroadmapforai6855800886480957440/)",1635490077.0,2021-10-29 08:47:57
[D] How is the landscape for new PhDs in Academia?,3,qigku2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qigku2/d_how_is_the_landscape_for_new_phds_in_academia/,5,"I'm a Canadian applying to graduate school in the field but I'd like to get some perspective into what I am getting myself into.

For those who have completed PhDs related to ML or statistical learning, how has your and your colleagues experiences in landing positions in academia? I read that 3% of PhD graduate end up as professors however there also is counts of an exodus from academia to industry. Does that make it easier for people to land professorship positions? What proportion (roughly speaking) of PhDs in the field that want to enter academia actually do? Are postdocs common or unnecessary? I assume these answers change with location. I'm interested in hearing perspectives from around the globe. I would also be quite interested in hearing people who work in academia in the Arabian Gulf. It seems like the GCC countries have been investing heavily into research institutes for AI and the profs there seem to get flush with funding.",1635524692.0,2021-10-29 18:24:52
[D] 3D Medical Imaging annotation,1,qilcjy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qilcjy/d_3d_medical_imaging_annotation/,1,"I  work in medical imaging space and we need ground truth 3D segmentation of anatomical structures in 3D CT imaging. It is very time consuming process and requires domain expertise. Does anyone know of  annotation services in this space? There seems to be quite a number of services out there offering image annotation services for machine learning, but most of them are focusing on 2D images. The task in 3D medical imaging is specialized and more demanding.",1635538369.0,2021-10-29 22:12:49
[D] Good landmark points annotation tools,0,qil09k,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qil09k/d_good_landmark_points_annotation_tools/,1,"Hello Everyone!
 I want to generate a image dataset with some landmark points on it for the labels. For that I am looking for some landmark annotation tools. Any suggestions would be really helpful. 
Thank you!",1635537386.0,2021-10-29 21:56:26
OneFlow: Redesign the Distributed Deep Learning Framework from Scratch[P],5,qi9hyr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qi9hyr/oneflow_redesign_the_distributed_deep_learning/,1,"Deep learning frameworks such as TensorFlow and PyTorch provide a productive interface for expressing and training a DNN model on a single device or using data parallelism. Still, they may not be flexible or efficient enough in training emerging large models on distributed devices, which require more sophisticated parallelism beyond data parallelism. Plugins or wrappers have been developed to strengthen these frameworks for model or pipeline parallelism, but they complicate the usage and implementation of distributed deep learning. **Paper:**[https://arxiv.org/pdf/2110.15032.pdf](https://arxiv.org/pdf/2110.15032.pdf); **Code:** [https://github.com/Oneflow-Inc/oneflow](https://github.com/Oneflow-Inc/oneflow) 

Aiming at a simple, neat redesign of distributed deep learning frameworks for various parallelism paradigms, we present OneFlow, a novel distributed training framework based on an SBP (split, broadcast and partial-value) abstraction and the actor model. SBP enables much easier programming of data parallelism and model parallelism than existing frameworks, and the actor model provides a succinct runtime mechanism to manage the complex dependencies imposed by resource constraints, data movement and computation in distributed deep learning.

We demonstrate the general applicability and efficiency of OneFlow for training various large DNN models with case studies and extensive experiments. The results show that OneFlow outperforms many well-known customized libraries built on top of the state-of-the-art frameworks.",1635502227.0,2021-10-29 12:10:27
[D] linking pythons multiprocessing with tensorflows mirrored strategy,0,qimg4k,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qimg4k/d_linking_pythons_multiprocessing_with/,0,"When performing inference on video data and you have access to a gpu cluster for inference does it make since to employ pythons multiprocessing library to read the video file faster while using tensorflows mirrored strategy to distribute each image across the gpu cluster to increase prediction speed? 

Or on the other hand, would it make more sense to instantiate multiple threads the grab different frames from the video file and then have the same model loaded onto different gpus and perfrom prediction this way?",1635541723.0,2021-10-29 23:08:43
[R] From Machine Learning to Robotics: Challenges and Opportunities for Embodied Intelligence,7,qi5z4d,MachineLearning,https://arxiv.org/abs/2110.15245,2,,1635486897.0,2021-10-29 07:54:57
"[R] I have been working on a learning/organizing rule of biological neurons for the past 2 years, and I am wondering whatever something similar was already discovered and/or is it worth trying to get published",162,qhljmo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhljmo/r_i_have_been_working_on_a_learningorganizing/,52,"It's still early to publish the mathematical parts because they are changing a lot, but there are some results for the time being:

1. The learning rule produces/organizes most-if-not-all primary visual cortex's non-complex neurons:

https://preview.redd.it/4j39li51m6w71.png?width=661&format=png&auto=webp&s=408f1b8a65a7327072ec8b98761eaad496032cfe

just by interconnecting several thousands of neurons, and flashing natural images to the neurons through ""pixels""/input neurons.

2. The neurons don't distinguish between interconnections and feedforward connections. (e.g. the interconnections are also recurrent connections like biological neurons)

2.1. The learning rule only depends on the neuron's outputs and its inputs, there is no superprocess like backpropagation that leaks the synapses' information; also no cost function or labels anywhere.

3. Likely has machine learning power. A year ago I filmed a cat for 19 seconds with manual camera rotations/zoom-in/zoom-out and I stacked 3 CNN layers with 50 filters/neurons each, showed the network the video for organization stage, and then stacked a single-neuron layer, and made it learn to detect the cat by giving just one point on the cat in a specific frame in the video, and it looks like the neuron somewhat survived the rotation/scale transformations. [The video and the activation map of the cat neuron can be downloaded here.](https://easyupload.io/8l50mu)

4. Converges pretty quickly (about 800-1000 organization steps)

5. The mathematical/theoretical's bottom line is that every behavior of an intelligent object should be backed by as much information/sources as possible. e.g. it's useful for a neuron to depend on many inputs as possible, so when many of its inputs fail, it will continue to function as usual; also this neuron represents real information, because itâ€™s unlikely that many different sources/inputs will tell the same information by chance.

Note that this is trying to be mimic the neurons' learning rule which is dissimilar from what spiking neural networks' trying to mimic in the brain, so the neurons here use RELU, rather than integrate-and-fire etc.",1635422835.0,2021-10-28 14:07:15
[R] Create graph from random walks in multidimensional space,1,qifnck,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qifnck/r_create_graph_from_random_walks_in/,0,"Imagine we have a camera-equipped robot that performs `k` random walks in a building, not necessarily starting every time from the same location. We collect a set of image sequences `O_1, O_2, ... O_k`, each one containing the images captured by the robot in that random walk.

Our goal is to transform the dataset of `k` image sequences into a graph, where images are connected by edges if, given the data, it would be possible for the robot to go from one to the other in less than `d` steps. Moreover, edges need to be *directed* since it might be possible to go from image A to image B, but not vice versa (e.g. image A is the image of a glass falling from the table, image B is the glass broken on the floor. Obviously, A->B is an edge but B->A is not).

The requirements are mainly that a) the task is performed by a NN model since it will need to generalize to new image data after training and (possibly) b) the complexity should be at most O(n^(2))

Could you point me to some relevant literature? I am having trouble finding works that operate in a similar settings.",1635522084.0,2021-10-29 17:41:24
Question Answering on Tabular Data- A survey [Project],2,qi8jkn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qi8jkn/question_answering_on_tabular_data_a_survey/,0,"I've written a blog about the few popular approaches to solve the task of question answering on tabular data: and also included one of my side project which led to a solution on the same!

Check it out here.

[https://blog.paperspace.com/tapas-question-answering/](https://blog.paperspace.com/tapas-question-answering/)

Also, this is the side project .

[https://github.com/abhijithneilabraham/tableQA](https://github.com/abhijithneilabraham/tableQA)

Looking for contributors in the same space, so feel free to drop a DM",1635498028.0,2021-10-29 11:00:28
[D] Why only accuracy as evaluation criteria in Few-shot and zero-shot learning?,0,qi99ka,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qi99ka/d_why_only_accuracy_as_evaluation_criteria_in/,0,"I have seen that majority of few and zero-shot learning papers use Accuracy as a benchmark. Unlike other domains like object detection, classical image recognition, why they don't use other benchmarks like Precision, recall or other criteria apart from Accuracy? Is it due to others are doing it, so people keep following, or other performance metrics are not reliable in a few-shot domain ?",1635501273.0,2021-10-29 11:54:33
[D] Uncomfortably framed Keras tutorial on heart disease prediction,7,qhyhxs,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhyhxs/d_uncomfortably_framed_keras_tutorial_on_heart/,4,"https://keras.io/examples/structured_data/structured_data_classification_from_scratch/

This example in the Keras documentation makes me uncomfortable with how little it mentions generalization and the importance of evaluation.

Many developers take their first steps in ML via Keras (particulary as it's featured in so many GCP courses) so it's important to contextualize the predictions and manage expectations of what one should reasonably expect when minimizing empirical risk and hoping for generalization.

""We use the features to predict whether a patient has a heart disease, [...] The last column, ""target"", indicates whether the patient has a heart disease (1) or not (0).""

No notion of imbalanced data or what it means for a patient to ""have a heart disease"". Are there different kinds? How about levels of severity? Perhaps not super important for an introductory tutorial, but this last statement makes me uncomfortable:

""This particular patient had a 18.8 percent probability of having a heart disease, as evaluated by our model.""

I think this should be wrapped with cautions, else it might lead new learners of Keras into having an overconfidence in what to responsibly expect from a model.

What do you think? Am I overly sensitive here? Something about this being about predicting people's heart disease, where wrong predictions are really bad, makes it especially important, I feel.",1635460938.0,2021-10-29 00:42:18
[D] How do you deal with covariate shift and concept drift in production?,27,qhnao8,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhnao8/d_how_do_you_deal_with_covariate_shift_and/,36,"To oversimplify, in reality you frequently have covariate shift (or just sample selecion bias) between your test and train set, essentially `P(X)` being changed across time. Aside from this there's the case where there is no covariate shift but `p(y|x)` is altered nonetheless, concept drift. From what I've read the former can be monitored in the upstream data but the latter frequently requires some feedback mechanism that involves ground-truth labelling.

I have a few days off from work per year to dedicate to research and I'm looking to tackle these two problems by implementing something that is 'simple enough'. I've started off by reading a number of survey papers and then digging a bit deeper with some more specific papers.

So far I've been thinking about measuring the distribution of the data upstream, either with process control charts or dividing the data into temporal windows and using KL-divergence to measure covariate shift. I'm especially hopeful of the process control charts as hey may be simple enough for non-data stakeholders to understand. After this I'll consider a retraining / reweighting strategies like ensembles, transfer learning etc.

For the case of concept drift I'll just be looking to continuously provide feedback in terms of ground truth labels and keep track of the performance over time. Potentially plotting the daily/weekly AUC in a process control chart too. After this I'll consider (the same) retraining / reweighting strategies.

Has anyone done anything similar at work? Do you have any recommended readings? If you haven't - do you have any comments on the approach so far?

Thanks in advance.",1635428403.0,2021-10-28 15:40:03
[R] Learning Graph Cellular Automata,5,qhrgxr,MachineLearning,https://arxiv.org/abs/2110.14237,2,,1635440440.0,2021-10-28 19:00:40
[D] How do you know when your model is good enough to deploy to production?,7,qhpdoo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhpdoo/d_how_do_you_know_when_your_model_is_good_enough/,11,"After you have trained a model, how do you know that it is good enough to deploy without it malfunctioning?",1635434398.0,2021-10-28 17:19:58
[N] Amazon launches AWS instances powered by Habanaâ€™s AI accelerator chip,100,qh95on,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh95on/n_amazon_launches_aws_instances_powered_by/,20,"https://venturebeat.com/2021/10/26/amazon-launches-aws-instances-powered-by-habanas-ai-accelerator-chip/

> Amazon Web Services (AWS), Amazonâ€™s cloud services division, today announced the general availability of Elastic Compute Cloud (EC2) DL1 instances. While new instance types generally arenâ€™t particularly novel, DL1 (specifically DL1.24xlarge) is the first type in AWS designed for training machine learning models, Amazon says â€” powered by Gaudi accelerators from Intel-owned Habana Labs.
> 
>",1635376254.0,2021-10-28 01:10:54
[D] State of the art in the document information extraction/parsing for resume parsing?,1,qhv879,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhv879/d_state_of_the_art_in_the_document_information/,1,"Hi everyone,


I've been looking for state of the art research paper/project/code for automatically extracting information from various layout of resumes.


Typical workflow I can estimate is to convert resume to image, detect text, table etc., apply rule based heuristic approach to extract the information based on NER etc. but I think that would be an outdated approach and will not be accurate and feasible enough to cover all the cases.


Need to extract information like Name, Contact details, skills, projects, company, job tenure and other resume related data.


I'd really appreciate if you have could share any information/experience in thisÂ regard.


Thanks",1635451138.0,2021-10-28 21:58:58
[D] Package for ordinal regression with automatic cross-validation?,0,qhz9h5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhz9h5/d_package_for_ordinal_regression_with_automatic/,2,"Ideally in python or R: I'd like to fit my data to a logit ordinal regression model and perform LOOCV for a l2 penalty. sklearn doesn't seem to support this, nor does the statsmodels module. rms in R has an option for an l2 penalty, but no cross validation. I suppose with a script of my own I can do what I want, but I want to avoid re-inventing the wheel if possible.",1635463375.0,2021-10-29 01:22:55
[D][P] Would you consider using this paradigm for writing features?,0,qhy4dq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhy4dq/dp_would_you_consider_using_this_paradigm_for/,6,"Hi I'm one of the authors that created [https://github.com/stitchfix/hamilton](https://github.com/stitchfix/hamilton) \- see [https://multithreaded.stitchfix.com/blog/2021/10/14/functions-dags-hamilton/](https://multithreaded.stitchfix.com/blog/2021/10/14/functions-dags-hamilton/) for context on how it came to be.

The core of the idea is that people write functions that look like this:

    def column_c(column_a: pd.Series, column_b: pd.Series) -> pd.Series:
        """"""Some doc string""""""
        return column_a + column_b

instead of:

    df['column_c'] = df['column_a'] + df['column_b']

Yes, I agree, if you're doing small things, then this is probably overkill. However if you are on a team, or have lots (e.g. 100s) of transforms defined over functions then that's where Hamilton really shines; it enables very uniform code (documentation, unit testable), no glue code, low maintenance.

But anyway, back to the question, would this be a paradigm you could see yourself coding in?

[View Poll](https://www.reddit.com/poll/qhy4dq)",1635459727.0,2021-10-29 00:22:07
[D] What even are frequencies in images?,2,qhq1zi,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhq1zi/d_what_even_are_frequencies_in_images/,9,"When dealing with newer Computer Vision methods, I struggle to understand some signal processing related concepts, first and foremost frequencies in images. I understand frequencies that occur in e.g. sound, but where does a time axis come from in images?

Perhaps this lack of understanding is even deeper, e.g. what is a â€žsignalâ€œ (or signal-to-noise ratio) when dealing with images?",1635436425.0,2021-10-28 17:53:45
"[D] Peter Henderson on RL Benchmarking, Climate Impacts of AI, and AI for Law",0,qhupqr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhupqr/d_peter_henderson_on_rl_benchmarking_climate/,1,"Hey, you might dig our new [interview](https://thegradientpub.substack.com/p/peter-henderson-on-rl-benchmarking) with Stanford JD-PhD Peter Henderson, whose research is about creating robust decision-making systems to create new ML methods for applications that are beneficial to society:

[https://thegradientpub.substack.com/p/peter-henderson-on-rl-benchmarking](https://thegradientpub.substack.com/p/peter-henderson-on-rl-benchmarking)

https://preview.redd.it/9ov3sehrt8w71.png?width=1200&format=png&auto=webp&s=24f4510b9ba510dd1ee714867e5b6fb516254359

 We discuss:

* [Reproducibility and Reusability in Deep Reinforcement Learning](https://www.peterhenderson.co/publication/henderson-2018/).Â 
* [Benchmark Environments for Multitask Learning in Continuous Domains](https://arxiv.org/pdf/1708.04352.pdf)
* [Reproducibility of Bench-marked Deep Reinforcement Learning Tasks for Continuous Control.](https://arxiv.org/abs/1708.04133)
* [Deep Reinforcement Learning that Matters](https://www.peterhenderson.co/publication/henderson-2018-deep/)
* [Reproducibility and Replicability in Deep Reinforcement Learning (and Other Deep Learning Methods)](https://www.peterhenderson.co/talk/ssc2018/)
* [Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning](https://arxiv.org/abs/2002.05651)
* [How  blockers can turn into a paper: A retrospective on 'Towards The  Systematic Reporting of the Energy and Carbon Footprints of Machine  Learning](https://www.peterhenderson.co/talk/mlretro2020/)
* [When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset](https://arxiv.org/pdf/2104.08671)â€
* [How US law will evaluate artificial intelligence for Covid-19](https://www.bmj.com/content/372/bmj.n234.full)

Apologies for the self-promo, but hope you enjoy!",1635449652.0,2021-10-28 21:34:12
[D] Migration of big datasets into the clouds,2,qhpncx,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhpncx/d_migration_of_big_datasets_into_the_clouds/,10,"lets say i have 100 to 200 GB of float32 .csv-files to train a tensorflow model on.

1. **How would migrating those into the cloud work in terms of timeefficiency?**
2. e.g. if the machine is 8$/Hour, wouldnt that be a whole day for uploading it already?(Compressing   and decompressing it wouldnt work either, since the machine should  only  have 8 cores, so that would take more time than it saves i guess.) 
3. 3. **How does that work with vast-ai, amazon aws, and google ?**
4. 4. Or am i bound to buy my own workstation?",1635435168.0,2021-10-28 17:32:48
[R] Two NeurIPS 2021 Papers on Weak-shot Learning,0,qi135y,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qi135y/r_two_neurips_2021_papers_on_weakshot_learning/,1,"I am glad to announce that we have two papers on weak-shot learning accepted by NeurIPS 2021. Weak-shot learning is a learning paradigm with full annotations for base categories and weak annotations for novel categories.

The first paper is about weak-shot classification: ""Weak-shot Fine-grained Classification via Similarity Transfer"".

paper link: [https://arxiv.org/pdf/2009.09197.pdf](https://arxiv.org/pdf/2009.09197.pdf)

code and dataset: [https://github.com/bcmi/SimTrans-Weak-Shot-Classification](https://github.com/bcmi/SimTrans-Weak-Shot-Classification)

The second paper is about weak-shot object detection: "" Mixed Supervised Object Detection by Transferring Mask Prior and Semantic Similarity "".

paper link: [https://arxiv.org/pdf/2110.14191.pdf](https://arxiv.org/pdf/2110.14191.pdf)

code: [https://github.com/bcmi/TraMaS-Weak-Shot-Object-Detection](https://github.com/bcmi/TraMaS-Weak-Shot-Object-Detection)

I have also written a brief introduction to weak-shot learning [https://arxiv.org/pdf/2110.02651.pdf](https://arxiv.org/pdf/2110.02651.pdf) and summarized the related papers/codes at [https://github.com/bcmi/Awesome-Weak-Shot-Learning](https://github.com/bcmi/Awesome-Weak-Shot-Learning). Welcome to pay attention to weak-shot learning!",1635469315.0,2021-10-29 03:01:55
"HACKtheMACHINE Unmanned [Virtual challenge, $90k in prizes, FREE entry, CS Bug Bounty, AI/ML Challenge, MBSE Challenge] [D]",0,qhq5x4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhq5x4/hackthemachine_unmanned_virtual_challenge_90k_in/,0,"&#x200B;

https://preview.redd.it/wjvwm4wpr7w71.png?width=1702&format=png&auto=webp&s=47a8c0c298ab2fe964480de4cdc9d584bdfda8ae

The U.S. Navy's premiere digital experience, HACKtheMACHINE is happening again from November 16 - 19! There are three events this time around... Hack the Pilot: Hack into a real unmanned vehicle autopilot system and get paid for finding bugs. Detective Bot: Use AI/ML to detect malicious data in high security environments. Top Model: Employ MBSE to simulate unmanned swarm missions. All of this and more from the comfort of your couch! Up to $90k in cash prizes! Free to enter!

This event started in 2016 as an effort to find solutions to the Navy's biggest technological problems, while providing a venue for Sailors in the Navy cybersecurity space to interface with their private sector counterparts.

To learn more visit [HACKtheMACHINE.ai](https://www.HACKtheMACHINE.ai)",1635436745.0,2021-10-28 17:59:05
[D] Interview Questions asked in AI/DL Research Scientist positions at top techs,35,qh59gp,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh59gp/d_interview_questions_asked_in_aidl_research/,22,"I was curious about the kind of questions that are asked in an interviews for AI/DL Research Scientist (or similar) positions in top tech companies like FAANG (but not limited to, of course). I tried finding online but could only get some vague answer without specifics. Those who experienced such interviews, can you share some questions/topics that you faced in such research interviews ?

While answering, please consider the following:

1. Be specific. Don't write something vague, like ""Basics of classifiers, Standard questions on SVM""
2. You can write in detail, like specific equations, theorems, follow up questions, specific papers discussed etc.
3. Software Engineering style questions are not preferred; try to focus on core research questions/topics.

*PS: You don't have to specify the name of the company if you don't want to, or whether you got the position or not.*",1635365016.0,2021-10-27 22:03:36
[Research] Feeding coordinates (Lat/Long or Projection) into neural network properly,97,qgvkkm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgvkkm/research_feeding_coordinates_latlong_or/,75,"hi there,

I'm looking to predict river level change with time using several stations distributed around a big geographical space. I have determined the parameters, however, one of the parameters which proved effective is the location of nearby measurements stations to the observed one. so I want my neural network to be spatially aware of each well location when its training the model.

things I have considered so far:

1. Calculate the Euclidian Distance between each well in relation to the observed well and use that as input (if i have 8 wells, then i would add 7 new variables to the existing parameters which will be inputs to the ANN).
2. Use Hillbert's space-filling curve to convert the 2D coordinates into 1D and feed it to the ANN.

However, so far Option (1) would give inaccurate results for similar distances as its a scaler value not vector. and for Option (2), I have seen people here in the forum say that its not appropriate to use it as its inverse is not continuous and thus inappropriate to use as input.

is what I have summarized so far correct? and is there any other option aside from the above to make the network recognize geographic locations?

your help/feedback would be extremely appreciated as I have been stuck at this issue for a while now.

EDIT: the responses have been extremely helpful and i am immensely grateful to all of you. i wanted to add a few things to the thread as the answers are raising a few issues in my mind.

when i created this thread, my original goal was to find a way to feed the network a map and let it know the locations of the measurement stations in it. and honestly i thought there would be a standard or a widespread way to do so but everyone is doing it in his own way and some more complicated than others and unfortunately with no degree of the effectiveness of each method.

i was speaking to a math friend who suggested to project the Lat/Long into UTM zone and use either Hilbert curve or Fourier transform to convert it into one dimension or frequency domain and feed it to the network, she said she wasn't sure of the solution but it might work so i thought i would add it to the discussion as option (3).",1635337758.0,2021-10-27 14:29:18
[D] Machine learning generative models for automatic design of multi-material 3D printed composite solids,1,qhl60o,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhl60o/d_machine_learning_generative_models_for/,0,"Hello,  In the following paper, I cannot find the source code they implemented (I tried to reach the author no reply), I am a newbie in that area of generative design for multi-material, I am wondering if anyone is familiar with similar work with source code attached.    Source: [https://www.sciencedi](https://www.sciencedi)",1635421516.0,2021-10-28 13:45:16
[D] TargetCLIP explained - Image-Based CLIP-Guided Essence Transfer (5-minute summary by Casual GAN Papers),27,qgzoey,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgzoey/d_targetclip_explained_imagebased_clipguided/,3,"There  has recently been a lot of interest concerning a new generation of   style-transfer models. These work on a higher level of abstraction and   rather than focusing on transferring colors and textures from one image   to another, they combine the conceptual â€œstyleâ€ of one image and the   objective â€œcontentâ€ of another in an entirely new image altogether. A   recent paper by Hila Chefer and the team at Tel Aviv University does   just that! The authors propose TargetCLIP, a blending operator that   combines the powerful StyleGAN2 generator with a semantic network CLIP   to achieve a more natural blending than with each model separately. On a   practical level, this idea is implemented with two losses - one that   ensures the output image is similar to the input in the CLIP space, the   other - that the shifts in the CLIP space are linked to shifts in the   StyleGAN space.

Full summary: [https://t.me/casual\_gan/165](https://t.me/casual_gan/165)

[TargetCLIP](https://preview.redd.it/6zvejtumk0w71.jpg?width=767&format=pjpg&auto=webp&s=ae20726ef2037564924e634825e4c668bc9200d0)

arxiv: [https://arxiv.org/pdf/2110.12427.pdf](https://arxiv.org/pdf/2110.12427.pdf)

code: [https://github.com/hila-chefer/TargetCLIP](https://github.com/hila-chefer/TargetCLIP)

web digest: [https://www.casualganpapers.com/clip\_image\_to\_image\_style\_transfer\_essence\_transfer/TargetCLIP-explained.html](https://www.casualganpapers.com/clip_image_to_image_style_transfer_essence_transfer/TargetCLIP-explained.html)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",1635349687.0,2021-10-27 17:48:07
[Discussion] Aren't all unserpervised learning tasks basically clustering afterall ?,2,qhj1e1,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qhj1e1/discussion_arent_all_unserpervised_learning_tasks/,7,"If  I think of unsupervised learning, I basically think of clustering but  also anomaly detection, learning the Gaussian mixture of a probability  distribution or learning latent variables in a data distribution....

But fundamentally isn't the purpose of anomaly detection to detect when a data point is outside a given (or learned) cluster ?

Isn't  learning a mixture to regroup the points of high probability density  and regroup them inside a cluster that we condition to be Gaussian.

Isn't learning latent variable of a distribution discovering the implict clusters of your data distribution ?

Isn't all the unsupervised task basically relying on a clustering ?

I  ask those questions to understand unsupervised learning, because (like  many of you I think) I see those unsupervised tasks as the only  philosophical way to reach a global intelligence scheme.

I  would like to see if we cannot extend the limits of clustering (see the  impossibility theorem for example...) to every unsupervised tasks.

Thank you in advance for your answers or remarks.

PS : Already post in r/ArtificialInteligence but they is fewer people there, so sorry for the double, I'm still new to this website.",1635412840.0,2021-10-28 11:20:40
[D] Time series generation using GANs - when to stop training?,15,qh26yp,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh26yp/d_time_series_generation_using_gans_when_to_stop/,9,"For image generation, one may inspect the synthesized images or rely on metrics like inception score and FID. But time series cannot be visually ""confirmed"" and I haven't been able to find peer-reviewed work for time-series-specific metrics.",1635356640.0,2021-10-27 19:44:00
[P] PyCM 3.3 released: Comparison of Classifiers Based on Confusion Matrix,6,qh7flk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh7flk/p_pycm_33_released_comparison_of_classifiers/,0,"Hi ML practitioners,

We wanted to bring to your attention another release of PyCM( Multi-class confusion matrix library in Python). In this version, the comparison system bugs are fixed and now it is possible to compare classifiers with more advanced settings.

&#x200B;

https://preview.redd.it/1d9o05k5a2w71.png?width=596&format=png&auto=webp&s=1f8e321778261c19a773b00c8b0215af61dca6cf

  
 

https://preview.redd.it/h6zda4sda2w71.png?width=1762&format=png&auto=webp&s=a254817d9607c9135cb62a765cd3b5c4c76959a6

    >>> from pycm import ConfusionMatrix, Compare
    >>> cm1 = ConfusionMatrix(matrix={0:{0:2,1:50,2:6},1:{0:5,1:50,2:3},2:{0:1,1:7,2:50}})
    >>> cm2 = ConfusionMatrix(matrix={0:{0:50,1:2,2:6},1:{0:50,1:5,2:3},2:{0:1,1:55,2:2}})
    >>> cm1.print_matrix()
    Predict  0        1        2        
    Actual
    0        2        50       6        
    
    1        5        50       3        
    
    2        1        7        50       
    
    
    >>> cm2.print_matrix()
    Predict  0        1        2        
    Actual
    0        50       2        6        
    
    1        50       5        3        
    
    2        1        55       2        
    >>> cp = Compare({""cm1"":cm1,""cm2"":cm2},class_weight={0:2,1:1,2:1})
    >>> print(cp)
    Best : cm1
    
    Rank  Name   Class-Score       Overall-Score
    1     cm1    0.43542           0.425
    2     cm2    0.3875            0.33056
    
    >>> cp.best_name
    'cm1'
    >>> cp2 = Compare({""cm1"":cm1,""cm2"":cm2},by_class=True,class_weight={0:5,1:1,2:1})
    >>> print(cp2)
    Best : cm2
    
    Rank  Name   Class-Score       Overall-Score
    1     cm2    0.45357           0.33056
    2     cm1    0.34881           0.425
    
    >>> cp3 = Compare({""cm1"":cm1,""cm2"":cm2},class_benchmark_weight={""PLRI"":0,""NLRI"":0,""DPI"":0,""AUCI"":1,""MCCI"":0,""QI"":0})
    >>> print(cp3)
    Best : cm1
    
    Rank  Name   Class-Score       Overall-Score
    1     cm1    0.46667           0.425
    2     cm2    0.33333           0.33056
    
    >>> cp3.best_name
    'cm1'

Website : [www.pycm.ir](https://www.pycm.ir)

Repo : [https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

&#x200B;

Hope you find it useful!",1635371090.0,2021-10-27 23:44:50
[R] The Efficiency Misnomer,5,qh4v7u,MachineLearning,https://arxiv.org/abs/2110.12894,1,,1635363934.0,2021-10-27 21:45:34
[D] Is Pytorch Lightning Production Ready?,26,qgqq07,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgqq07/d_is_pytorch_lightning_production_ready/,28,"Hi.

I'm planning to work on multiple RnD projects that are going to be released into products if they indicate good performance, and I was thinking of correctly choosing my stack. Ever since I've discovered PL, my life has become way more eaiser than before, but I was always doing solo stuff working on research projects. Now we are trying to design/implement an ML training loop, I was thinking of choosing PL as an interface over pytorch to automate a lot of things that we were going to write from scratch. What is the general consensus of PL for production in here? Is it unrelaible?",1635318181.0,2021-10-27 09:03:01
"[D] Deciding to publish, but where?",12,qgtnz2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgtnz2/d_deciding_to_publish_but_where/,9,"Hello guys, I'm just curious about where will I publish my works in the future.

As you know, there's a various of conferences in the field of AI such as ICCV, ECCV, ICML, NeurlIPS, AAAI, etc.

However, In my perspective, I don't know what is different specifcally. I know that all of them is for computer vision fields, and they all are top conferences, but where will I decide publicating my works?

Usually, this question would be conveyed to my supervisor, but I don't have it.

Thank you all.",1635330853.0,2021-10-27 12:34:13
[D] What are some SOTA image classification architectures that run on edge-devices?,5,qgvefy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgvefy/d_what_are_some_sota_image_classification/,2,I am trying to deploy a classification model on a Raspberry Pi 4 that is set up with PyTorch 1.9.0. The device is connected to a camera that periodically takes sky photographs and does a simple cloud classification task. I now have a small dataset with me and I am thinking of using a classification model that can run on the 8GB version.,1635337173.0,2021-10-27 14:19:33
[D] The Ownership Dilemma of ML Pipelines In Production,0,qh62wg,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh62wg/d_the_ownership_dilemma_of_ml_pipelines_in/,4,"One question that organizations developing machine learning need to answer is **who owns ML pipelines in production**? Is it the data scientist who creates the model? Is it the data engineer who deploys it in production? Is it someone else altogether?  


My personal opinion here is that it only makes sense for the producer of the models, i.e., the data scientist, to be the one who takes the model to production. Obviously this requires higher abstractions  to accommodate for the skill set gap between a production deployment vs the often script-like experimentation phase in the ML development lifecycle.   


I've hashed the argument above in a blog post here: [https://towardsdatascience.com/taking-on-the-ml-pipeline-challenge-3dfeb7b24cc6](https://towardsdatascience.com/taking-on-the-ml-pipeline-challenge-3dfeb7b24cc6)   


Would be happy to hear opinions on what people in this community think:  Should the process of taking ML to production be owned by the data scientist, or should the data scientist just 'stay in their lanes' and throw it over the wall to more engineering-driven teams?",1635367293.0,2021-10-27 22:41:33
"How can I train an RNN on data with multiple potentially ""correct"" outputs for each input? [R]",2,qh12bn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh12bn/how_can_i_train_an_rnn_on_data_with_multiple/,3,"Specifically,  I'm thinking about a cryptography problem in which a  text is  enciphered with a simple cipher (for example Vigenere) but  spaces are  preserved. I'd like to teach an RNN (or Transformer) to do  ""cribbing""  in the following way:

Input: a sequence containing the characters of a given enciphered word of length *L*, plus positional encodings.

Output: a sequence of numbers that is then converted to characters.

Loss: minimum Levenshtein distance between the output and any valid English word of length *L*

How  would I actually set up this architecture (namely, is the fact  that  I'm not comparing against a fixed output for each sample a  concern?). A  concern I have with the way I've formulated the problem  above is that  the network could simply learn to output the same English  word for  every sequence of a given length- how could I prevent against  this?",1635353493.0,2021-10-27 18:51:33
[Discussion] Model 2 Binary executable,2,qh0hcn,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh0hcn/discussion_model_2_binary_executable/,1,I have some models that could potentially work very well in realtime CPU inference and I've been thinking to create somehow a binary executable to use them or some kind of real software. What are my options? Did any of you create something like this?,1635351880.0,2021-10-27 18:24:40
[R] Parameter Prediction for Unseen Deep Architectures,19,qgnvyt,MachineLearning,https://arxiv.org/abs/2110.13100,5,,1635306796.0,2021-10-27 05:53:16
[D] A Guide to Teslaâ€™s Configurable Floating Point Formats & Arithmetic,44,qghljd,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qghljd/d_a_guide_to_teslas_configurable_floating_point/,26,"Tesla just randomly dropped a PDF with details of the custom floating point formats they've created for their Dojo training hardware: https://tesla-cdn.thron.com/static/SBY4B9_tesla-dojo-technology_OPNZ0M.pdf

I think it's pretty interesting. They want to eliminate 32 bit floating point from training almost entirely, using custom 16-bit and even 8-bit floating point formats instead, with a configurable ""exponent bias"" that is shared between many numbers and can apparently be learned during training. Also, they have stochastic rounding which seems like a great idea for low precision formats. Worth a glance if you care about hardware.",1635286259.0,2021-10-27 00:10:59
[D] How can companies like Facebook use Pytorch for commercial applications when BN and dropout are patented?,238,qg4750,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg4750/d_how_can_companies_like_facebook_use_pytorch_for/,106,"Tensorflow has [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), so layers such as Batch Norm and Dropout are covered in the license. Pytorch uses [The 3-Clause BSD License](https://opensource.org/licenses/BSD-3-Clause), which does not cover patent infringements.

Are there some loopholes to use Pytorch with these layers or do they pay for using these? Or just ignore and use them regardless of the consequences?",1635247192.0,2021-10-26 13:19:52
[D] how to make best use of gpu cluster?,1,qh0xbu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh0xbu/d_how_to_make_best_use_of_gpu_cluster/,6," 

How do you guys choose to make the most use of a gpu cluster for inference for a DL pipeline that is made up of multiple models?

Do you just do distributed inference for each model or if some of the models can run in paralell send individual models to specific gpus?",1635353109.0,2021-10-27 18:45:09
[Research] Machine Learning Topics(data science),0,qh8ai4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qh8ai4/research_machine_learning_topicsdata_science/,1,Any suggestions for Data science or Machine learning for my Masters thesis. Iâ€™m in Ghana #datasciencewithpython,1635373587.0,2021-10-28 00:26:27
[D] Semi-supervised learning with CycleGAN,2,qguvg2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qguvg2/d_semisupervised_learning_with_cyclegan/,7,"Hey!   
I have been training CycleGAN with medical images for unsupervised image-to-image style transfer. The results are good but not there yet. So, I thought of doing a semi-supervised learning experiment. The images are largely unpaired, but I have got paired (registered) images as well. So, I designed an experiment in which I fed 10% paired & 90% unpaired data for training. Then I tried different percentages of paired data but didn't see much difference in the end result. 

The above observation led me to the conclusion that CycleGAN doesn't have a loss function that helps it directly benefit from the paired data, the cycle consistency loss helps but it's still indirect. There is no direct way to compare the generated images with the ground truth in the case of paired data.

Then I had this crazy idea (disclaimer: it may sound stupid), what if we introduce another L1 loss term for the direct comparison of the generated image with the (in a sense) ground truth. I know for the most part of the training it will confuse the model because of the high percentage of unpaired data (and therefore, it will have a lot smaller weight compared to cycle consistency loss and identity loss). But, can this loss be considered a small noise in the case of unpaired data? And for paired data, it will actually help the model take direct advantage of data alignment.

So, I wanted to ask my fellow machine learning practitioners, especially the seasoned ones, is there any merit to this idea or is it utterly asinine?",1635335399.0,2021-10-27 13:49:59
"[P] ImageNet size-accuracy pareto front, 2012-2021",22,qghjqi,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qghjqi/p_imagenet_sizeaccuracy_pareto_front_20122021/,10,"&#x200B;

https://preview.redd.it/taqxj24javv71.png?width=1458&format=png&auto=webp&s=2e109873952da6c6fd211f779bfd81cea8c4b53a

Paperswithcode has expanded their [imagenet results](https://paperswithcode.com/sota/image-classification-on-imagenet) with parameter counts. I was curious to see how size-vs-accuracy has evolved over time. Caveat: some 40% of the results lack parameter counts, so those are excluded here.",1635286103.0,2021-10-27 00:08:23
"[D] Neural Network/Algorithm for Unsupervised, Multivariate, Sequential Classification - Human Movement Patterns",2,qgucxy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgucxy/d_neural_networkalgorithm_for_unsupervised/,2,"I\`m trying to classify human movement states (standing, walking, etc.) from joint (knee, hip, etc.) angle data over time. To use unlabeled movement datasets, I am looking for an unsupervised method. Does anybody have an idea what I could use?",1635333537.0,2021-10-27 13:18:57
[D] What qualifies as a full stack ML Engineer?,75,qg5aya,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg5aya/d_what_qualifies_as_a_full_stack_ml_engineer/,49,"I see the term ""full stack ml engineer"" pop up more and more. I'm curious what people feel is the minimum requirement to be able to call oneself  that with a straight face. And also if it's really a trend we're seeing or just a temporary buzz word.

To my mind, when I think of a full stack ML engineer you need to have a 3 out of 5 or better proficiency in these areas

* DevOps
* CloudOps
* MLOps 
* Data Science
* Data Engineering

Depending on the specific job/company you can probably get away with a couple of 2s in there. Curious what other people are thinking, and if you think we are moving toward or away from full stack roles in ML.",1635251274.0,2021-10-26 14:27:54
[R] Spectral Bias in Practice: The Role of Function Frequency in Generalization,6,qgltks,MachineLearning,https://arxiv.org/abs/2110.02424,2,,1635299661.0,2021-10-27 03:54:21
[D] How you deploy your ML model?,25,qg9zcc,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg9zcc/d_how_you_deploy_your_ml_model/,52,"Hello ML community,

I wanted to get some quick feedback from the ML engineers in this community about their deployment hardware/software specification. For example, when you deploy a 12 layer BERT or YoloV5s or some other model:

1. Do use GPU or CPU? (and which model of GPU or CPU)
2. Do you need batch size 1 or larger batch size?
3. If batch size 1, then what is the target latency requirement that usually works well?
4. What software stack do you use? Is it CUDA, TensorRT, OpenVINO or other.
5. What you like and what you hate about the deployment process?
6. And anything else related

In general, the ML deployment space is a bit fragmented and complicated, so it would be good to see what people use and what works well. Feel free to provide any input or feedback!",1635265095.0,2021-10-26 18:18:15
"[P] Mlflow, fastapi, streamlit template",25,qg9m51,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg9m51/p_mlflow_fastapi_streamlit_template/,4,"Combined torch, streamlit, fastapi, mlflow in a sample project. This was done to practice the tools and because I haven't found a similar pipeline. 

[https://github.com/zademn/mnist-mlops-learning](https://github.com/zademn/mnist-mlops-learning)",1635264076.0,2021-10-26 18:01:16
[D] How can I mark uncertain point according to marked point in image,0,qgrm1g,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgrm1g/d_how_can_i_mark_uncertain_point_according_to/,1,"There is a photo, i already have a part of annotation of image, the annotation is incomplete, many uncertain points in it. I want convert uncertain points to certain points according to existing knowledge. I find a method named CRF(Conditional Random Field), but it isn't use existing knowledge, do you have something else methods?",1635322045.0,2021-10-27 10:07:25
[Project] Aim v3.0.0 - revamped UI and revamped backend to query experiments faster and nicer!,21,qg930d,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg930d/project_aim_v300_revamped_ui_and_revamped_backend/,4,"Hey r/Machinelearning!

I am Gev, co-author of Aim. 

Excited to share with you the latest version of **Aim 3.0.0**! 

We have been hard at work in the past couple of months and made lots of changes. 

The most important changes include:

**Completely revamped UI**

* Home page and run detail page
* Runs, metrics and params explorers
* Bookmarks and Tags

**Revamped Python SDK**

* New and much more intuitive (but still quite vanilla) API to track your training runs
* New and 10x faster embedded storage based on Rocksdb. It will allow us to store virtually any type of AI metadata (as opposed to AimRecords that was specifically designed for metrics and hyperparams)

We have also published our roadmap:

repo: [github.com/aimhubio/aim](http://github.com/aimhubio/aim) 

blog: [bit.ly/3vzh2cA](https://t.co/3WH2KkKIRq?amp=1)

**Would love your feedback on our work!!** ðŸ˜Š

https://i.redd.it/ftho4d6pctv71.gif",1635262589.0,2021-10-26 17:36:29
"[D] How do you handle ""What are you expecting for a salary?"" during job interviews?",224,qftzey,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qftzey/d_how_do_you_handle_what_are_you_expecting_for_a/,133,"I get tripped up on this. every. time. and the salary discussions on here are more about ""how much do you make"" and not ""how do you negotiate ML Salaries?"" -- we seem to be in a weird place because the job landscape is so vast and responsibilities are not always the same between companies for the same position title.

I'm an ML Engineer with ~5 years of experience, prior experience as a software engineer supporting an ML team as well as experience as a data analyst for an ML Product. I've driven business value both customer-facing and backend, had experience working as a makeshift product manager, moved on-prem ML architecture to the cloud, yadda yadda yadda. I have experience.

However, I never know what to say when folks ask what salary I'm expecting. I kinda just ball park in the mid $100s, but I think that I might be underselling myself sometimes -- especially when talking with bay area companies. They tell me ""we pay bay area salaries"" and I honestly have no fucking clue what that means. 

Direct question: If a company expects me to move out to the bay area, should I be asking for mid 200s or higher, or keep it at the ""average"" I see of ~150?",1635206693.0,2021-10-26 02:04:53
[D] Help with JODIE model data (Temporal Interaction Networks),1,qgo8ir,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgo8ir/d_help_with_jodie_model_data_temporal_interaction/,1," Hey everyone,

Iâ€™m trying to create my own Reddit dataset to implement JODIE model used in the â€˜Predicting Dynamic Embedding Trajectory in Temporal Interaction Networksâ€™ paper (Paper: [https://arxiv.org/abs/1908.01207](https://arxiv.org/abs/1908.01207) Code: [https://github.com/srijankr/jodie](https://github.com/srijankr/jodie)) The paper is a bit vague on what features they use and how the data looks like.

I also downloaded the reddit.csv data they had available but it looks strange, not sure how it was formatted. There are only 5 columns but there are a bunch of numbers in front of those columns as seen in the picture. Does anyone know any information of how their data is set up or how it needs to be formatted to use the model?

[reddit data](https://preview.redd.it/xbfdzcfa4xv71.png?width=1648&format=png&auto=webp&s=5eb4217864960d033be4158ae9cfe6d9bc55c17f)

Please let me know if there is a better sub I could post this! Thanks for the help!",1635308019.0,2021-10-27 06:13:39
[D] Is there a clear way for doing Font Style Transfer between words with neural networks?,5,qgbelu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgbelu/d_is_there_a_clear_way_for_doing_font_style/,3,"I am currently working on Font Style Transfer between words. Here is an example:

&#x200B;

[ Left: Source image. Middle: Style images. Right: Target. ](https://preview.redd.it/wj26py5owtv71.png?width=3444&format=png&auto=webp&s=a73ef7c1b049bdd4ee083830f7e1126bf0e06040)

So I tried a couple of architectures for font style transfer. One of them is [FET-GAN](https://github.com/liweileev/FET-GAN) which worked pretty fine for font transfer between letters, but had some problems:

* It couldn't generalize to new fonts from style images. Only trained fonts could be transferred.
* It had a poor performance for word images since the variation in the training data is higher than letter images.

That's why I want to ask if there are other architectures that solve the above mentioned problems.",1635269034.0,2021-10-26 19:23:54
[R] Neural Tangent Kernel Eigenvalues Accurately Predict Generalization,47,qfy76l,MachineLearning,https://arxiv.org/abs/2110.03922,11,,1635221031.0,2021-10-26 06:03:51
[N] Microsoft Releases â€˜ORBITâ€™ Dataset: A Real-World Few-Shot Dataset for Teachable Object Recognition,3,qgchx9,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgchx9/n_microsoft_releases_orbit_dataset_a_realworld/,0,"Object recognition algorithms have come a long way in recent years, but they still require training datasets containing thousands of high-quality, annotated examples for every object category.

Few-shot learning addresses this huge demand for datasets by training models to recognize entirely new things from only a few examples. Meta-learning algorithms, in particular, which â€˜learn to learnâ€™ utilizing episodic training, can potentially reduce the number of training examples required to train a model. The majority of few-shot learning research, on the other hand, has been driven by benchmark datasets that lack the substantial variability that applications confront when deployed in the real world.

To bridge this gap, Microsoft researchers, in collaboration with the City, University of London, present the ORBIT dataset and few-shot benchmark that helps learn new objects from a small number of high-variation samples. This new benchmark dataset includes a total of 2,687,934 frames, containing 3,822 videos of 486 objects captured on mobile phones by 77 people who are blind or have low vision.

# [Quick 5 Min Read](https://www.marktechpost.com/2021/10/26/microsoft-ai-research-releases-orbit-dataset-a-real-world-few-shot-dataset-for-teachable-object-recognition/) |[Paper](https://arxiv.org/abs/2104.03841#)| [Github](https://github.com/microsoft/ORBIT-Dataset) | [Microsoft Blog](https://www.microsoft.com/en-us/research/blog/announcing-the-orbit-dataset-advancing-real-world-few-shot-learning-using-teachable-object-recognition/)

https://preview.redd.it/zeajdfwq5uv71.png?width=1391&format=png&auto=webp&s=298af0e1ddaecec4b61b12d3bf7ff2099caa822b",1635272018.0,2021-10-26 20:13:38
[D] StyleGAN3 using a starting image?,0,qgixti,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgixti/d_stylegan3_using_a_starting_image/,0,"Hello,  
I've been playing with StyleGAN3 setups in notebooks for a bit, but I'm really hoping to find an implementation that starts with an image of my choosing. Do you have any suggestions for where to start?",1635290239.0,2021-10-27 01:17:19
[Discussion] Biotech and HealthTech companies using ML,0,qgmvay,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qgmvay/discussion_biotech_and_healthtech_companies_using/,1,"Are there any US/Canadian Biotech and HealthTech companies doing interesting work using ML?  
So far, in my limited research..I came across these Startup to Mid-size companies

* [https://www.quantum-si.com/](https://www.quantum-si.com/)
* [https://www.butterflynetwork.com/](https://www.butterflynetwork.com/)
* [https://www.laddertx.com/](https://www.laddertx.com/)",1635303183.0,2021-10-27 04:53:03
[D] Efficiently loading videos in PyTorch without extracting frames,9,qg27dt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg27dt/d_efficiently_loading_videos_in_pytorch_without/,17,"Hi,

I was wondering if someone knew an efficient way to load videos in PyTorch without extracting frames to files before. I'm working with the VoxCeleb2 dataset with contains more than 1 million videos and I have calculated that saving frames to PNG would require about 18 To of disk space so I would prefer not to have to extract frames.

So far I'm using VideoCapture from OpenCV but I can't use my GPU to 100% because the data loading takes too much time,  I tried Nvidia DALI but it is even slower than OpenCV.

Thank you",1635238496.0,2021-10-26 10:54:56
[R] A Comprehensive Comparison of Word Embeddings in Event & Entity Coreference Resolution,1,qg5f1m,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qg5f1m/r_a_comprehensive_comparison_of_word_embeddings/,0,"Hello reddit, this is my first paper which has been accepted at Findings of EMNLP 2021.

Words are made letters that cannot be understood by AI as is. Thus, word embeddings are tools used to encode a vocabulary of words into a mathematical space which allows deep learning models to ingest textual data. To date, many word embeddings methods exist with various characteristics.

Hence, this paper studies how the various kind and various combinations of these embeddings perform. Additionally, I found that while there exist various kind of embeddings which have been trained differently, combining them does not greatly improve performance. This has a few consequence such as the fact that word embeddings are better compared when used alone instead of alongside others otherwise their difference in performance is overshadowed by the performance already provided by other embeddings in the system.

[https://arxiv.org/abs/2110.05115](https://arxiv.org/abs/2110.05115)",1635251654.0,2021-10-26 14:34:14
[R] Facebook AI Releases SaLinA: A Flexible and Simple Library for Learning Sequential Agents,57,qfi5sz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfi5sz/r_facebook_ai_releases_salina_a_flexible_and/,5,"A Facebook AI research team releases SaLinA, a reinforcement learning (RL) library for model-based RL, differentiable environments and multi-agent RL that simplifies the implementation of complex sequential learning models. 

Here is a quick read: [Facebook AI Releases SaLinA: A Flexible and Simple Library for Learning Sequential Agents.](https://syncedreview.com/2021/10/25/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-130/)

The paper *SaLinA: Sequential Learning of Agents* is on [arXiv](https://arxiv.org/abs/2110.07910).",1635172729.0,2021-10-25 16:38:49
[D] UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction,40,qfk1yt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfk1yt/d_umap_uniform_manifold_approximation_and/,19,"I am just working on a clustering project and curious what people will think about this algorithm. [https://arxiv.org/abs/1802.03426](https://arxiv.org/abs/1802.03426)

I read guides and many says it is better than tSNE.

I would love to hear about your experience with any other clustering algorithms (other then the normal scikit learn ones)?",1635178013.0,2021-10-25 18:06:53
[P] Pywick - High-Level Training framework for Pytorch,23,qfjjm6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfjjm6/p_pywick_highlevel_training_framework_for_pytorch/,0,"Hi AI practitioners,

We wanted to bring to your attention another huge release of the [Pywick](https://github.com/achaiah/pywick) training framework. Some notable features that you may find useful are:
- 700+ classification network variants
- Dockerized runtime for easy execution in the cloud. Demo included with the docker image!
- Full configuration via `yaml` files with no coding required to get started
- Bleeding edge optimizers, loss functions, activation functions etc
- Thorough documentation

Hope you find it useful!",1635176603.0,2021-10-25 17:43:23
[D] New in-depth AI interview episode out! Yuval was featured on 2 minute papers for his incredible work on AI toonification.,253,qf7ar3,MachineLearning,https://v.redd.it/u5ec6kazeiv71,3,,1635129850.0,2021-10-25 04:44:10
Normalizing data for anomaly detection [D],2,qfy49c,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfy49c/normalizing_data_for_anomaly_detection_d/,2,"Can performing a min max normalization on a dataset that contains anomalies affect the performance of a model since the values of the anomalous data points will have some kind of affect on the values of the normal data points due to the nature of min max normalization? And if so, is there anything that can be done to get around this?",1635220742.0,2021-10-26 05:59:02
[R] Microsoft AI Open-Sources â€˜PyTorch-DirectMLâ€™: A Package To Train Machine Learning Models On GPUs,97,qfaxcv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfaxcv/r_microsoft_ai_opensources_pytorchdirectml_a/,19,"The Microsoft Windows AI team has announced the [f](https://devblogs.microsoft.com/windowsai/introducing-pytorch-directml-train-your-machine-learning-models-on-any-gpu/)[irst preview of DirectML as a backend to PyTorch for training ML models](https://devblogs.microsoft.com/windowsai/introducing-pytorch-directml-train-your-machine-learning-models-on-any-gpu/). This release allows accelerated machine learning training for PyTorch on any DirectX12 GPU and WSL, unlocking new potential in computing with mixed reality.

Microsoft AI team has teamed up with the PyTorch framework to release a preview package that provides scoped support for CNNs (convolutional neural networks). In this new device named â€œDML,â€ Direct ML APIs and Tensor primitives are called through by introducing minimal overhead when calling into operators; they work in much like other existing backends.

# [3 Min Quick Read](https://www.marktechpost.com/2021/10/24/microsoft-ai-open-sources-pytorch-directml-a-package-to-train-machine-learning-models-on-gpus/)| [Github](https://github.com/microsoft/DirectML/) | [Microsoft Blog](https://devblogs.microsoft.com/windowsai/introducing-pytorch-directml-train-your-machine-learning-models-on-any-gpu/)

https://preview.redd.it/xiwwpljxmjv71.jpg?width=1392&format=pjpg&auto=webp&s=d3fb42185c6f4abc754e79c0638080f97f9bda56",1635144637.0,2021-10-25 08:50:37
[D] - Algorithmically choosing best training data for semantic segmentation,3,qfv2za,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfv2za/d_algorithmically_choosing_best_training_data_for/,0,"I'm working on a semantic segmentation problem with access to a very large dataset of image-label pairs for training. While quantity of the training data has never been an issue, the quality is. What I mean is a large percentage of our data does not have very accurate GT segmentation labels. My intuition is that its better to have smaller, but high quality image-label pairs, rather than having a very large, but inaccurate image-label pairs.

&#x200B;

1. First of all, is my intuition correct and are there papers to support this
2. Secondly, is there a good, algorithmic way to choose high quality training data from this large pool of image-label pairs ? I'm thinking of computer vision algorithms or some sort of preprocessing to filter out potentially bad training examples ?",1635210357.0,2021-10-26 03:05:57
[D] Usefulness of self-supervised pretraining,11,qfm81h,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfm81h/d_usefulness_of_selfsupervised_pretraining/,7,"I've skimmed a few videos/papers on self-supervised learning, and it seems like a useful way to pretrain a model if you have limited labeled training data, but I don't often hear a lot people using it in that way.

A few questions:

1. Is it usually any more effective than pretraining an autoencoder/denoising autoencoder using all of data and then fine-tuning using labeled data? I'm not entirely sure why it would be? I guess I might be missing the point of self-supervised learning.
2. Newer methods seem to require a ton of compute (I think a ""small batch size"" was 128 in one of the papers I skimmed). Can older methods Ex. Jigsaw still be useful? The task I'm thinking about has a fair amount of labeled data (About equal to unlabeled), but I'm wondering if pretraining would improve performance marginally.",1635184050.0,2021-10-25 19:47:30
[D] [R] Transitioning to research roles,9,qfgbtk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfgbtk/d_r_transitioning_to_research_roles/,13,"Looking for advice on how to transition to a more research-related career and how to get more research experience.

I graduated last year with a degree in Computer science and Mathematics and have been working as a data scientist/analyst since then. Right now I'm interested in pursuing a master's in computer science researching ML and am wondering what the best route to take is. 

I have a few months of research experience working with a professor during my last year in university but that's about it. I was considering cold emailing some profs at other universities to see if I could work with them but not sure how weird that is as I'm graduated and have been working full time for a year now.

I know this is a fairly saturated field and can be quite competitive when applying to top universities so I don't have too high hopes that I'll be accepted to a master's program at schools I want with profs I am interested in. I could apply to the school I graduated from but it is not as great for ML in terms of profs and I am not sure if it is worth my time.

As well, if I would not be able to get accepted to Master's, then am looking at how to transition from data scientist to more research roles and wondering how feasible that would be.
Any advice is great thanks!",1635167192.0,2021-10-25 15:06:32
Is semantic segmentation solved? [D],0,qfurkx,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfurkx/is_semantic_segmentation_solved_d/,5,"So, I have been going through a few models used for semantic segmentation. 

U-net, Seg-net, PSP net, Deep Lab seems to be the popular ones. Do these models solve the problem of semantic segmentation?

Also, is it possible to bring in any changes to the existing model and make it better? I wanted to know are there any methods which can improve the existing models.",1635209279.0,2021-10-26 02:47:59
[N] OpenAI Gym maintainer plans to deprecate and replace MuJoCo and Box2D environments with Brax-based environments.,51,qf788d,MachineLearning,https://github.com/openai/gym/issues/2456,13,,1635129594.0,2021-10-25 04:39:54
[D] MLP's are actually nonlinear âžž linear preconditioners (with visuals!),224,qex0o7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qex0o7/d_mlps_are_actually_nonlinear_linear/,57,"In spirit of yesterday being a [bones day](https://www.tiktok.com/@jongraz/video/7022251358833118469), I put together a few visuals last night to show off something people might not always think about. Enjoy!

Let's pretend our goal was to approximate this function with data.

[\`cos\(norm\(x\)\)\` over \`\[-4Ï€, 4Ï€\]\`](https://i.redd.it/9nwp1rueofv71.gif)

To demonstrate how a neural network ""makes a nonlinear function linear"", here I trained a 32 Ã— 8 [multilayer perceptron](https://github.com/tchlux/tchlux.github.io/blob/dfe4c113826bbefca41109b9b0c8697b3e00e9e7/documents/piecewise_linear_regression_model.f90) with [PReLU](https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html) activation on the function `cos(norm(x))` with a random uniform 10k points over the `[-4Ï€, 4Ï€]` square. The training was done with 1k steps of full-batch Adam (roughly, [my own version of Adam](https://github.com/tchlux/tchlux.github.io/blob/master/documents/piecewise_linear_regression_model.f90#L664-L696)). Here's the final approximation.

[\(8 Ã— 32\) PReLU MLP approximation to \`cos\(norm\(x\)\)\` with 10k points](https://i.redd.it/ji8ykw1iofv71.gif)

Not perfect, but pretty good! Now here's where things get interesting. What happens if you look at the ""last embedding"" of the network, what does the function look like in that space? Here's a visual where I've taken the representations of the data at that last layer and projected them onto the first two [principal components](https://setosa.io/ev/principal-component-analysis/) with the true function value as the z-axis.

[Last-layer embedding of the 10k training points for the MLP approximating \`cos\(norm\(x\)\)\`](https://i.redd.it/0zt6443kofv71.gif)

Almost perfectly linear! To people that think about what a neural network does a lot, this might be obvious. But I feel like there's a new perspective here that people can benefit from:

# When we train a neural network, we are constructing a function that nonlinearly transforms data into a space where the curvature of the ""target"" is minimized!

In numerical analysis, transformations that you make to data to improve the accuracy of later approximations are called ""preconditioners"". Now preconditioning data for linear approximations has many benefits other than just minimizing the loss of your neural network. [Proven error bounds](https://tchlux.github.io/documents/tchlux-2020-thesis-slides-theorem.pdf) for piecewise linear approximations (many neural networks) are affected heavily by the curvature of the function being approximated (full proof is in [Section 5 of this paper](https://tchlux.github.io/papers/tchlux-2020-NUMA.pdf) for those interested).

&#x200B;

>*What does this mean though?*

It means that after we train a neural network for *any* problem (computer vision, natural language, generic data science, ...) we don't have to use the last layer of the neural network (*ahem*, linear regression) to make predictions. We can use k-nearest neighbor, or a [Shepard interpolant](https://en.wikipedia.org/wiki/Inverse_distance_weighting), and the accuracy of those methods will usually be improved significantly! Check out what happens for this example when we use k-nearest neighbor to make an approximation.

[Nearest neighbor approximation to \`3x+cos\(8x\)\/2+sin\(5y\)\` over unit cube.](https://i.redd.it/bz4ssu2nofv71.gif)

Now, train a small neural network (8Ã—4 in size) on the \~40 data points seen in the visual, transform the entire space to the last layer embedding of that network (8 dimensions), and visualize the resulting approximation back in our original input space. This is what the new nearest neighbor approximation looks like.

[Nearest neighbor over the same data as before, but after transforming the space with a small trained neural network.](https://i.redd.it/xg5rageoofv71.gif)

[Pretty neat!](https://youtu.be/Hm3JodBR-vs) The maximum error of this nearest neighbor approximation decreased significantly when we used a neural network as a preconditioner. And we can use this concept *anywhere*. Want to make distributional predictions and give statistical bounds for any data science problem? Well that's really easy to do with lots of nearest neighbors! And we have all the tools to do it.

&#x200B;

>***About me:*** *I spend a lot of time thinking about how we can progress towards useful digital intelligence (AI). I do not research this full time (maybe one day!), but rather do this as a hobby. My current line of work is on building theory for solving arbitrary approximation problems, specifically investigating a generalization of transformers (with nonlinear attention mechanisms) and how to improve the convergence / error reduction properties & guarantees of neural networks in general.*  
>  
>*Since this is a hobby, I don't spend lots of time looking for other people doing the same work. I just do this as fun project. Please share any research that is related or that you think would be useful or interesting!*",1635097260.0,2021-10-24 19:41:00
[P] These Days Style GAN be like (Code and Paper links in the comments),882,qeo7fx,MachineLearning,https://i.redd.it/5qmiz1tax5v71.jpg,65,,1635064767.0,2021-10-24 10:39:27
[D] Creating Reduced Audio Representations,0,qfqnox,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfqnox/d_creating_reduced_audio_representations/,8,"Hi all,

I am currently trying to find methods to represent the structure of an audio clip in a few data points. The clips are short, \~100ms. What would be some metrics which model the audio's structure? 

Thanks!",1635196671.0,2021-10-25 23:17:51
[R] Just Ask for Generalization,26,qf6hsp,MachineLearning,https://evjang.com/2021/10/23/generalization.html,1,,1635126991.0,2021-10-25 03:56:31
[R] Efficient Visual Self-Attention - Link to a free online lecture by the author in comments,70,qeyhwb,MachineLearning,https://i.redd.it/udmgzldy2gv71.png,11,,1635101582.0,2021-10-24 20:53:02
[R] ByteTrack: Multi-Object Tracking by Associating Every Detection Box,1183,qeihw2,MachineLearning,https://v.redd.it/sf125fyg0bv71,64,,1635040318.0,2021-10-24 03:51:58
[D] Trouble Modelling High Dimensional Regression Problem with Autoencoder,13,qf4qxs,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qf4qxs/d_trouble_modelling_high_dimensional_regression/,22,"I've been given the task to fit an Autoencoder to a large dataset of 600 dimensional vectors. After constructing some relatively simple AEs (eg., nn.Linear(600) --> nn.Linear(150) --> nn.Linear(5), reverse..), I've found that it's incredibly hard to converge onto a non-trivial solution. These vectors have been normalized in such a way that their means lie \~0.50, and are bounded between 0-1. I have attached an image illustrating a few of the vectors within my dataset.

I'm finding that since these vectors essentially look like noise centered around some global mean value, any model that I attempt to train collapses onto some noisy curve centered around the mean. Importantly, these vectors all represent something important, so smoothing or other preprocessing tricks are out of the question. Sadly, there doesn't seem to be much structure within the data, so a CNN model is also out.

This is basically a high-dimensional regression problem with diverse data. Does anyone have any tips for training an AE model with such data? Is it even possible? Any tips or recommendations would be great!

EDIT:
I realize I was vague. These vectors are the Fourier transform of the original signal, where the first 300 values are the real portion and the second 300 values are the corresponding imaginary (phase) information. 

[600 Dimensional Vectors with Mean \~0.50](https://preview.redd.it/wphl7n7oohv71.png?width=1690&format=png&auto=webp&s=761f893d52e67f8ab9f8d5207065b46401be5c4a)",1635120965.0,2021-10-25 02:16:05
[D] Paper Explained - Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (Video Walkthrough),21,qf1drv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qf1drv/d_paper_explained_symbolic_knowledge_distillation/,2,"[https://youtu.be/kP-dXK9JEhY](https://youtu.be/kP-dXK9JEhY)

Symbolic knowledge models are usually trained on human-generated corpora that are cumbersome and expensive to create. Such corpora consist of structured triples of symbolic knowledge. This paper takes a different approach and attempts to generate such a corpus by prompting GPT-3. Results show that clever prompting, combined with targeted small critic models trained on human ratings can outperform both human-generated data, as well as the teacher model (GPT-3) itself. The results of this paper give a general recipe for automatically building corpora for various NLP tasks by extracting samples from large language models.

&#x200B;

OUTLINE:

0:00 - Intro & Overview

2:30 - Sponsor: Weights & Biases

4:15 - Commonsense Knowledge Graphs

7:50 - ATOMIC dataset

10:00 - Generating the corpus from a model

13:00 - Prompting GPT-3

15:30 - Generating Events

18:40 - Generating Inferences

23:00 - Evaluating the created dataset

26:45 - Introducing the critic

31:25 - Using the critic to filter the data

36:30 - Training a student on the generated data

41:00 - Key Findings

44:45 - Comments & Conclusion

&#x200B;

Paper: [https://arxiv.org/abs/2110.07178](https://arxiv.org/abs/2110.07178)

Code & Corpus: [https://github.com/peterwestai2/symbolic-knowledge-distillation](https://github.com/peterwestai2/symbolic-knowledge-distillation)",1635110049.0,2021-10-24 23:14:09
[Discussion] Examples of classical / learning-based computer vision methods performing better when using under / overexposed images.,2,qfbi5g,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfbi5g/discussion_examples_of_classical_learningbased/,1,I am looking for examples where classical / learning-based computer vision methods performing better when using under/overexposed images. Any relevant paper recommendation would be great.,1635147346.0,2021-10-25 09:35:46
[P] UI to link entities to a knowledge base ontology,1,qfdwr6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfdwr6/p_ui_to_link_entities_to_a_knowledge_base_ontology/,2,"Hi all!

I have just released a new major feature which I have been working on for a while now. It is now possible to do named entity disambiguation (or named entity linking) from DataQA. There is no other UI-based solution to solve this type of problem. The way it works is:

* upload text with entity annotations
* upload your ontology
* the UI will give you suggestions as you label, so you don't need to search through your 1000s of knowledge bases if you have a large ontology.

https://i.redd.it/8p9r1oqvrkv71.gif

It's all open-source and available here: [https://github.com/dataqa/dataqa](https://github.com/dataqa/dataqa).

A tutorial on how to use it: [https://dataqa.ai/docs/tutorials/medical\_entity\_disambiguation/ned\_side\_effects/](https://dataqa.ai/docs/tutorials/medical_entity_disambiguation/ned_side_effects/).

I would appreciate any feedback :-)",1635158475.0,2021-10-25 12:41:15
[D] Multitask Prompted Training Enables Zero-shot Task Generalization (Explained),4,qf3ln0,MachineLearning,https://youtu.be/YToXXfrIu6w,0,,1635117021.0,2021-10-25 01:10:21
[D] Simple Questions Thread,17,qetu2q,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qetu2q/d_simple_questions_thread/,112,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",1635087615.0,2021-10-24 17:00:15
[D] What is the approx monthly cost of ML infrastructure training + inference at your company,0,qfdln1,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qfdln1/d_what_is_the_approx_monthly_cost_of_ml/,4,"

[View Poll](https://www.reddit.com/poll/qfdln1)",1635157156.0,2021-10-25 12:19:16
[Discussion] How valuable are workshop publications compared to journal publications?,2,qf3xvu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qf3xvu/discussion_how_valuable_are_workshop_publications/,13,"Hi there! I am an Undergraduate student that has been conducting research in the Machine Learning field for about two years now. This summer, I received two top authorships at both NeurIPS and ICML. My work has been heavily skewed towards applications of Machine Learning in the field of ecology, so I was able to get my papers accepted into workshops dedicated to the intersection of climate change and machine learning. I am curious how strong of a resume item these workshop publications will be for my future. Now that I have these two publications under my belt, should I focus on a Journal publication?",1635118186.0,2021-10-25 01:29:46
[D] CIPS Follow-Up Paper explained - Harnessing the Conditioning Sensorium for Improved Image Translation (5-minute summary by Casual GAN Papers - The Author of OG CIPS),0,qf66pv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qf66pv/d_cips_followup_paper_explained_harnessing_the/,1,"Hey everyone!

I was one of the authors of the original CIPS paper and I thought it would be fun to do a breakdown of this follow-up paper that takes CIPS into the 3D world!

If you have been following generative ML for a while you might have noticed more and more GAN papers focusing on the underlying 3D  representation of the generated images. CIPS-3D is a 3D-aware GAN model proposed by Peng Zhou and the team at Shanghai Jiao Tong University  & Huawei that combines a low-res NeRF (surprise) with a CIPS  generator (genuine surprise) to achieve high quality 256x256 3D-aware  image synthesis as well as transfer learning and 3D-aware face stylization.

Fresh out of the oven! Full summary: [https://www.casualganpapers.com/3d-aware-gan-based-on-cips-and-nerf/CIPS-3D-explained.html](https://www.casualganpapers.com/3d-aware-gan-based-on-cips-and-nerf/CIPS-3D-explained.html)

[CIPS-3D](https://reddit.com/link/qf66pv/video/k3960pu23iv71/player)

arxiv: [https://arxiv.org/pdf/2110.09788.pdf](https://arxiv.org/pdf/2110.09788.pdf)  
code: [https://github.com/PeterouZh/CIPS-3D](https://github.com/PeterouZh/CIPS-3D)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",1635125891.0,2021-10-25 03:38:11
[P] The implementation of Conv2D Layer in PyTorch,0,qf4p52,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qf4p52/p_the_implementation_of_conv2d_layer_in_pytorch/,3,"I traced down the implementation of Conv2D Layer down to the choice of the math library (e.g.,cuDNN, MKLDNN, MIOpen, XNNPack) according to the device.  


Thanks to Emacs, CScope, Ripgrep. 

https://preview.redd.it/gglf58mrnhv71.png?width=3838&format=png&auto=webp&s=f5b21aa1712aa73b25b30a371b9efa8a8312c5c0",1635120783.0,2021-10-25 02:13:03
[D][R] Looking for benchmarking papers for edge computing,4,qetewa,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qetewa/dr_looking_for_benchmarking_papers_for_edge/,6,"Hey everyone, I'm curious if there are any good references/papers/resources which analyze, critique and benchmark decision-making systems that are running at the ""edge"" (think a Raspberry Pi or equivalent). I'm interested on offline, edge-first computing applications using AI to make decisions in near-real-time without constantly calling to an outside server for the answer. So any application or paper that's a little more complicated than nested if-statements and focuses on doing everything locally.

NOTE: I know of the existence of NVIDIA Jetson and other mobile edge inference platforms, I see the cool real-time object detection demos on here everyday, but I'm expressly not looking for ""mobile lightweight NN inference benchmarking"". I'm looking for something more in the domain of discussion similar to Tesla's Autopilot decision-making, but maybe a tad more lightweight. Thanks!

Additional note: As I clarify below, the TL;DR is I'm looking for ML papers/researchers (NN-based or traditional) that are focused on doing ML, including training, ""live"" in the field, not just inference using a downloaded and pre-trained model.",1635086231.0,2021-10-24 16:37:11
[D] Multi-prompt Learning in NLP (Paper Summary),4,qerxy1,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qerxy1/d_multiprompt_learning_in_nlp_paper_summary/,0,"Prompt-learning is the latest paradigm to adapt pre-trained language models (PLMs) to downstream NLP tasks. ðŸ”¥

Is one prompt enoughâ“ or Can we have an ensemble of prompts to achieve generalizationâ“ðŸ¤”

Paper summary: https://youtu.be/iUNDg0etR9U

â© Paper Title: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing
â© Paper: https://arxiv.org/abs/2107.13586v1
â© Author: Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig
â© Organisation: Carnegie Mellon University, National University of Singapore",1635081279.0,2021-10-24 15:14:39
"[News][Research] Isolate Voice, Music and Sound Effects With AI | Mitsubishi Research Lab (MERL)",97,qe5duu,MachineLearning,https://youtu.be/Rpxufqt5r6I,6,,1634997627.0,2021-10-23 16:00:27
[N] DeepMind's founder reveals future goals in computational biology regarding AlphaFold during a conference,314,qdyzue,MachineLearning,https://i.redd.it/4jbf2sxl35v71.png,12,,1634968628.0,2021-10-23 07:57:08
[N] Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,16,qee4po,MachineLearning,https://arxiv.org/abs/2110.04725,2,,1635025064.0,2021-10-23 23:37:44
[D] How does the DeepSparse Engine work?,6,qecovo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qecovo/d_how_does_the_deepsparse_engine_work/,6,"Neural magic has open source tools for pruning models and the [DeepSparse](https://github.com/neuralmagic/deepsparse) efficiently executes the sparse models on CPU. Below is the only description of how it works I could find.
 [source](https://neuralmagic.com/blog/how-neural-magics-deep-sparse-technology-works/)
>Mimicking the Brain:  Locality of Reference
In addition to sparsifying the network, ourÂ DeepSparse Engineâ€™s breakthrough sparse kernels execute this sparse computation more effectively. The deeply sparsified computation is memory bound, which is unfortunately not good for a CPU. Our solution to this memory boundedness is to execute the neural network depth-wise rather than layer-after-layer. It might seem like magic, but we are able to break the network into Tensor Columns, vertical stripes of computation that fit completely in cache without having to read or write to memory. Tensor Columns mimic the locality of the brain using the locality of reference of the CPUs cache hierarchy: the outputs of the columnâ€™s small section of a layer of neurons waits in cache for the next layer as the sparse execution unfolds depth-wise.

Can you make sense of this? How can you execute a model depth-wise rather than layer-after-layer?",1635020456.0,2021-10-23 22:20:56
[R] Artificial early language acquisition from videos,8,qe9bvr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qe9bvr/r_artificial_early_language_acquisition_from/,0,"Hi!

During the last months, I built a ML model to artificially reproduce [**language acquisition from videos**](https://hyugen-ai.medium.com/artificial-early-language-acquisition-from-videos-c216ebac9be8)**.**  The idea was to reproduce the link between sounds and images, without using supervised learning in the usual way. I wrote an article to detail the task I created and the unsupervised learning methods I used to train the models. 

It also includes some results, analysis and a demo:

* **Article**: [https://hyugen-ai.medium.com/artificial-early-language-acquisition-from-videos-c216ebac9be8](https://hyugen-ai.medium.com/artificial-early-language-acquisition-from-videos-c216ebac9be8)

I know that the model is incomplete for now but I hope I'll be able to enhance it. If you have some comments on my work I would like to read them and answer questions or insights you could have.

Thanks for your feedback!

^(ps: I also participate on the sub with another account; and I guess it fits with \[R\]/\[D\]/\[P\] tags)",1635010150.0,2021-10-23 19:29:10
[P] arXiv DOOM,235,qdnrn3,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdnrn3/p_arxiv_doom/,18,"arXiv DOOM is a parody of the ever-increasing number of papers that appear on arXiv every day, it allows you to fight the one hundred most-recent papers in the cs.CV category.

Play here: https://sniklaus.com/arxivdoom

Watch demo: https://twitter.com/simon_niklaus/status/1450863810160459777",1634929236.0,2021-10-22 21:00:36
[N] 2021 Jacquelin Perry AI fellowship in human kinematics,0,qeftil,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qeftil/n_2021_jacquelin_perry_ai_fellowship_in_human/,0," Dear ML community,

This is an invitation to apply to the 2021 Jacquelin Perry AI fellowship in human kinematics. This fully paid fellowship runs for five weeks, from November 5, 20216 through to December 10, 2021. For the Fall 2021 edition, we are able to compensate only those who are currently based in the US with plans to expand this internationally for the forthcoming editions.

URL: [http://www.jacquelinperryfellowship.org/](http://www.jacquelinperryfellowship.org/)

**Vision statement**

The ubiquity of high-sampling rate motion sensor chips has presented us with the potential to transform the humble smartphone into a centerpiece of healthcare hardware democratization. Recent literature has shown initial promise in three facets of healthcare: Gait- and posture-related disorders, Geriatric care and Treatment of neurodegenerative disorders. However, these studies have been conducted in a siloed manner with small datasets and oft using low-capacity shallow machine learning models. The goal of this fellowship is to responsibly unite and standardize all of the academic-world's IMU sensor datasets into one open-sourced dataset that is mandated to specifically fuel innovations in the areas of healthcare mentioned above. In doing so, we'd like to celebrate the incredible pioneering efforts of Dr. Jacquelin Perry (1918-2013).",1635030693.0,2021-10-24 01:11:33
[R] Shaking the foundations: delusions in sequence models for interaction and control,10,qe0sl3,MachineLearning,https://arxiv.org/abs/2110.10819,5,,1634977126.0,2021-10-23 10:18:46
[N] Deepfaking Genitalia Into Blurred Porn Leads to Man's Arrest in Japan,546,qd990q,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qd990q/n_deepfaking_genitalia_into_blurred_porn_leads_to/,43,"[https://www.gizmodo.com.au/2021/10/deepfaking-genitalia-into-blurred-porn-leads-to-mans-arrest-in-japan/](https://www.gizmodo.com.au/2021/10/deepfaking-genitalia-into-blurred-porn-leads-to-mans-arrest-in-japan/)

If you want to try out the neural network yourself, you can check out my fork of the code: [https://github.com/tom-doerr/TecoGAN-Docker](https://github.com/tom-doerr/TecoGAN-Docker)

The fork adds a docker environment, which makes it much easier to get the code running.",1634876882.0,2021-10-22 06:28:02
[D] Appleâ€™s text recognition models do not seem to generalise well to rotated images.,94,qdge9o,MachineLearning,https://www.reddit.com/gallery/qdge9o,31,,1634907536.0,2021-10-22 14:58:56
[R] New model leveraging flu data generates highly accurate prediction of COVID-19 spread,0,qe4seb,MachineLearning,https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009363,0,,1634995106.0,2021-10-23 15:18:26
[R] Reduced false positives in autism screening via digital biomarkers inferred from deep comorbidity patterns,0,qe4rig,MachineLearning,https://www.science.org/doi/10.1126/sciadv.abf0354,0,,1634995014.0,2021-10-23 15:16:54
[D] Leveraging Out-of-domain Data to Improve Punctuation Restoration via Text Similarity,0,qe8sbv,MachineLearning,https://youtu.be/jxOpu4hXPJY,0,,1635008511.0,2021-10-23 19:01:51
[D] StyleGAN2 vs StyleGAN3,5,qdo1t4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdo1t4/d_stylegan2_vs_stylegan3/,4,"Hello,

As you know StyleGAN3 has just released. I am curious about the difference between StyleGAN2 and StyleGAN3. Is the structure kinda similar or is there a huge difference?",1634930072.0,2021-10-22 21:14:32
[R] Deeper Is Not Necessarily Better: Princeton U & Intelâ€™s 12-Layer Parallel Networks Achieve Performance Competitive With SOTA Deep Networks,11,qdii2u,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdii2u/r_deeper_is_not_necessarily_better_princeton_u/,7,"In the new paper Non-deep Networks, a research team from Princeton University and Intel Labs argues it is possible to achieve high performance with â€œnon-deepâ€ neural networks, presenting ParNet (Parallel Networks), a novel 12-layer architecture that achieves performance competitive with its state-of-the-art deep counterparts. 

Here is a quick read: [Deeper Is Not Necessarily Better: Princeton U & Intelâ€™s 12-Layer Parallel Networks Achieve Performance Competitive With SOTA Deep Networks.](https://syncedreview.com/2021/10/22/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-129/)

The code is available on the projectâ€™s [GitHub](https://github.com/imankgoyal/NonDeepNetworks). The paper *Non-deep Networks* is on [arXiv](https://arxiv.org/abs/2110.07641).",1634914063.0,2021-10-22 16:47:43
[D] Replicating Clip+VQGAN Settings Again,0,qdwbzo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdwbzo/d_replicating_clipvqgan_settings_again/,3,"Hey guys! I imported a reference image of a building for the text-to-image code to follow. I loved the results. Because the reference image is of a building, I want to replicate THE EXACT SAME results that the text-to-image AI created with the first reference image, but on a different perspective of the building; hence a different reference image. Does anyone know how I can lock the settings in place, so the code can generate the same result again but using a different reference image? Thanks! The code I used is here: [https://colab.research.google.com/drive/1go6YwMFe5MX6XM9tv-cnQiSTU50N9EeT](https://colab.research.google.com/drive/1go6YwMFe5MX6XM9tv-cnQiSTU50N9EeT)",1634957563.0,2021-10-23 04:52:43
[D] should anomaly detection with auto-encoder only be trained with normal data?,1,qduekv,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qduekv/d_should_anomaly_detection_with_autoencoder_only/,8,"Some papers proposed anomaly detection with AE/VAE. What bugs me it that they remove anomalous data prior to training (in my opinion this is not unsupervised). I actually do not feel good about removing the anomalous data.

One paper I saw claimed it is not necessary for VAE.
Yet, I wonder if by training with both normal and anomalous data some model assumptions might be violated and if using normal data for training is â€œdirtyâ€ trick. At last anomalies 1:1k or 1:10k should influence the gradient",1634950284.0,2021-10-23 02:51:24
"[P] An analysis of 7,020,950 NFT transactions on the Ethereum blockchain",272,qcychj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcychj/p_an_analysis_of_7020950_nft_transactions_on_the/,29,"Hi everyone. Not sure how interested /r/MachineLearning is in crypto, but figured that you were the perfect folks to present this dataset: [https://www.kaggle.com/simiotic/ethereum-nfts](https://www.kaggle.com/simiotic/ethereum-nfts)

Our dataset contains 7,020,850 NFT transactions that took place on the Ethereum blockchain between April 1, 2021 and September 25, 2021, across 727,102 accounts.

We discovered that the top 16.71% of NFT owners control 80.98% of NFTs. This isnâ€™t so different from traditional free markets. :)

We also discovered a formula to distinguish utility tokens from security tokens. Utility tokens (like the Ethereum Name Service) come with concrete external functionality. Security tokens are usually purchased purely as investments.

The full report is available on GitHub: [https://github.com/bugout-dev/moonstream/blob/main/datasets/nfts/papers/ethereum-nfts.pdf](https://github.com/bugout-dev/moonstream/blob/main/datasets/nfts/papers/ethereum-nfts.pdf)

We also published a public Kaggle notebook on which we ran our analysis: [https://www.kaggle.com/simiotic/ethereum-nft-analysis](https://www.kaggle.com/simiotic/ethereum-nft-analysis)",1634841416.0,2021-10-21 20:36:56
[N] Symposium on Trustworthy Machine Learning on Thurs Oct 28,2,qdq3k0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdq3k0/n_symposium_on_trustworthy_machine_learning_on/,0,"Hi, I am involved with the Trustworthy ML Initiative, I hope it's OK for me to post a recent event organized by us, with a joint effort from Montreal AI Ethics Institute, just in case there are friends here interested in the trustworthy ML related topics. :D

Some of the details are pasted below, or better viewed here: [https://tinyurl.com/trustmlsymposium](https://tinyurl.com/trustmlsymposium)

\-----------------------------------

The Trustworthy ML Initiative celebrates its one-year anniversary with this special event, held jointly with Montreal AI Ethics Institute.

To achieve the promise of AI as a tool for societal impact, black-box models must not only be ""accurate"" but also satisfy trustworthiness properties that facilitate open collaboration and ensure ethical and safe outcomes. The purpose of this un-symposium is to discuss the interdisciplinary topics of robustness, fairness, privacy, and ethics of AI tools. In particular, we want to highlight the significant gap in deploying these AI models in practice when the stakes are high for commercial applications of AI where millions of human lives are at risk. We welcome researchers, stakeholders, and domain experts to join us.

Agenda (Eastern Time):

10:30am -- Opening remarks: What is the invisible elephant in the room?

10:45am -- Panel on ""Interdisciplinary Research in Trustworthy ML -- Challenges and Way Forward"". 

Panelists: Danielle Belgrave (DeepMind), Tom Dietterich (Oregon State Univ), Kush Varshney (IBM Research). Moderator: Subho Majumdar (Splunk)

11:45am -- Break

12:00 -- Townhall on ""Practical Challenges of Applying Trustworthy ML in Industry"". 

Panelists: Stella Biderman (EleutherAI), Cristian Canton Ferrer (Facebook), Krishnaram Kenthapadi (Amazon). Moderator: Abhishek Gupta (Montreal AI Ethics Institute)

1:00 -- Break. 

Adjourn to separate Zoom link [https://us02web.zoom.us/j/84698021270?pwd=cjBXaWVuRlRuZUI1VGczcXJTZVZmQT09](https://us02web.zoom.us/j/84698021270?pwd=cjBXaWVuRlRuZUI1VGczcXJTZVZmQT09)

1:15 to 3pm -- Social with breakout rooms. Host: Chirag Agarwal (Harvard Univ)

\#1 -- Interpretability. Host: Chhavi Yadav (UCSD)

\#2 -- Is unfairness a security risk? Host: Mohammad Yaghini (U Toronto)

\#3 -- Robustness. Host: Haohan Wang (CMU)

\#4 -- Fairness. Host: Marta Lemanczyk (Hasso Plattner Institute)

\#5 -- Causal inference. Host: Maggie Makar (U Michigan)",1634936230.0,2021-10-22 22:57:10
[R] Overcome survivorship bias in liver transplant patients,4,qdkt5e,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdkt5e/r_overcome_survivorship_bias_in_liver_transplant/,3,"Hi, I'm working on my master thesis in the field of ML and AI and I'm stuck with a problem related to [survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias).

I have \~2000 patients from the national waiting list for liver transplantation. Every patient has around 30 features (age, gender, pathologies, diagnosis, ...) and my job is to obtain the **transplant benefit: how much the patient life was extended thanks to the transplant**.

Example: given a patient, he was added to the waiting list in 2000. He will die in 2001 if he doesn't receive a transplant. He will die in 2004 if he receives a transplant. This means that the transplant benefit is 36 months or 3 years.

The first step is to build a model to regress how much a patient will survive without receiving a transplant. For example: given model M and patient p_i, I want that M(p_i) = y_i, where y_i is the amount of months p_i survives without getting the transplant.

The problem related to survivorship bias is the following: **patients aren't from the same ""population""** [visualization](https://imgur.com/IZ3tT14):
 - some patients weren't able to receive a transplant (e.g. discarded because incompatible): group A
 - other patients received a transplant and survived some more time: group B

At the moment, I'm forced to train my model M on group A patients because only they haven't received a transplant and they are died because of this. On the contrary group B patients have received a transplant, so cannot be used at this stage.

The issue arises when I want to compute the transplant benefit of transplanted patient p_j:
 1) check when p_j died after the transplant: months_t
 2) use model M to obtain survival without transplant: M(p_j) = months_w
 3) transplant benefit = months_t - months_w

Do you see the issue? M was trained only on group A patients, so it is highly inaccurate when applied to group B patients. It's a survivorship bias problem because the group that I can use for training doesn't represent the entire population but only a subset, that is, only patients discarded and incompatible.

Is there a way to overcome this problem?
It's two days that I'm searching papers and articles on ways to solve this issue, but most of the ""solutions"" propose to use data I have no access to (e.g. train model M on group B patients, but I haven't their death date _without_ transplant!)

**TL;DR**
I have two groups of patients: A and B. Group B has missing labels so I train my model only on group A patients. But this creates huge problems because the different groups have very different populations and characteristics: group B contains patients that survived while group A patients were discarded (survivorship bias).",1634920676.0,2021-10-22 18:37:56
"[Discussion] Framerate interpolation with other, lower quality video, which has a higher framerate",1,qdsycu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdsycu/discussion_framerate_interpolation_with_other/,6,"There are two same content videos. The catch is that one is low quality 60Fps, but the other one is high quality 30Fps. Is there a tool to interpolate the high quality to 60 fps that can be trained with the low quality video to reduce interpolation artifacts?",1634945119.0,2021-10-23 01:25:19
"[P] Easy to install, use, extend, run experiments and sink results: PyTorch Implementation for ProSelfLC-CVPR 2021",0,qdshgh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdshgh/p_easy_to_install_use_extend_run_experiments_and/,0,"PyTorch Implementation for ProSelfLC-CVPR 2021

[https://github.com/XinshaoAmosWang/ProSelfLC-CVPR2021](https://github.com/XinshaoAmosWang/ProSelfLC-CVPR2021)

* Easy to install
* Easy to use
* Easy to extend: new losses, new networks, new dataset and loaders
* Easy to run experiments and sink results

&#x200B;

[Accuracy curve](https://preview.redd.it/4bh1wdw303v71.png?width=440&format=png&auto=webp&s=7e0b5d491160f17a33a7e393b31c961aa88a84e8)

&#x200B;

&#x200B;

[Loss curve](https://preview.redd.it/cshq24li03v71.png?width=449&format=png&auto=webp&s=4ab56fdb5b24e5ba690cbd3a323ad33c313b2ee4)

&#x200B;

[params.csv](https://preview.redd.it/9vprjr9n03v71.png?width=2514&format=png&auto=webp&s=685505987cb0e4d56614ff08eb4ad3517b825f79)

&#x200B;

[accuracy\_loss.xlsx](https://preview.redd.it/bqzrpizr03v71.png?width=589&format=png&auto=webp&s=93cfce012df92c9b7c7e585adb1ab682881df5c8)",1634943537.0,2021-10-23 00:58:57
Mujoco Tutorial [R],0,qe0zpt,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qe0zpt/mujoco_tutorial_r/,3,Can someone recommend an in-depth tutorial for Mujoco (not the Python interface). Thanks.,1634978163.0,2021-10-23 10:36:03
[R] Handling Large Consecutive Missing data for forecasting using LSTM,11,qddelh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qddelh/r_handling_large_consecutive_missing_data_for/,8,"Hi there. I am trying to use **LSTM for water level forecasting during the monsoon**. I have multiple input water stations and one output water station. I am taking 4 days of data to predict the output station's water level the next day. I have a dataset from 1960 to 2013. The problem with my dataset is that it contains many missing values. Most of the **data are missing during the non-monsoon months**. Also, data is missing from 2003 to 2005 and in other places, many months too. I have imputed the data using python libraries, but the results are not good as the **gap is too large**.

Imputing such data is creating too much noise. As my focus is primarily on monsoon data, **can I discard the non-monsoon months?**

Another thing I wanted to know is that is it okay to perform **Litwise Deletion** on a dataset that will be used to train a neural network model for forecasting? Especially when I take a length of input data(like consecutive days of data as input)?

Personally, as my primary goal is to make water level predictions in monsoon time, I want to discard the non-monsoon months and remove any missing data by Litwise deletion. I did perform the forecasting prediction by discarding the non-monsoon months, and I got **better results** than imputing the whole dataset(for monsoon and non-monsoon months).",1634895938.0,2021-10-22 11:45:38
"[N] PyTorch 1.10 Release, including CUDA Graphs APIs, Frontend and compiler improvements",161,qcvfaj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcvfaj/n_pytorch_110_release_including_cuda_graphs_apis/,20,"Source: [https://github.com/pytorch/pytorch/releases/tag/v1.10.0](https://github.com/pytorch/pytorch/releases/tag/v1.10.0) and [https://pytorch.org/blog/pytorch-1.10-released/](https://pytorch.org/blog/pytorch-1.10-released/)

# Highlights

We are excited to announce the release of PyTorch 1.10. This release  is composed of over 3,400 commits since 1.9, made by 426 contributors.  We want to sincerely thank our community for continuously improving  PyTorch.

PyTorch 1.10 updates are focused on improving training and performance of PyTorch, and developer usability. Highlights include:

&#x200B;

* CUDA Graphs APIs are integrated to reduce CPU overheads for CUDA workloads.
* Several frontend APIs such as FX, torch.special, and nn.ModuleParametrization, have moved from beta to stable.
* Support for automatic fusion in JIT Compiler expands to CPUs in addition to GPUs.
* Android NNAPI support is now available in beta.",1634833126.0,2021-10-21 18:18:46
[R] Teach Me to Explain: A Review of Datasets for Explainable NLP,8,qdadbs,MachineLearning,https://arxiv.org/abs/2102.12060,2,,1634881716.0,2021-10-22 07:48:36
[D] NearestNeighbors - Gowers Distance consumes RAM very quickly,1,qdkz5o,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdkz5o/d_nearestneighbors_gowers_distance_consumes_ram/,4,"I am trying to cluster a mixed dataset having both numerical and  categorical dataset in Python using ""gower's"" distance followed by  hyper-parameter tuned DBSCAN algorithm. Traditional clustering algos don't work for categorical attributes/columns and I don't want to drop such attributes. The complete code and the dataset can be referred [here](https://github.com/arjun-majumdar/machine-learning-musing/blob/master/Mixed_Dataset_Clustering_Example.ipynb).

*Gower's distance* creates a square matrix of dimensions: m x m where 'm'  is the number of rows in the dataset. For the Adult dataset,  it's shape is (32561, 15). So, gower's distance creates a square matrix of  shape (32561, 32561). Assuming each number takes 32-bit floating point  precision, this matrix takes up roughly 4 GB of RAM.

To hyper-parameter tune DBSCAN algo, DMDBSCAN method is used. On using the *NearestNeighbors* algorithm on this matrix-

    neighbors = NearestNeighbors(n_neighbors = 28)
    nbrs = neighbors.fit(loaded_gower_dist) 

crashes the Kernel in Google Colab environment which has around 12 GB of RAM. Any way to avoid this? Also, this shows that Gower's distance doesn't scale well as the dataset size grows.

Can you suggest better way(s) to avoid this which also scales well?",1634921161.0,2021-10-22 18:46:01
[D] How good are the new M1 Pro / Max macs with ARM processors for deep learning with PyTorch?,52,qcyw4l,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcyw4l/d_how_good_are_the_new_m1_pro_max_macs_with_arm/,13,"Hi,

I need a mac for mobile development but I also use PyTorch. I have seen that TensorFlow has the [TensorFlow metal](https://developer.apple.com/metal/tensorflow-plugin/) plugin provided by Apple for this purpose, but I don't know how PyTorch behaves in this field.

Do you have some insights ?

Thanks",1634843006.0,2021-10-21 21:03:26
"[R] Meta-learning, social cognition and consciousness in brains and machines",0,qdjcv2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qdjcv2/r_metalearning_social_cognition_and_consciousness/,2,"Published in Neural Networks on 18 Oct 2021

Open Access: https://www.sciencedirect.com/science/article/pii/S0893608021003956

**Abstract**

The intersection between neuroscience and artificial intelligence (AI) research has created synergistic effects in both fields. While neuroscientific discoveries have inspired the development of AI architectures, new ideas and algorithms from AI research have produced new ways to study brain mechanisms. A well-known example is the case of reinforcement learning (RL), which has stimulated neuroscience research on how animals learn to adjust their behavior to maximize reward. In this review article, we cover recent collaborative work between the two fields in the context of meta-learning and its extension to social cognition and consciousness. Meta-learning refers to the ability to learn how to learn, such as learning to adjust hyperparameters of existing learning algorithms and how to use existing models and knowledge to efficiently solve new tasks. This meta-learning capability is important for making existing AI systems more adaptive and flexible to efficiently solve new tasks. Since this is one of the areas where there is a gap between human performance and current AI systems, successful collaboration should produce new ideas and progress. Starting from the role of RL algorithms in driving neuroscience, we discuss recent developments in deep RL applied to modeling prefrontal cortex functions. Even from a broader perspective, we discuss the similarities and differences between social cognition and meta-learning, and finally conclude with speculations on the potential links between intelligence as endowed by model-based RL and consciousness. For future work we highlight data efficiency, autonomy and intrinsic motivation as key research areas for advancing both fields.

*Keywords*

Model-based reinforcement learning, Meta-learning, Social cognition, Consciousness",1634916519.0,2021-10-22 17:28:39
[R] AI Researchers From Huawei and Shanghai Jiao Tong University Introduce â€˜CIPS-3Dâ€™: A 3D-Aware Generator of GANs,7,qd7cve,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qd7cve/r_ai_researchers_from_huawei_and_shanghai_jiao/,1,"The StyleGAN architecture is a great way to generate high-quality images, but it lacks the ability to control camera poses precisely. The recent NeRF based Generators have made progress towards creating real results so far as they canâ€™t produce photorealistic images.

Researchers at Huawei and Shanghai Jiao Tong University have developed [CIPS-3D](https://github.com/PeterouZh/CIPS-3D), an approach that synthesizes each pixel value independently, just as its 2D version did.

The proposed generator consists of a shallow 3D NeRF network simplified to alleviate memory complexity and has the capacity for deep 2D INR (implicit neural representation) networks without any spatial convolution or up-sampling operations. The proposed generatorâ€™s design is consistent with the well-known semantic hierarchical principle of GANs, where early layers ((i.e., the shallow NeRF network in the generator) determine pose and middle/high ((i.e., the INR network in the generator) control color scheme. The early NeRF network enables the research team to control camera pose explicitly easily.

# [Quick 5 Min Read](https://www.marktechpost.com/2021/10/21/ai-researchers-from-huawei-and-shanghai-jiao-tong-university-introduce-cips-3d-a-3d-aware-generator-of-gans/) | [Paper](https://arxiv.org/pdf/2110.09788.pdf) | [Github](https://github.com/PeterouZh/CIPS-3D)

&#x200B;

https://reddit.com/link/qd7cve/video/i2a51lxtxwu71/player",1634869864.0,2021-10-22 04:31:04
[R] DeepMindâ€™s Fictitious Co-Play Trains RL Agents to Collaborate with Novel Humans Without Using Human Data,29,qct7u0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qct7u0/r_deepminds_fictitious_coplay_trains_rl_agents_to/,1,"A DeepMind research team explores the problem of how to train agents to collaborate well with novel human partners without using human data and presents Fictitious Co-Play (FCP), a surprisingly simple approach designed to address this challenge. 

Here is a quick read: [DeepMindâ€™s Fictitious Co-Play Trains RL Agents to Collaborate with Novel Humans Without Using Human Data.](https://syncedreview.com/2021/10/21/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-128/)

The paper *Collaborating With Humans Without Human Data* is on [arXiv](https://arxiv.org/abs/2110.08176).",1634826936.0,2021-10-21 16:35:36
[R] Unknown Object Segmentation from Stereo Images,1,qdchtj,MachineLearning,https://arxiv.org/abs/2103.06796,2,,1634891615.0,2021-10-22 10:33:35
[D] Pure math relevant to machine learning?,123,qckeph,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qckeph/d_pure_math_relevant_to_machine_learning/,62,"I am currently a UG in Math and CS and am interested in machine learning. I would like to know what areas of pure/abstract math are being applied to, or give insight into the field of machine learning, deep learning, neural networks etc. I tried searching for similar questions but most of them are > 4 years old and  I would like to know about the current scene.

Apart from the absolute basics like calculus, statistics and linear algebra, are there other areas of math being applied? For example, abstract algebra, complex analysis, topology, measure theory etc. I have read blogposts by Jeremy Kun, Chris Olah and a few others about these things, but am not sure if they are accurate descriptions of the work being done currently.

Also, is there any future scope of this becoming relevant?",1634793771.0,2021-10-21 07:22:51
"[D] Link grammars, symbolic representations, and about using similarities and substitutions algorithms.",0,qd7khs,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qd7khs/d_link_grammars_symbolic_representations_and/,6,"This hypothesis is highly related to the presentations given by Linas VepÅ¡tas first and then Anton Kolonin at SingularityNet AGI-21 Conference about, link grammars, symbolic representations, and about using similarities and substitutions algorithms.

In: [https://www.youtube.com/watch?v=Rcgydm9dlYg](https://www.youtube.com/watch?v=Rcgydm9dlYg)

You need to see the first two presentations to understand what I'm trying to comunicate here.

Iâ€™ll try to be very concise. In short my statement is that with similarities and substitutions you can have solutions of previously unsolved problems. In other words, generate new knowledge using previous related knowledge.

I apologize in advance for my lack of proper technical terms. But, if the idea get through, that will be enough for me. I hope my explanation is understandable.

The following is a graph composed by sub-graphs, the nodes are linked by the equal â€˜=â€™ relation, which goes in both directions in case there is no more links, by default is left to right. The nodes can be single values or sets.

For briefness and explain-ability I will use this notation instead of an actual graph (image).

About my notations:Â 

* \[v1, v2\] is a set,
* '=' is the relation,
* Every line can be seen as a sub-graph.
* I will use // to add a comment for explanation
* A â€˜queryâ€™ is a new node that has no similarity relation in the graph and therefore the algorithm needs to be used

**The initial state of the graph and a case of a query and the result:**

    Alice = programmer
    Bob = engineer
    Mary = designer
    John = programmer
    Rose = engineer
    Joe = designer
    [engineer, job] = [model, system]
    [programmer, job] = [develop, system]
    [designer, job] = [design, UI]
    Alice = available
    Bob = available
    Mary = available
    Alice = [available, programmer]
    Bob = [available, engineer]
    Mary = [available, designer]
    // initially available, but later relations are updated to working
    John = working 
    Rose = working
    Joe = working
    R1 = name
    R2 = name
    solution = [
        [[available, engineer], [engineer, job]], 
        [[available, programmer], [programmer, job]], 
        [[available, designer], [designer, job]]
    ]
    [new, request] = [unsolved, request]
    [request, name] = request
    [solution, name] = solution
    [resolve, request] = work
    work = [solution, for, request]
    // if we have a [unsolved, request] we want to [resolve, request]
    // this is a simplification. It could have more meaning if we add more 
    // details to this relation
    [unsolved, request] = [resolve, request] 
    // the query is [Medical, report, system]
    // the query is linked to be equal to a [new, request]
    [new, request] = [Medical, report, system]
    // generated [request, R1] to identify the query 
    [Medical, report, system] = [request, R1] 
    // given that [unsolved, request] = [new, request]
    [request, R1] = [unsolved, request] 
    [request, R1] = [request, name] // given that R1 = name
    [request, R1] = request
    [request, R1] = [resolve, request] // given that: [unsolved,request]=[request,R1]
    [resolve, [request, R1]] = work // given that: request = [request, R1]
    // here [solution, R1] can be generated by a stored procedure for simplicity, 
    // and in this case is used to
    // identify the node that will be the actual solution
    // given that: work = [resolve, request], and work = [solution, for, request]
    [solution, for, [request, R1]] = [solution, R1] 
    [solution, R1] = [solution, name] 
    [solution, R1] = solution
    // final state, here the replacement for the node â€˜solutionâ€™ is 
    // used to produce the final relation
    [solution, R1] = [
      [Rose, [model, system]], 
      [John, [develop, system]], 
      [Joe, [design, UI]]
    ]
    // For the next part, to do a new query I will do the following to avoid ambiguity
    // this can be resolve in multiple ways, in this case Iâ€™ll go with this
    [request, R1] != [new, request] 

**At this point I will do a new query to illustrate how with substitutions we can generate a new knowledge, given the previous state of graph.**

    [new, request] = [Hotel, management, system] // new query is [Hotel, management, system]
    [Hotel, management, system] = [request, R2] // generate a identification node
    [request, R2] = [request, name] // given that: R2 = name
    [request, name] = [request, R1] // we have this relation, then
    [request, R2] = [request, R1] // therefore
    [request, R2] = [solution, R1] // is the current most similar, but
    [request, R2] is not similar enough to [solution, R1] because:
    // If we take into account the full sequence of substitutions we will notice 
    // that [Hotel,management,system] is not equal to [Medical,report,system]
    // We can do better, if we use 'solution' instead of [solution, R1]:
    
    [solution, R1] = solution
    [request, R2] = solution // given the previous relation
    [request, R2] = [
        [[available, engineer], [engineer, job]], 
        [[available, programmer], [programmer, job]], 
        [[available, designer], [designer, job]]
    ]
    [request, R2] = [
        [Bob, [model, system]], 
        [Alice, [develop, system]], 
        [Mary, [design, UI]]
    ]
    // final output
    [solution, R2] = [
        [Bob, [model, system]], 
        [Alice, [develop, system]], 
        [Mary, [design, UI]]
    ]

This example is not really that interesting, and looking at the result it seems pretty obvious. But the key here is that is generalizable, and is just substitutions.

This substitution mechanism can be applied to multiple cases to solve any kind of situations, given that it has enough previous knowledge. Is like applying a formula, step by step. Every step is guided by a previously known relation. This mechanism should be the algorithm applied directly to the graph, so that is the process with which the graph change from one state to the next state.",1634870602.0,2021-10-22 04:43:22
[D] Sensorium Paper explained - Harnessing the Conditioning Sensorium for Improved Image Translation (5-minute summary by Casual GAN Papers),7,qcu7js,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcu7js/d_sensorium_paper_explained_harnessing_the/,0,"Image  to image translation appears more or less â€œsolvedâ€ on the surface,  yet  there are still several important challenges to overcome. One such   challenge is the ambiguity in multi-modal, reference-guided   image-to-image domain translation. Believing that the choice of what to   preserve as the â€œcontentâ€ of the input image, and â€œstyleâ€ should be   transferred from the target image during domain translation depends   heavily on the task at hand, Cooper Nederhood and his colleagues propose   Sensorium, a new model that conditions its output on the information   from various off-the-shelf pretrained models depending on the task.   Sensorium enables higher quality domain translation for more complex  scenes.

Fresh out of the oven! Full summary: [https://www.casualganpapers.com/multimodal-style-conditioned-image-to-image-domain-translation/Sensorium-explained.html](https://www.casualganpapers.com/multimodal-style-conditioned-image-to-image-domain-translation/Sensorium-explained.html)

[Sensorium](https://preview.redd.it/e4ywow6emtu71.png?width=1636&format=png&auto=webp&s=164ce4a93e5128554bd54ce34089f0530bf5ecf8)

arxiv: [https://arxiv.org/abs/2110.06443](https://arxiv.org/abs/2110.06443)  
code: ?

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",1634829699.0,2021-10-21 17:21:39
[D] Going through Deep Ensemble Paper and Have Question,2,qczrqk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qczrqk/d_going_through_deep_ensemble_paper_and_have/,6,"The deep ensemble paper says they capture many modes but not the curvature of each.  Variation methods say they capture the curvature of one of the modes.  The obvious question is then: why not do ensembles of variation methods?

Does anyone do this?  How are the results?

Paper is here: [https://arxiv.org/abs/1912.02757](https://arxiv.org/abs/1912.02757)",1634845476.0,2021-10-21 21:44:36
"[N] Open Colloquium by Prof. Max Welling: ""Is the next deep learning disruption in the physical sciences?""",112,qcaia2,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcaia2/n_open_colloquium_by_prof_max_welling_is_the_next/,16,"&#x200B;

https://preview.redd.it/u04golp53ou71.jpg?width=2560&format=pjpg&auto=webp&s=20cfd7604b6637a491cd0429eaa4ce04f34497b2

We invite everyone to the online research colloquium by Max Welling from the University of Amsterdam. Participation is free.

**When:** 26 October, 16:20 (GMT+3 / Moscow time)

**Title**: Is the next deep learning disruption in the physical sciences?

**Join:** [https://cs.hse.ru/en/announcements/516500821.html](https://cs.hse.ru/en/announcements/516500821.html) 

**Abstract:** 

A number of fields, most prominently speech, vision and NLP have been disrupted by deep learning technology. A natural question is: ""which application areas will follow next?"". My prediction is that the physical sciences will experience an unprecedented acceleration by combining the tools of simulation on HPC clusters with the tools of deep learning to improve and accelerate this process. Together, they form a virtuous cycle where simulations create data that feeds into deep learning models which in turn improves the simulations. In a way, this is like building a self-learning computational microscope for the physical sciences. In this talk I will illustrate this using two recent pieces of work from my lab: molecular simulation and PDE solving. In molecular simulation we try to predict molecular properties or digitally synthesize molecules with prescribed properties. We have built a number of equivariant graph neural networks to achieve this. Partial differential equations (PDEs) are the most used mathematical model in natural sciences to describe physical processes. Intriguingly, we find that PDE solvers can be learned from data using graph neural networks as well, which has the added benefit that we can learn a solver that can generalize across PDEs and different boundary conditions. Moreover, it may open the door to ab initio learning of PDEs directly from data.

About the speaker: 

**Prof. Dr. Max** **Welling** is a research chair in Machine Learning at the University of Amsterdam and a Distinguished Scientist at MSR. He is a fellow at the Canadian Institute for Advanced Research (CIFAR) and the European Lab for Learning and Intelligent Systems (ELLIS) where he also serves on the founding board. His previous appointments include VP at Qualcomm Technologies, professor at UC Irvine, postdoc at U. Toronto and UCL under supervision of prof. Geoffrey Hinton, and postdoc at Caltech under supervision of prof. Pietro Perona. He finished his PhD in theoretical high energy physics under supervision of Nobel laureate prof. Gerard â€˜t Hooft.",1634762680.0,2021-10-20 22:44:40
[D] Are there any apps that run model inference on the mobile device (Android or iPhone) itself?,1,qd2w6w,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qd2w6w/d_are_there_any_apps_that_run_model_inference_on/,9,"As the title says: I want to know if there are there any apps that run model inference on the mobile device (Android or iPhone) itself?


The devices are becoming more powerful and now with pixel 6 including what Google is calling an AI chip, I am trying to understand the use cases for running the inference on the mobile device versus running it on the cloud and accessing via a service call.",1634854778.0,2021-10-22 00:19:38
[P] Introducing WebEnv,1,qd2llj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qd2llj/p_introducing_webenv/,0,"Announcing version 0.1.0 of the [WebEnv](https://github.com/Antipurity/webenv) environment for joint self-supervised and reinforcement learning. It exposes a web browser to ML agents, and focuses on deployment and user experience with a pre-trained agent.

Sounds unassuming, but it hides surprising depth, because without a huge robot fleet that humans interact with every day, the Web is the closest environment that we can get to an AGI-training environment.

WebEnv is not exactly coming for your job of surfing the Web, but, well, it's complicated. It is intended to eventually create a platform by providing a clear business case for continuously training and scaling up general-purpose agents that serve users, allowing culture to develop around turning Web-surfing into a well-integrated and useful virtual life.

First needs proof that semi-useful agents can be trained, though.

I don't have the compute for it. Maybe it's far too early, or maybe it can help push the boundaries of RL in the real (enough) world, but we'll never know unless we try.",1634853873.0,2021-10-22 00:04:33
[R] Discovering and Achieving Goals via World Models,17,qckqv7,MachineLearning,https://arxiv.org/abs/2110.09514,6,,1634795056.0,2021-10-21 07:44:16
[D] When are multiple networks superior to a single network?,1,qczy6d,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qczy6d/d_when_are_multiple_networks_superior_to_a_single/,16," 

Hello,

My background is in material science. I am using neural networks to predict various material and physical properties (density, stiffness, toughness, strength, etc.)

I first built a single neural network taking 6 inputs and predicting 12 outputs (material properties). The performance was quite high for some properties but very poor for others. (based on R2 scores)

I decided to split the network into multiple ""specialized networks"", taking in the same inputs as the previous model, but only outputting 2 or 3 properties at maximum. I was happily surprised to see that the performance of the neural networks was now high for almost all properties.

I understand that mathematically, we are not solving the same problem, but does anyone have some insights on why can't a single model make highly accurate predictions for all properties?

Is it possible but would necessitate more careful tuning of hyperparameters, or are specialized networks always superior?

Thank you. Looking forward to your inputs.",1634846003.0,2021-10-21 21:53:23
[D] How would you deploy an optimization type model? ex: CLIP+VQGAN,10,qckgsj,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qckgsj/d_how_would_you_deploy_an_optimization_type_model/,8,Theres a lot of resources on trying to commercially deploy models for inference but what if you need to deploy a model that needs to optimize at every request like CLIP based models if input text is given - is there a way to deploy such a model in a way that scales?,1634793993.0,2021-10-21 07:26:33
[R] LCS: Learning Compressible Subspaces for Adaptive Network Compression at Inference Time,6,qcm2dr,MachineLearning,https://arxiv.org/abs/2110.04252,2,,1634800319.0,2021-10-21 09:11:59
[D] Look for papers on applied deep learning ensemble topics,0,qcvd2b,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcvd2b/d_look_for_papers_on_applied_deep_learning/,1,"I am looking for papers with either empirical or theoretical evidence demonstrate the following about convolutional neural nets:

1. Same CNN trained with different seeds doesn't make the same mistakes on test.
2. Adversarial examples are transferable among different CNN architectures.
3. Different CNNs architectures trained on same dataset produce uncorrelated errors on test set.

Reddit kudos will be given to good finds!",1634832946.0,2021-10-21 18:15:46
[P] Effects of Metadata filtering with HNSW on Recall and Query time,1,qctl9b,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qctl9b/p_effects_of_metadata_filtering_with_hnsw_on/,2,"A common limitation of deep neural networks is that you cannot filter using symbolic queries (think of a SQL `""WHERE foo=bar""` clause). When using vector search engines to search through deep neural network embeddings, such filters can be added. This article investigates how setting such filters affects both recall and query time when using the HNSW (Hierarchical Navigable Small Worlds) Approximate Nearest Neighbor algorithm:

[Effects of filtered HNSW Searches on Recall and Latency | Towards Data Science](https://towardsdatascience.com/effects-of-filtered-hnsw-searches-on-recall-and-latency-434becf8041c)",1634827997.0,2021-10-21 16:53:17
[D] DAE have their Keras tickets closed due to inactivity but no fix?,128,qbz44d,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbz44d/d_dae_have_their_keras_tickets_closed_due_to/,52,"Keras these days,

<Lots of comments discussing bug>

&#x200B;

<Lots of silence since no attention given to bug>  


Also Keras,  ""Closing this issue due to lack of recent activity .Please feel free to reopen if you still have concern.Thanks! """,1634730950.0,2021-10-20 13:55:50
[D] How do you store time series data for RnD of a team project?,1,qcmpax,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcmpax/d_how_do_you_store_time_series_data_for_rnd_of_a/,8,"Hi.

We are trying to create an ML development loop (ML CI/CD if it is the right term?), and we have fairly big time series data. Prior to this, for our very initial RnD phase, some of the people in the group worked with the raw data on disk to see how it is behaving. But this is not necessarily scalable for us I think from now on. We want to generate ""trainable"" datasets from this time series, mix it with our new datasets moving on, and we don't have a lot of storage for all people to work with 1 storage unit each, all have the identical data. Each person will experiment on a new model and even though we have a queue for them running, we want to make it as concurrent as possible.

Does it even makes sense to offer concurrency on the data i/o? And if yes, How do companies work around concurrent data i/o needs for huge datasets, and are there any resource for me to read about that? I used to work as a freelancer before and usually did not work at problems at this scale.",1634802919.0,2021-10-21 09:55:19
[R] NormFormer: Improved Transformer Pretraining with Extra Normalization,18,qc42it,MachineLearning,https://arxiv.org/abs/2110.09456,3,,1634745368.0,2021-10-20 17:56:08
[R] An Introduction to Probabilistic Programming,184,qbq5bs,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbq5bs/r_an_introduction_to_probabilistic_programming/,17,"A book-length (300 pages) treatment of probabilistic programming (Stan, Pymc3, etc)! 

Abstract: ""This book is a graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages.
We start with a discussion of model-based reasoning and explain why conditioning is a foundational computation central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a first-order probabilistic programming language (PPL) whose programs correspond to graphical models with a known, finite, set of random variables. In the context of this PPL we introduce fundamental inference algorithms and describe how they can be implemented.
We then turn to higher-order probabilistic programming languages. Programs in such languages can define models with dynamic computation graphs, which may not instantiate the same set of random variables in each execution. Inference requires methods that generate samples by repeatedly evaluating the program. Foundational algorithms for this kind of language are discussed in the context of an interface between program executions and an inference controller.
Finally we consider the intersection of probabilistic and differentiable programming. We begin with a discussion of automatic differentiation, and how it can be used to implement efficient inference methods based on Hamiltonian Monte Carlo. We then discuss gradient-based maximum likelihood estimation in programs that are parameterized using neural networks, how to amortize inference using by learning neural approximations to the program posterior, and how language features impact the design of deep probabilistic programming systems.""

Link: https://arxiv.org/abs/1809.10756",1634694207.0,2021-10-20 03:43:27
[D] Swin Transformers: Why are shifted windows better than sliding windows?,13,qc4ph5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qc4ph5/d_swin_transformers_why_are_shifted_windows/,3,"I understand the purpose behind why the windows are not the same for each layer in order to improve connectivity.

What I do not understand is why shifted windows have lower latency when compared to their sliding window counterparts. The paper ([https://arxiv.org/pdf/2103.14030.pdf](https://arxiv.org/pdf/2103.14030.pdf)) says the following:

""A key design element of Swin Transformer is its shift of the window partition between consecutive self-attention layers, as illustrated in Figure 2. The shifted windows bridge the windows of the preceding layer, providing connections among them that significantly enhance modeling power. This strategy is also efficient in regards to real-world latency: all query patches within a window share the same key set1, which facilitates memory access in hardware. In contrast, earlier sliding window based self-attention approaches \[33, 50\] suffer from low latency on general hardware due to different key sets for different query pixels. Our experiments show that the proposed shifted window approach has much lower latency than the sliding window method, yet is similar in modeling power. The shifted window approach also proves beneficial for all-MLP architectures.""

I don't understand he query-key relations and how they are related to the different window strategies.",1634747064.0,2021-10-20 18:24:24
[R] Direct simultaneous speech to speech translation,2,qcga8k,MachineLearning,https://arxiv.org/abs/2110.08250,1,,1634779862.0,2021-10-21 03:31:02
[R] CVPR expands social media ban,86,qbslai,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbslai/r_cvpr_expands_social_media_ban/,20,"Statement by the #CVPR2022 Program Chairs regarding the Social Media Policy:
> Per the motion passed in the CVPR2021 PAMI-TC meeting, authors should NOT use social media to promote their paper submissions to CVPR during the review period. We are imposing a policy slightly stronger than what passed in the motion, where we define the social media silence period. By our definition, the social media silence period starts four weeks before the paper submission deadline, until the final paper decision notifications are sent to authors. Per the currently planned schedule, the social media silence period is from 10/19/2021 to 03/02/2022. Any social media promotion of a paper incurred in this period, proactively initiated by the authors, is deemed a policy violation.
Updates to the author guidelines and FAQ's will be posted shortly.

https://twitter.com/CVPR/status/1450641549294018560/photo/1",1634702977.0,2021-10-20 06:09:37
"Silhouette score vs SSE, which is most important as K means clustering evaluation? ""[P]""",2,qcbfos,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qcbfos/silhouette_score_vs_sse_which_is_most_important/,1,"I'm trying to compute the optimal K clusters. The SSE suggests that 3 clusters would be optimal, however, the Silhouette score is lowest at 2 clusters.

&#x200B;

First, did I compute it correctly?

Second, which evaluation metric should be leading?

&#x200B;

```

for n\_clusters in range(2,10):

clusterer = KMeans(n\_clusters=n\_clusters)

preds = clusterer.fit\_predict(data)

centers = clusterer.cluster\_centers\_

&#x200B;

score = silhouette\_score(data, preds)

print(""For n\_clusters = {}, silhouette score is {})"".format(n\_clusters, score))

```

&#x200B;

These are the results for Silhouette score:

\- For n\_clusters = 2, silhouette score is 0.6810461692117465)

\- For n\_clusters = 3, silhouette score is 0.5418681782397876)

&#x200B;

```

sse = {}

for k in range(1, 10):

kmeans = KMeans(n\_clusters=k, max\_iter=1000).fit(iris)

\#data\[""clusters""\] = kmeans.labels\_

\#print(data\[""clusters""\])

sse\[k\] = kmeans.inertia\_ # Inertia: Sum of distances of samples to their closest cluster center

sse

```

These for SSE

\- 1: 594.8006666666665,

\- 2: 133.46431822602608,

\- 3: 69.42973924466338,

\- 4: 49.437815584415574,",1634765251.0,2021-10-20 23:27:31
[P] see if two statements contradict eachother,0,qc8ysr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qc8ysr/p_see_if_two_statements_contradict_eachother/,4,I am wondering about current best-practice to see if two statements contradict or agree with each other? Is there any implementation with BERT or something else with NLP people can fine-tune and use?,1634758466.0,2021-10-20 21:34:26
[D] Boring machine learning is where it's at,5,qbyhlz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbyhlz/d_boring_machine_learning_is_where_its_at/,17,"*Note: This is a short opinion piece I wrote, which I thought might be relevant to post here given all the ""career"" talk that's been going on, I'd be rather curious to hear if people here think this is a valid perspective, or if I'm glancing over some important stuff.*

It surprises me that when people think of ""software that brings about the singularity"" they think of text models, or of RL agents. But they sneer at decision tree boosting and the like as boring algorithms for boring problems.

To me, this seems counter-intuitive, and the fact that most people researching ML are interested in subjects like vision and language is flabergasting. For one, because getting anywhere productive in these fields is really hard, for another, because their usefulness seems relatively minimal.

I've said it before and I'll say it again, human brains are very good at the stuff they've been doing for a long time. This ranges from things like controlling a human-like body to things like writing prose and poetry. Seneca was as good of a philosophy writer as any modern, Shakespear as good of a playwright as any contemporary. That is not to say that new works and diversity in literature isn't useful, both from the perspective of diversity and of updating to language and zeitgeist, but it's not game-changing.

Human brains are shit at certain tasks, things like finding the strongest correlation with some variables in an n-million times n-million valued matrix. Or heck, even finding the most productive categories to quantify a spreadsheet with a few dozen categorical columns and a few thousand rows. That's not to mention things like optimizing 3d structures under complex constraints or figuring out probabilistic periodicity in a multi-dimensional timeseries.

The later sort of problem is where machine learning has found the most amount of practical usage, problems that look ""silly"" to a researcher but implacable to a human mind. On the contrary, 10 years in, computer vision is still struggling to find any meaningfully large market fits outside of self-driving. There are a few interesting applications, but they have limited impact and a low market cap. The most interesting applications, related to bioimaging, happen to be things people are quite bad at; They are very divergent from the objective of creating human-like vision capabilities, since the results you want are anything but human-like.

Even worst, there's the problem that human-like ""AI"" will be redundant the moment it's implemented. Self-driving cars are a real challenge precisely until the point when they become viable enough that everybody uses them, afterwards, every car is running on software and we can replace all the fancy CV-based decision making with simple control structures that rely on very constrained and ""sane"" behaviour from all other cars. Google assistant being able to call a restaurant or hospital and make a booking for you, or act as the receptionist taking that call, is relevant right until everyone starts using it, afterwards everything will already be digitized and we can switch to better and much simpler booking APIs.

That's not to say *all* human-like ""AI"" will be made redundant, but we can say that its applications are mainly well-known and will diminish over time, giving way to simpler automation as people start being replaced with algorithms. I say its applications are ""well known"" because they boil down to ""the stuff that humans can do right now which is boring or hard enough that we'd like to stop doing it"". There's a huge market for this, but it's huge in the same way as the whale oil market was in the 18th century. It's a market without that much growth potential.

On the other hand, the applications of ""inhuman"" algorithms are boundless, or at least only bounded by imagination. I've argued before that science hasn't yet caught up to the last 40 years of machine learning. People prefer designing equations by hand and validating them with arcane (and easy to fake, misinterpret and misuse) statistics, rather than using algorithmically generate solutions and validating them with simple, rock-solid methods such as CV. People like Horvath are hailed as genius-level polymaths in molecular biology for calling 4 scikit-learn functions on a tiny dataset.

*Note: Horvath's work is great and I in no way want to pick on him specifically, the world would be much worse without him, I hope epigenetic clocks predict he'll live and work well into old age. I don't think he personally ever claimed the ML side of his work is in any way special or impressive, this is just what I've heard other biologists say.*

This is not to say that the scientific establishment is doomed or anything, it's just slow at using new technologies, especially those that shift the onus of what a researcher ought to be doing. The same goes for industry; A lot of high-paying, high-status positions involve doing work algorithms are better at, precisely because it's extremely difficult for people, and thus you need the smartest people for it.

However, market forces and common sense are at work, and there's a constant uptick in usage. While I don't believe this can bring about a singularity so to speak, it will accelerate research and will open up new paradigms (mainly around data gathering and storage) and new problems that will allow ML to take centre stage.

So in that sense, it seems obvious to postulate a limited and decreasing market for human-like intelligence and a boundless and increasing market for ""inhuman"" intelligence.

This is mainly why I like to focus my work on the latter, even if it's often less flashy and more boring. One entirely avoidable issue with this is that the bar of doing better than a person is low, and the state of benchmarking is so poor as to make head-to-head competition between techniques difficult. Though this in itself is the problem I'm aiming to help solve.

That's about it, so I say go grab a spreadsheet and figure out how to get the best result on a boring economics problem with a boring algorithm; Don't worry so much about making a painting or movie with GANs, we're already really good at doing that and enjoy doing it.",1634728658.0,2021-10-20 13:17:38
[P] Collection of Kaggle Past Solutions (to learn ideas and techniques),322,qb8q56,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qb8q56/p_collection_of_kaggle_past_solutions_to_learn/,10,"&#x200B;

https://preview.redd.it/uw11kx0wwdu71.jpg?width=2669&format=pjpg&auto=webp&s=51dbabc93bf7962be73a5099cabab4c057f421f5

I have collected here \[1,2\] almost all available solutions and ideas with codes shared by top performers in the past Kaggle competitions. This list will gets updated as soon as a new competition finishes. It allows you to search over the Kaggle past competitions solutions and ideas. Please share it with your friends.

\[1\] [https://github.com/faridrashidi/kaggle-solutions](https://github.com/faridrashidi/kaggle-solutions)

\[2\] [https://farid.one/kaggle-solutions/](https://farid.one/kaggle-solutions/)",1634639653.0,2021-10-19 12:34:13
[D] Thoughts on decentralized deep learning,8,qbufcu,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbufcu/d_thoughts_on_decentralized_deep_learning/,4,I recently got interested in decentralized training of deep learning models and wanted to know if anyone else thinks this is a pretty interesting topic. Also what are your thoughts on hivemind ([https://github.com/learning-at-home/hivemind](https://github.com/learning-at-home/hivemind)).,1634710681.0,2021-10-20 08:18:01
[R] World Model Learning and Inference,17,qbqnid,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbqnid/r_world_model_learning_and_inference/,0,"New paper published in Neural Networks (Dec 2021 Edition)

Open access: https://www.sciencedirect.com/science/article/pii/S0893608021003610

Authors: Karl Friston, Rosalyn J. Moran, Yukie Nagai, Tadahiro Taniguchi, Hiroaki Gomi, Josh Tenenbaum

**Abstract**

Understanding information processing in the brainâ€”and creating general-purpose artificial intelligenceâ€”are long-standing aspirations of scientists and engineers worldwide. The distinctive features of human intelligence are high-level cognition and control in various interactions with the world including the self, which are not defined in advance and are vary over time. The challenge of building human-like intelligent machines, as well as progress in brain science and behavioural analyses, robotics, and their associated theoretical formalisations, speaks to the importance of the world-model learning and inference. In this article, after briefly surveying the history and challenges of internal model learning and probabilistic learning, we introduce the free energy principle, which provides a useful framework within which to consider neuronal computation and probabilistic world models. Next, we showcase examples of human behaviour and cognition explained under that principle. We then describe symbol emergence in the context of probabilistic modelling, as a topic at the frontiers of cognitive robotics. Lastly, we review recent progress in creating human-like intelligence by using novel probabilistic programming languages. The striking consensus that emerges from these studies is that probabilistic descriptions of learning and inference are powerful and effective ways to create human-like artificial intelligent machines and to understand intelligence in the context of how humans interact with their world.",1634695896.0,2021-10-20 04:11:36
[D] Offering free ML learning advice!,114,qbdjuc,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbdjuc/d_offering_free_ml_learning_advice/,112,"Hi!

I'm an ML scientist who worked in an industry position for multiple years. I had no ML background when I joined the industry and am self-taught. I also eventually taught courses on ML at a university.

I wanted to help folks with any questions they might have about learning ML or upskilling themselves. Feel free to post questions below, and I'd be happy to answer anything I can!",1634656335.0,2021-10-19 17:12:15
[D] What is your ML experiment workflow? Discussion on training a model to the Latex tables,0,qc2apm,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qc2apm/d_what_is_your_ml_experiment_workflow_discussion/,0,"I hope this is a discussion for everyone working on ML. I have been looking for ways to tune my experimental setup so I could easily train models, but at the same time export results faster. 

My current experimental setup is running experiments on a machine with a GPU, then collecting results to Weights and Biases ([https://wandb.ai/](https://wandb.ai/)), then to Latex tables. I'm trying to automate as much as possible since collecting results takes time and to make things easier. 

What are your experimental workflows?",1634740591.0,2021-10-20 16:36:31
[D] How make to search for research contribution,1,qc19x7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qc19x7/d_how_make_to_search_for_research_contribution/,15,"I am in my 4th year of my doctorate, currently have no luck in publishing my paper event I dont make it to submission. My supv always mentioning that my research is not provide strong contribution for the venue I am targeting. I have many drafts on hold because of this, not be able to submit to any machine learning venue.

My question is what makes strong contribution to a paper, I see many papers are published because they provide a method that outmatch the SOTA.?",1634737655.0,2021-10-20 15:47:35
[R] FlexConv: Continuous Kernel Convolutions with Differentiable Kernel Sizes,52,qbdm52,MachineLearning,https://arxiv.org/abs/2110.08059,8,,1634656532.0,2021-10-19 17:15:32
[D] Open-set classification vs. few-shot learning,1,qbz5xq,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbz5xq/d_openset_classification_vs_fewshot_learning/,3,"Is is true to say that few/single shot learning falls under open set-classification? As different literatures gives confusion answers about it. Some say it falls under few-shot open shot, some say its completely different from open-set. I want to know your opinion about this.",1634731123.0,2021-10-20 13:58:43
[P] Apply KITT4SME Open Call for a grant up to â‚¬100k,0,qc3hne,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qc3hne/p_apply_kitt4sme_open_call_for_a_grant_up_to_100k/,1,"Hi everyone!

My name is Zeki, from SUPSI, the coordinator partner of a H2020 funded project, KITT4SME. Our project aims to deliver a platform which offers affordable, tested and validated AI-based solutions for the manufacturing industry. Currently we are looking for AI developer SMEs to integrate their solutions to our platform via our open calls which provides an EU funding of â‚¬100k.

If you are,

* an SME established in the [H2020 eligible countries](https://ec.europa.eu/info/research-and-innovation/statistics/framework-programme-facts-and-figures/horizon-2020-country-profiles_en), and
* having an AI-based solution for the use of manufacturing sector,

you can apply to our open calls to receive an EU funding of **â‚¬100k**.

We are organizing an info webinar on Monday, 25 Oct at 11:00 CEST, to present the details of the open call and answering your questions. You can register yourself [here](https://forms.gle/6KzAAzudiPTJ3MzM8) and find the answers to your questions at the event.

Until that time, you can find more info on [kitt4sme.eu/open-call/](https://kitt4sme.eu/open-call/) and you can always reach me via here or via the contact form on the website.

Sorry if I created the post in a wrong place, but any suggestions or comments would be more than welcome.

[Open call at a glance](https://preview.redd.it/7lv6nsxuimu71.png?width=1280&format=png&auto=webp&s=628d1e4a6ab94cc9b7dd9ece7888449e9ce92300)

[Info webinar details](https://preview.redd.it/v0rvof8yimu71.png?width=1280&format=png&auto=webp&s=19daf3a78ec71159542cba4e025ecaa7d022791a)

Hope to hearing from you!

Zeki",1634743793.0,2021-10-20 17:29:53
[R] Phase transitions in when feedback is useful,4,qbql7p,MachineLearning,https://arxiv.org/abs/2110.07873,1,,1634695679.0,2021-10-20 04:07:59
[R] Learning in High Dimension Always Amounts to Extrapolation,39,qbbknr,MachineLearning,https://arxiv.org/abs/2110.09485,20,,1634650228.0,2021-10-19 15:30:28
[D] iForest with single feature,0,qbvpnz,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbvpnz/d_iforest_with_single_feature/,1,"Is it possible to train an isolation forest with a single feature? And what would that look like?

I remember with SVMs, it was better to have less features if you can not provide enough training data. And SVMs can work with only one. Is that the same with iForests?

&#x200B;

Thanks for every answer in advance!",1634716554.0,2021-10-20 09:55:54
[P] Mapping an image to a 3D face model (iPhone AR compatible),20,qbcfu6,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbcfu6/p_mapping_an_image_to_a_3d_face_model_iphone_ar/,4,"I've been interested in 3D models for AR recently. I set out to create a script to generate 3D head models from selfies which led me down the rabbit hole of creating a (free) service for this.


I retrained [DECA](https://github.com/YadiraF/DECA) using the original dataset and additional data that I generated and then combined this with a library I wrote to convert .obj files to .usdz, a format used on iPhone to describe 3D models that are compatible with its AR viewer.


Here is the service: [https://facemodel.me](https://facemodel.me)


I'm in the process of tidying up the GitHub repo and will make it public if there is interest. I'm posting here to get feedback on the generated 3D models and to discuss whether there are other use cases for downstream ML applications that people here would find useful.",1634653016.0,2021-10-19 16:16:56
[R] StyleNeRF: A 3D-Aware Generator for High-Resolution Image Synthesis with Explicit Style Control,9,qbdj8n,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbdj8n/r_stylenerf_a_3daware_generator_for/,0,"In a paper currently under double-blind review for ICLR 2022, researchers propose StyleNeRF, a 3D-aware generative model that can synthesize high-resolution images at interactive rates while preserving high-quality 3D consistency, and can even generalize to unseen views with control on styles and poses. 

Here is a quick read: StyleNeRF: [A 3D-Aware Generator for High-Resolution Image Synthesis with Explicit Style Control.](https://syncedreview.com/2021/10/19/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-126/)

The paper *StyleNeRF: A Style-based 3D Aware Generator for High-resolution Image Synthesis* is on [OpenReview](https://openreview.net/forum?id=iUuzzTMUw9K).",1634656283.0,2021-10-19 17:11:23
"[N] DeepMind acquires MuJoCo, makes it freely available",550,qaouds,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qaouds/n_deepmind_acquires_mujoco_makes_it_freely/,36,See the [blog post](https://deepmind.com/blog/announcements/mujoco). Awesome news!,1634570505.0,2021-10-18 17:21:45
[P] the copent package v0.2.1 now on PyPI,1,qbpl8y,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbpl8y/p_the_copent_package_v021_now_on_pypi/,1,"The nonparametric methods for estimating copula entropy and transfer entropy are implemented for statistical (conditional) independence testing in the package.

Copula Entropy is a mathematical concept for multivariate statistical independence measuring and testing, and proved to be equivalent to mutual information. Estimating copula entropy can be applied to many cases, including but not limited to variable selection and causal discovery (by estimating transfer entropy). Please refer to Ma and Sun (2011) <[doi:10.1016/S1007-0214(11)70008-6](https://doi.org/10.1016%2FS1007-0214%2811%2970008-6)\> and Ma (2019) <[arXiv:1910.04375](https://arxiv.org/abs/1910.04375)\> for more information.

In this version, a bug which may cause log0 error is fixed.

PyPI: [https://pypi.org/project/copent/](http://pypi.org/project/copent/)

GitHub:  [https://github.com/majianthu/pycopent/](https://github.com/majianthu/pycopent/)

Any comments are welcome.",1634692246.0,2021-10-20 03:10:46
[R][P] OpenFL: An open-source framework for Federated Learning,2,qbetp0,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbetp0/rp_openfl_an_opensource_framework_for_federated/,1,"Federated learning (FL) is a computational paradigm that enables organisations to collaborate on machine learning (ML) projects without sharing sensitive data, such as, patient records, financial data, or classified secrets. Open Federated Learning (OpenFL) is an open-source framework for training ML algorithms using the data-private collaborative learning paradigm of FL. OpenFL works with training pipelines built with both TensorFlow and PyTorch, and can be easily extended to other ML and deep learning frameworks. Here, we summarise the motivation and development characteristics of OpenFL, with the intention of facilitating its application to existing ML model training in a production environment. Finally, we describe the first use of the OpenFL framework to train consensus ML models in a consortium of international healthcare organisations, as well as how it facilitates the first computational competition on FL.

The main goal of OpenFL framework is provide an easy-enough tool to deal with FL training jobs. To start working with OpenFL, you could use *pip*, *Docker*, or *source code* installation:

PyPI:

    pip install openfl

Docker:

    docker pull intel/openfl

From source:

    git clone https://github.com/intel/openfl.git
    cd openfl
    pip install -e .

Some useful links can be found below:

Paper: [https://arxiv.org/abs/2105.06413](https://arxiv.org/abs/2104.04767)

Code: [https://github.com/intel/openfl](https://github.com/intel/openfl)

Docs: [https://openfl.readthedocs.io/en/latest/](https://openfl.readthedocs.io/en/latest/)

Slack: [https://join.slack.com/t/openfl/shared\_invite/zt-ovzbohvn-T5fApk05\~YS\_iZhjJ5yaTw](https://join.slack.com/t/openfl/shared_invite/zt-ovzbohvn-T5fApk05~YS_iZhjJ5yaTw)",1634660035.0,2021-10-19 18:13:55
[D] LaMa Paper explained - Resolution-robust Large Mask Inpainting with Fourier Convolutions (5-minute summary by Casual GAN Papers),2,qbe17v,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbe17v/d_lama_paper_explained_resolutionrobust_large/,1,"Ever   tried to take a scenic picture just to be photobombed by some random  tourists? Donâ€™t worry, Roman Suvorov and the team at SAIC-Moscow  recently unveiled a model called LaMa (large mask inpainting) that takes  care of it for you. The model excels at inpainting large irregular  masks using fast Fourier convolutions that have a receptive field equal  to the entire image and a specialized wide receptive field perceptual  loss that boosts the consistency for distant regions of an image.! A    surprising yet extremely useful outcome of the paper is that the  pretrained model scales up to 2k resolutions quite trivially.

Fresh out of the oven! Full summary: [https://www.casualganpapers.com/large-masks-fourier-convolutions-inpainting/LaMa-explained.html](https://www.casualganpapers.com/large-masks-fourier-convolutions-inpainting/LaMa-explained.html)

[LaMa](https://i.redd.it/1m1f2c34ffu71.gif)

arxiv: [https://arxiv.org/pdf/2109.07161.pdf](https://arxiv.org/pdf/2109.07161.pdf)  
code: [https://github.com/saic-mdal/lama](https://github.com/saic-mdal/lama)

Subscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!",1634657735.0,2021-10-19 17:35:35
[D] One Class SVM with dirty trainingset,3,qba50e,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qba50e/d_one_class_svm_with_dirty_trainingset/,5,"How or why can you train an OC-SVM with outliers in the trainingset?

For exmaple in [scikit](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html) you can specify ""nu"" for an upper bound on the fraction of training errors (default 50% Errors?). I thought you need to provide clean traningdata for a clean boundary.  How are the initial outliers dealt with?

Thank you!",1634645311.0,2021-10-19 14:08:31
"[R] Facebook AI Introduce â€˜SaLinAâ€™: A Lightweight Library To Implement Sequential Decision Models, Including Reinforcement Learning Algorithms (Paper, Github link included)",18,qb137c,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qb137c/r_facebook_ai_introduce_salina_a_lightweight/,1,"Deep Learning libraries are great for facilitating the implementation of complex differentiable functions. These functions typically have shapes like f(x) â†’ y, where x is a set of input tensors, and y is output tensors produced by executing multiple computations over those inputs. In order to implement a new f function and create a new prototype, one will need to assemble various blocks (or modules) through composition operators. Despite of the easy process, this approach cannot handle the implementation of sequential decision methods. Classical platforms are well-suited for managing the acquisition, processing, and transformation of information in an efficient way.

When it comes to reinforcement learning (RL), these all implementations get critical. A classical deep-learning framework is not enough to capture the interaction of an agent with their environment. Still, extra code can be written that does not integrate well into these platforms. It has been considered to use multiple reinforcement learning (RL) frameworks for these tasks, but they still have two drawbacks:

* New abstractions are being created all the time in order to model more complex systems. However, these new ideas often have a high adoption cost and low flexibility, making them difficult for laypersons who may not be familiar with reinforcement learning techniques.
* The use cases for RL are as vast and varied as the problems it solves. For that reason, there is no one-size-fits all library available on these platforms because each platform has been designed to solve a specific type of problem with their unique features from model-based algorithms through batch processing or multiagent playback strategies, among other things â€“ but they canâ€™t do everything.

As a solution to the above two problems, Facebook researchers introduce [â€˜SaLinAâ€™](https://arxiv.org/pdf/2110.07910.pdf). SaLina works towards making the implementation of sequential decision processes, including reinforcement learning related, natural and simple for practitioners with a basic understanding of how neural networks can be implemented. SaLina proposes to solve any sequential decision problem by using simple â€˜agentsâ€™ that process information sequentially. The targeted audience are not only RL researchers or computer vision researchers, but also NLP experts looking for a natural way of modelling conversations in their models, making them more intuitive and easy to understand than previous methods.

# [Quick 7 Min Read](https://www.marktechpost.com/2021/10/18/facebook-ai-introduce-salina-a-lightweight-library-to-implement-sequential-decision-models-including-reinforcement-learning-algorithms/) | [Paper](https://arxiv.org/pdf/2110.07910.pdf)| [Github](https://github.com/facebookresearch/salina) | [Twitter Thread](https://twitter.com/LudovicDenoyer/status/1450003583609544704?s=20)

&#x200B;

&#x200B;

https://preview.redd.it/naiww5s8abu71.jpg?width=1137&format=pjpg&auto=webp&s=0c9640ab881044980943489fb6db703888126170",1634607660.0,2021-10-19 03:41:00
[R] A New Efficient Transformer: PoNet: Pooling Network for Efficient Token Mixing in Long Sequences,0,qbokzp,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbokzp/r_a_new_efficient_transformer_ponet_pooling/,3,Paper on arxiv [https://arxiv.org/abs/2110.02442](https://arxiv.org/abs/2110.02442),1634688740.0,2021-10-20 02:12:20
[D] Pretraining/transfer learning with SageMaker BlazingText (word2vec)?,0,qbeyas,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbeyas/d_pretrainingtransfer_learning_with_sagemaker/,2,"I have a training set consisting of a description and a binary label. From reading previous work, I know that using pretrained [fasttext embeddings](https://fasttext.cc/docs/en/english-vectors.html) should work well for my use case. I need to be able to make predictions on unseen words (OOV). My company is already using aws/sagemaker, so using [SageMaker Blazing text with subword embedding](https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/blazingtext_word2vec_subwords_text8/blazingtext_word2vec_subwords_text8.ipynb) seems like a good approach.

However, they are providing their own training data in the example - does this means it tries to learn word embeddings from scratch from only this training data? I was expecting to be able to pass pretraining as a parameter, and then fine tune it with my own data so those words get added to the known dictionary.

But the way it looks to me now, is that I either use a lookup against a hardcoded list of pretrained embeddings (like GloVe or one of the word vector datasets from fasttext), which means I'm not using my own data and also have to come up with a solution to handling OOV. Or I use Blazing text which can handle OOV, but then I don't take advantage of any pretrained model?

So my main question is this: can I use Blazing text to get pretrained OOV embeddings? And if so how?

Would be great if I could also understand how transfer learning would work in this case, so I can make use of my own classification data. However, the embeddings will be used in a downstream classification task, so I guess I could say the fine tuning happens there?

* Use pretrined blazing text model to get embeddings for both seen and unseen words (through subword embeddings)
* Combine these embeddings with other features
* Fit a model around the embeddings + features to get the classification

Appreciate any help!",1634660396.0,2021-10-19 18:19:56
[D] Recommender systems,0,qbdxh7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbdxh7/d_recommender_systems/,1,I would like to To integrate personalized recommendation systems in the website related to travel. I have basic knowledge in the domain. I would like to ask if I should create a model from scratch or are there any API that makes things work smarter.,1634657445.0,2021-10-19 17:30:45
[D] Impact of Memory and Core Count for GPU vs Neural Engine on M1 Max,14,qazxuk,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qazxuk/d_impact_of_memory_and_core_count_for_gpu_vs/,10,"Does anyone know M1 uses its GPU core for training and neural engine for inferencing, or it utilizes both for inferencing and training?

&#x200B;

Also would it be able to train large model since m1 max now supports up to 64gb memory?",1634603636.0,2021-10-19 02:33:56
[P] `torch_cka` - Compare PyTorch models and gain insights,1,qbbrzo,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbbrzo/p_torch_cka_compare_pytorch_models_and_gain/,0,"GitHub Link - [torch\_cka](https://github.com/AntixK/PyTorch-Model-Compare)

Comparing two neural networks can be a daunting task. Just comparing their performance isnâ€™t always the best way. By comparing their internal representations / learned features, a lot more insight can be obtained. 

I wrote a quick library to compare the features of PyTorch models by their similarity. This is done through the metric - Centered Kernel Alignment (CKA). CKA is based on HSIC (Hilber-Schmidt Independence Criterion) for detecting non-linear dependencies with scalability. [Reference.](https://arxiv.org/abs/2010.15327)

Compared to other similarity metrics, CKA is scalable as you can use a minibatched version over the entire dataset ( I could even run on the whole ImageNet testset). The size of networks that can be compared depends on your memory but you can always select specific features of your interest.

CKA can compare quite different architectures like CNNs and ViTs. This can also be used for ablation studies where the internal representations are studied rather than just final performance (which comes with its own statistical significance issues). 

Check out the ReadMe for some more examples! Hope it is useful to your work!",1634650923.0,2021-10-19 15:42:03
[D] PhD Student Internship Big Tech Zurich - Low Pay,5,qaxyuh,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qaxyuh/d_phd_student_internship_big_tech_zurich_low_pay/,21,"Hi everyone,

I will soon start an internship in Zurich within one of the GAFAM. 

The recruiter sent me my contract few weeks ago and I discovered that my yearly salary would be 45k chf + 1000chf monthly stipend (+ a one time 750chf bonus). 

From what I checked this is quite a low salary to live comfortably in Zurich. And so I asked him if this was really the right figures and if there could not be any mistake. I also told him about the salary I had during another internship in one of the other GAFAM. But he only replied to me that he did not have much control on those numbers and that he was asking his superiors about those numbers. He also told me that it was the right figures. 

So my questions are the two following:
1) Do you think this is a decent salary for an internship as a PhD student in a big tech company in Zurich? 
2) If not what do you think I should do? Maybe ask my manager about it?
3) What is a good internship salary for Zurich?

Thanks for your help! ðŸ˜Š",1634597231.0,2021-10-19 00:47:11
[R] Graph Neural Networks with Learnable Structural and Positional Representations,63,qai1i7,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qai1i7/r_graph_neural_networks_with_learnable_structural/,8,"Hi all,

Presenting a general framework for Graph Neural Networks to learn positional encodings (PE) alongside structural representations, applicable to any MP-GNNs, including (Graph) Transformers.  


""**Graph Neural Networks with Learnable Structural and Positional Representations**""  
Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio and Xavier Bresson.  


Paper: [https://arxiv.org/abs/2110.07875](https://arxiv.org/pdf/2110.07875.pdf)  
Code: [https://github.com/vijaydwivedi75/gnn-lspe](https://github.com/vijaydwivedi75/gnn-lspe)   


**#2minutebrief**  
Nodes in a graph do not have canonical positional information, like the global word positions in a sentence. This gives rise to limitations such as the lack of (global) structural information when message-passing GNNs are applied to learn on graphs. As a result, such models cannot distinguish isomorphic nodes or other graph symmetries.

In this work, we consider this problem of graph PEs and propose a framework named LSPE that can be used with any MP-GNNs to learn positional and structural feature representations at the same time, thus effectively capturing the two essential properties and tuning these w.r.t. to the task at hand.  


[Fig. The general MPGNNs-LSPE architecture.](https://preview.redd.it/b6421c00z5u71.png?width=1335&format=png&auto=webp&s=f28a693b4512eb0392c186c758afd189d7ab84e8)

In brief, LSPE enhances capabilities of an MP-GNN in the following way:  
1. At the input layer, PEs are initialized with k-dimensional Random Walk that encodes the landing probabilities of a node to itself in 1 to k steps. This leads to unique node representations (at the input itself) for nodes which have unique k-hop neighborhoods in the graph.  
2. At the GNN layers, both the structural and positional representations are updated with separate learnable parameters but following the same analytical update function of a GNN instance chosen.  
3. At the final layer, the learned structural and positional representations are fused to output the resultant node features which is then used for the learning task being dealt with. In addition, a positional loss is used to tune the final layer positional features.  


Above simple steps improves several MP-GNNs and Transformer-GNNs providing a performance boost of up to 64% on molecular datasets. At the same time, we retain the efficient linear complexity of message-passing while generating more expressive node embedding.  


More background and details in the paper.",1634543447.0,2021-10-18 09:50:47
"[R] BigScience's first paper, T0: Multitask Prompted Training Enables Zero-Shot Task Generalization",24,qangm5,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qangm5/r_bigsciences_first_paper_t0_multitask_prompted/,2,"The first modeling paper out of BigScience ([https://bigscience.huggingface.co/](https://bigscience.huggingface.co/)) is here!  

T0 shows zero-shot task generalization on English natural language prompts, outperforming GPT-3 on many tasks while being 16x smaller! 

&#x200B;

[Zero-shot example](https://preview.redd.it/im50ymslu7u71.png?width=1364&format=png&auto=webp&s=3856761d93bd15ae85e392551151087346fcdef0)

A very big collection of prompts (\~2'000 prompts for 170+ datasets) was released ([https://github.com/bigscience-workshop/promptsource](https://github.com/bigscience-workshop/promptsource)) along with the model and the paper.

This was an international collaborative effort, with over 40 people across more than 25 organizations. 

The group included dedicated researchers and engineers from different universities, companies, and think tanks.  

Model: [https://huggingface.co/bigscience/T0pp](https://huggingface.co/bigscience/T0pp) 

Repo: [https://github.com/bigscience-workshop/promptsource](https://github.com/bigscience-workshop/promptsource) 

Paper: [https://arxiv.org/abs/2110.08207](https://arxiv.org/abs/2110.08207)

&#x200B;

Additionally, the T0 models were released in the Hugging Face Model Hub and you can try it out in your browser here: [https://huggingface.co/bigscience/T0pp](https://t.co/QvEaqkfmgk?amp=1)",1634566252.0,2021-10-18 16:10:52
[P] Open-Source Synthetic Data Generation Library,19,qan38i,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qan38i/p_opensource_synthetic_data_generation_library/,6,"Hi r/MachineLearning community! We are the guys from YData and some time ago we have created an open-source project exclusively for synthetic data ( [https://github.com/ydataai/ydata-synthetic](https://github.com/ydataai/ydata-synthetic) ). Hopefully, this will be useful for you!

The purpose of the library is to help you creating synthetic data. It could be used when the original data is not enough or when you don't want any identifiable information to ensure individual's privacy. As it is open-sourced, it's always nice if you guys want to contribute or give some feedback :)

Some tutorial notebooks to get started with the library: [https://github.com/ydataai/ydata-synthetic/tree/master/examples/regular](https://github.com/ydataai/ydata-synthetic/tree/master/examples/regular)

If you've got any ideas or you want to discuss the implementations, feel free to hangout in our friendly synthetic data slack community at [https://slack.ydata.ai](https://slack.ydata.ai/) !",1634565048.0,2021-10-18 15:50:48
[P] Open-Source Implementation of EGNNs with DGL,3,qb01ym,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qb01ym/p_opensource_implementation_of_egnns_with_dgl/,3,"Wanting to use the awesome Equivariant Graph Neural Network (EGNN) layer type from Satorras et al. 2021 in your Deep Graph Library (DGL) project? Look no further! I am open-sourcing my DGL implementation of the layer on GitHub. Enjoy! [\#GraphNNs](https://twitter.com/hashtag/GraphNNs?src=hashtag_click) [\#research](https://twitter.com/hashtag/research?src=hashtag_click)

https://github.com/amorehead/EGNN-DGL",1634604059.0,2021-10-19 02:40:59
[D] I'm bored in my industry. what is your industry and how do you like it?,72,qafpph,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qafpph/d_im_bored_in_my_industry_what_is_your_industry/,60,"I work in e-commerce and have for about 5 years. There are pros: my company has good resources for ML (money, cloud computing, empowered to work on what I want) and supports me and my team, but I'm bored as hell (seems to be an issue where the business hasnâ€™t caught up to the ML, so weâ€™re stuck waiting for parts of the site to be updated and fixed). You can only optimize so much to sell things to folks online before you're limited by the Data Engineering feeds or UI/UX team on motivating the product.

All that said, I don't want to do OpenAI or some crazy C++ purist, cutting edge ML -- fuck, I don't even really want to do ML with vision! I would just like to work in an industry that is a bit more fun, or existentially fulfilling on some level. What else is out there?

I know about Finance, Hospitality and Healthcare, but where else are some interesting applications of ML?",1634532958.0,2021-10-18 06:55:58
[D] New macbook chips for ML?,0,qbaosy,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qbaosy/d_new_macbook_chips_for_ml/,11," Yesterday I saw the announcement of the new chips, and it looks too powerful! Training models with those chips must be a pleasure! Is this a new era for training machine learning in laptops?

I think I will grab the M1 pro. Just wanted to chat and know what you guys think.",1634647275.0,2021-10-19 14:41:15
[D] Overcoming Survivorship bias?,5,qas2cr,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qas2cr/d_overcoming_survivorship_bias/,5,"I have a biased training data for only ""Good Customers"" and need to make a predictor for any given customer. How do I go about it? If someone could point me in a direction. I know I havent elaborated but if someone can list down the standard industry approaches to solving this problem?",1634579782.0,2021-10-18 19:56:22
[R] Mention Memory: Incorporating Factual Knowledge From Various Sources Into Transformers Without Supervision,6,qanxac,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qanxac/r_mention_memory_incorporating_factual_knowledge/,1,"A research team from the University of Southern California and Google proposes TOME, a ""mention memory"" approach to factual knowledge extraction for NLU tasks. A transformer model with attention over a semi-parametric representation of the entire Wikipedia text corpus, TOME can extract information without supervision and achieves strong performance on multiple open-domain question answering benchmarks. 

Here is a quick read: [Mention Memory: Incorporating Factual Knowledge From Various Sources Into Transformers Without Supervision.](https://syncedreview.com/2021/10/18/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-125/)

The paper *Mention Memory: Incorporating Textual Knowledge into Transformers Through Entity Mention Attention* is on [arXiv](https://arxiv.org/abs/2110.06176).",1634567745.0,2021-10-18 16:35:45
[P] Progress with OpenCL backend for pytorch,115,qa85d4,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qa85d4/p_progress_with_opencl_backend_for_pytorch/,23,"I finally managed to train on Pytorch with OpenCL several common vision networks: alexnet, resnet etc.

<https://github.com/artyom-beilis/pytorch_dlprim>

Performance is very good. Also lower than native pytorch and little bit lower than dlprimitives microframework, it seems to run mostly on par with TF2 giving 77% of TF performance in training and same performance in inference

Training or gtx 960 of batch = 16 images 224x224, units ms per batch. Lower is better:

| Framework       | alexnet  | resnet18 | resnet50 |  mobilenet |
|-----------------|----------|----------|----------|--------|------------|
|pytorch/cuda     | 107.108  | 129.456  | 388.951  | 177.434    |     
|pytorch/opencl   | 147.814  | 213.319  | 651.216  | 382.590    |     
|dlprimitives     | 106.033  | 198.092  | 605.541  | 344.599    |     
|keras/tf2-cuda   |  90.005  | 183.447  | 501.362  | 322.416    |",1634506105.0,2021-10-17 23:28:25
[P] Colab Notebook: Log and visualize StyleGAN3 training runs,2,qatlio,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qatlio/p_colab_notebook_log_and_visualize_stylegan3/,0,"Hi everyone,

Wanted to share a Colab notebook that I thought folks here might appreciate. It allows you to log and visualize [StyleGAN3](https://github.com/NVlabs/stylegan3) training runs (images, videos, metrics, hyperparams)â€”using Comet for the logging/visualization

Full disclaimer, I work at Comet as their Head of Community, and to log your own training runs, you do need a free Comet account. But you can run the notebook without the logging as well, and we thought something like this might be of use to folks who are currently experimenting with the new StyleGAN3 architecture.

Here's the link to the notebook if it's of any interest: [https://colab.research.google.com/drive/1\_Mk\_pP8H9lkLLvAs\_ae8X4ji8N253caC#scrollTo=HEaeYEdWiKPY](https://colab.research.google.com/drive/1_Mk_pP8H9lkLLvAs_ae8X4ji8N253caC#scrollTo=HEaeYEdWiKPY)

And I've included a GIF below of what the visualization looks like in the Comet UI. Thanks, everyone! :)",1634584086.0,2021-10-18 21:08:06
[R] Frozen in Time: Learning a Joint Text-Video Embedding for Retrieval (+ live demo),8,qajx50,MachineLearning,https://www.reddit.com/r/MachineLearning/comments/qajx50/r_frozen_in_time_learning_a_joint_textvideo/,0,"Paper: [Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval](https://arxiv.org/pdf/2104.00650.pdf)

**Live Demo:** [**http://meru.robots.ox.ac.uk/frozen-in-time/**](http://meru.robots.ox.ac.uk/frozen-in-time/) **(visual search over millions of videos).**

Project page: [https://www.robots.ox.ac.uk/\~vgg/research/frozen-in-time/](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/)

Code: [https://github.com/m-bain/frozen-in-time](https://github.com/m-bain/frozen-in-time)

New Public Dataset: [https://m-bain.github.io/webvid-dataset/](https://m-bain.github.io/webvid-dataset/) (2.5M captioned videos, 10M coming soon)

Summary:

>End-to-end encoder for visual retrieval that uses only self-attention blocks. This allows flexible training of millions of variable length videos and images jointly.

Abstract:

>Our objective in this work is video-text retrieval - in particular a joint embedding that enables efficient text-to-video retrieval. The challenges in this area include the design of the visual architecture and the nature of the training data, in that the available large scale video-text training datasets, such as HowTo100M, are noisy and hence competitive performance is achieved only at scale through large amounts of compute. We address both these challenges in this paper.We propose an end-to-end trainable model that is designed to take advantage of both large-scale image and video captioning datasets. Our model is an adaptation and extension of the recent ViT and Timesformer architectures, and consists of attention in both space and time. The model is flexible and can be trained on both image and video text datasets, either independently or in conjunction. It is trained with a curriculum learning schedule that begins by treating images as 'frozen' snapshots of video, and then gradually learns to attend to increasing temporal context when trained on video datasets. We also provide a new video-text pretraining dataset WebVid-2M, comprised of over two million videos with weak captions scraped from the internet. Despite training on datasets that are an order of magnitude smaller, we show that this approach yields state-of-the-art results on standard downstream video-retrieval benchmarks including MSR-VTT, MSVD, DiDeMo and LSMDC.",1634552848.0,2021-10-18 12:27:28
[P] Trained an AI with ML to navigate an obstacle course from Rocket League,2149,kp5pxi,MachineLearning,https://gfycat.com/oldfashionedhorriblegreathornedowl,57,,1609621471.0,2021-01-02 22:04:31
[P] Doing a clone of Rocket League for AI experiments. Trained an agent to air dribble the ball.,2943,klbvaw,MachineLearning,https://v.redd.it/379qv12hrs761,67,,1609104382.0,2020-12-27 22:26:22
